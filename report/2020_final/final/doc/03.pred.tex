\newpage
\changeindent{0cm}
\section{ショートカット探索}
\label{sec:pred}
\changeindent{2cm}

\begin{figure}[t]
  \begin{center}
    \includegraphics[clip,width=6cm]{./fig/03.pred/image.png}
  \end{center}
  \caption{ショートカット概念図}
  \label{fig:image}
\end{figure}

DARTS で柔軟なアーキテクチャを探索することを実験の目的とし, 構造探索タスクとして
深層畳み込みネットワークの VGG19\cite{Simonyan15} の
ショートカット接続\cite{mao2016image}を考える.
図 \ref{fig:image} のように, ショートカット接続は,
畳み込み部において 1 つ以上特徴を飛ばした接続とする.
VGG19 は分岐がない単純なネットワーク構造であるため, ベースモデルに適しているとして選択した.

VGG19 は 16 層の畳み込み層と 3 層の線形結合層を持つ.
表 \ref{tab:vgg} は, VGG19 の畳み込みニューラルネットワーク
(Convolutional Neural Network: CNN) 部分の構造を示している.
構成する関数は, フィルターサイズが 3 $\times$ 3 の畳み込み層 (Conv2d) ,
Batch Normalization (BN)\cite{Ioffe2015BatchNA} ,
活性化関数 (Rectified Linear Unit: ReLU) ,
ストライドが 2 の Max Pooling (MaxPool) である.
この VGG19 に対し層を飛ばして接続するショートカットの数と位置を求め,
性能を向上させることを目的とする.

\begin{table}[t]
  \begin{center}
    \caption{VGG19 の構造}
		\vspace{-1mm}
    例として入力する画像を (32, 32, 3) 次元としている.
		\vspace{1mm}
		\vspace{3mm}
    \begin{tabular}{|c|c|c|l|}
    \hline
    \textbf{index} & \textbf{image size} & \textbf{channels} & \multicolumn{1}{c|}{\textbf{applied function}} \\ \hline
    input & 32 $\times$ 32 & 3   & \multicolumn{1}{c|}{-}         \\ \hline
    1     & 32 $\times$ 32 & 64  & 3$\times$3\_Conv2d, BN, ReLU          \\ \hline
    2     & 16 $\times$ 16 & 64  & 3$\times$3\_Conv2d, BN, ReLU, MaxPool \\ \hline
    3     & 16 $\times$ 16 & 128 & 3$\times$3\_Conv2d, BN, ReLU          \\ \hline
    4     & 8 $\times$ 8   & 128 & 3$\times$3\_Conv2d, BN, ReLU, MaxPool \\ \hline
    5     & 8 $\times$ 8   & 256 & 3$\times$3\_Conv2d, BN, ReLU          \\ \hline
    6     & 8 $\times$ 8   & 256 & 3$\times$3\_Conv2d, BN, ReLU          \\ \hline
    7     & 8 $\times$ 8   & 256 & 3$\times$3\_Conv2d, BN, ReLU          \\ \hline
    8     & 4 $\times$ 4   & 256 & 3$\times$3\_Conv2d, BN, ReLU, MaxPool \\ \hline
    9     & 4 $\times$ 4   & 512 & 3$\times$3\_Conv2d, BN, ReLU          \\ \hline
    10    & 4 $\times$ 4   & 512 & 3$\times$3\_Conv2d, BN, ReLU          \\ \hline
    11    & 4 $\times$ 4   & 512 & 3$\times$3\_Conv2d, BN, ReLU          \\ \hline
    12    & 2 $\times$ 2   & 512 & 3$\times$3\_Conv2d, BN, ReLU, MaxPool \\ \hline
    13    & 2 $\times$ 2   & 512 & 3$\times$3\_Conv2d, BN, ReLU          \\ \hline
    14    & 2 $\times$ 2   & 512 & 3$\times$3\_Conv2d, BN, ReLU          \\ \hline
    15    & 2 $\times$ 2   & 512 & 3$\times$3\_Conv2d, BN, ReLU          \\ \hline
    16    & 1 $\times$ 1   & 512 & 3$\times$3\_Conv2d, BN, ReLU, MaxPool \\ \hline
    \end{tabular}
    \label{tab:vgg}
  \end{center}
\end{table}


モデル中の潜在的特徴は高さ・幅・チャンネル数を持つデータであるが,
特徴の次元は場所によって異なるため, ショートカットは次元を変換する必要がある.
したがってショートカット関数は以下のように設定した.
\begin{enumerate}
  \item 次元が同じ場合：恒等関数
  \item チャンネル数が違う場合：Pointwise Convolution
  \item 高さと幅が半分の場合：Factorized Reduce
  \item それ以外の場合：ショートカットを定義しない
\end{enumerate}
Pointwise Convolution は, 要素ごとの畳み込みでチャンネル数を調整する.
Factorized Reduce は, 畳み込み層を偶数列と奇数列の 2 枚 用いることで, 情報の損失を防ぎつつ特徴次元を半分にすることができる.

ショートカットに使用する関数の制限によってショートカット位置の候補は 61 であるため,
探索空間は $2^{61}$ である.
演算子の種類は固定することで, アーキテクチャ $\bm{\alpha}$ は畳み込み部に相当するグラフの重みをもつ隣接行列と定義した.


%4.1
\changeindent{0cm}
\subsection{提案手法 : DARTS}
\label{sec:pred.01}
\changeindent{2cm}

DARTS でネットワーク構造を探索するときの, 学習の手順は
\begin{enumerate}
  \item 探索：アーキテクチャ $\bm{\alpha}$ の訓練
  \item 構成：$\bm{\alpha}$ からネットワークを構成
  \item 評価：得られたネットワークをバックプロパゲーションにより訓練し, テストデータで性能を評価
\end{enumerate}
の 3 段階から成る.


\subsubsection{探索}

探索段階では, 勾配降下法によって $\bm{\alpha}$ の更新を行う.
このとき探索用のネットワークは, ショートカットの本数も探索するため,
$\bm{\alpha}$ に対する重み補正 $\bm{\beta}$ を用いて (\ref{equ:cut}) 式 で定義する.

\begin{equation}
  \label{equ:cut}
  \bm{x}_i = f^{\mathrm{c}}_{i-1, i}(\bm{x}_{i-1}) + \bm{\beta}_i \sum_{j \in S_i} \bm{\alpha}_{ij} f^{\mathrm{s}}_{j, i} (\bm{x}_j)
\end{equation}

ここで $f^{\mathrm{c}}(・)$, $f^{\mathrm{s}}(・)$ は, VGG の畳み込み関数とショートカット関数,
$S_i$ はノード $i$ とショートカットで接続する先行 (predecessor) ノードのインデックス集合である.

ただし $\bm{\beta}=0$ で勾配の更新ができなくなるので,
\begin{equation}
  \label{equ:beta}
  \hat{\bm{\beta}} = \begin{cases}
    \exp(\bm{\beta} - 1) & (\bm{\beta} \leq 1) \\
    \log(\bm{\beta}) + 1 & (\mathrm{otherwise})
  \end{cases}
\end{equation}
で 0 とならないように補正した $\hat{\bm{\beta}}$ を用いた.

\begin{figure}[t]
  \begin{center}
    \begin{tikzpicture}
      \begin{scope}[scale=2]
        \draw[->,>=stealth,semithick] (-1,0)--(2,0)node[above]{$\beta$}; %x軸
        \draw[->,>=stealth,semithick] (0,-1)--(0,2)node[right]{$\hat{\beta}$}; %y軸
        \draw(0,0)node[below right]{O}; %原点
        \draw(1,0)node[below]{$1$}; %点(-1,0)
        \draw(0,1)node[left]{$1$}; %点(0,1)
        \draw(0,{1/e})node[above left]{$\frac{1}{e}$}; %点(0,1)
        \draw[dotted,domain=0:1] plot(\x,{1});
        \draw[dotted,domain=0:1] plot({1}, \x);
        \draw[very thick, domain=1:2] plot(\x,{ln(\x)+1})node[above]{$\hat{\bm{\beta}} = \log(\bm{\beta}) + 1$};
        \draw[very thick, domain=-1:1] plot(\x,{exp(\x - 1)})node[below right]{$\hat{\bm{\beta}} = \exp(\bm{\beta} - 1)$};
      \end{scope}
    \end{tikzpicture}
  \end{center}
  \caption{$\bm{\beta}$ の補正関数}
  \label{fig:pred/beta}
\end{figure}




\subsubsection{構成}

構成段階では, 探索段階で得られた $\bm{\alpha}$ から具体的なネットワークをサンプリングする.
$\bm{\alpha}$ の値に対する, ネットワークの構成手法はいくつか考えられるため,
\begin{itemize}
  \item 構成手法 A : predecessors の中で大きい順に採択
  \item 構成手法 B : 閾値以上のエッジを採択
\end{itemize}
の 2 通りの手法を設定した.
% (predecessorsはあるノードに接続している前のノードの集合)

\subsubsection{評価}
評価段階では, 構成段階で得られたネットワークを学習し,
最大の正答率をネットワークの性能とする.
このときのパラメータは, グリッドサーチによる最適化 API の Optuna \cite{akiba2019optuna}
で事前学習した結果から設定する.




\clearpage\newpage
\changeindent{0cm}
\subsection{提案手法 : DARTS + TDGA}
\label{sec:pred.02}
\changeindent{2cm}

\begin{figure}[t]
  \begin{center}
    \includegraphics[clip,width=6cm]{./fig/03.pred/datdga.png}
  \end{center}
  \caption{提案手法 : DARTS + TDGA 概念図}
  \label{fig:image_ga}
\end{figure}

実験 1 では $\bm{\alpha}$ の学習程度によって重み $\bm{w}$ の学習しやすさに偏りがあったため,
収束するグラフ構造にばらつきが見られた.

そこで実験 1 から得られた構造探索のための実験設定をもとに,
個体表現を $\bm{\alpha}$ とした遺伝的アルゴリズムによって,
アーキテクチャを複数同時に管理, 最適化し,
個体群の多様性を維持しつつ, 安定的なネットワーク構造の学習を図った.
単純に個体数を増やすと計算コストが定数倍されるため,
重み $\bm{w}$ は全体で共有する One-Shot モデルを利用することで高速化した.


\subsubsection{多様性項の拡張}
(\ref{eq:Entropy}) 式では, 多様性の計算にエントロピーを利用していたが,
個体 $\bm{\alpha}$ は実数値の行列であるためエントロピーは計算できない.

\begin{equation}
H = \sum_{k \in \mathcal{P}} \sqrt{\mathrm{MSE}(\bm{\alpha}_k, \bar{\bm{\alpha}})} \label{eq:Entropy-new}
\end{equation}

そこで行列の各要素の標準偏差の平均として, $H$ の実数値拡張をした.


\subsubsection{実験手順}
実験は 3 段階からなり, 事前学習した DARTS から, $\bm{w}$, $\bm{\alpha}$ を引き継ぎ,
TDGA で再探索し, 得られた最良個体のアーキテクチャを学習して性能を評価する.
図 \ref{fig:image_ga} に $\bm{w}$, $\bm{\alpha}$ の関係を示す.

\begin{enumerate}
  \item 事前学習
  \item アーキテクチャ再探索
  \item 性能評価
\end{enumerate}

\subsubsection{事前学習}

事前学習では, TDGA に引き継ぐための初期値を DARTS で学習する.
実験 1 の結果から辺ごとに計算する手法が有効であるとわかったため,
正規化として先行研究の DARTS でも利用されていた,
Softmax ではなく Hard Sigmoid に変更する.
この変更によって, $\alpha$ の正規化された値と生の値の相互変換ができるようになり,
実験間のデータ受け渡しが容易となった.


\subsubsection{アーキテクチャ再探索}

以下に TDGA をベースとして, DARTS の学習ステップの追加と多様性項の実数値拡張をした提案手法のアルゴリズム

\begin{itemize}
  \item 提案手法 1. DARTS + TDGA ($\bm{w}$, $\bm{\alpha}$)
  \item 提案手法 2. DARTS + TDGA ($\bm{\alpha}$)
  \item 提案手法 3. DARTS + TDGA
\end{itemize}

\noindent
を アルゴリズム \ref{alg1}, \ref{alg2}, \ref{alg3} に示す.
アルゴリズム \ref{alg1} をベースとして, DARTS による勾配更新のステップを無効にすることで,
DARTS が TDGA の学習に与える影響をみる.

\begin{algorithm}[tb]
  \caption{提案手法 1. DARTS + TDGA ($\bm{w}$, $\bm{\alpha}$)}
  \label{alg1}
  \begin{enumerate}
    \item DARTSで事前学習したモデルの重みを引き継いだ初期個体を生成
    \item 重み $\bm{w}$ を $\displaystyle \nabla_{\bm{\alpha}} \mathcal{L}_{\mathrm{train}}(\bm{w}, \bar{\bm{\alpha}})$ で更新
    \item 個体 $\bm{\alpha}_i$ を $\displaystyle \nabla_{\bm{\alpha}} \mathcal{L}_{\mathrm{valid}}(\bm{w}, \bm{\alpha}_i)$ で更新
    \item 適応度 $\displaystyle \mathcal{L}_{\mathrm{test}}(\bm{w}, \bm{\alpha}_i)$ で個体 $\bm{\alpha}_i$ を評価
    \item エリート個体選択
    \item 交叉で子個体群生成
    \item 親個体群と子個体群の突然変異
    \item エリート個体と親個体群, 子個体群に熱力学的選択によって次世代とする
    \item 収束するまで 2. に戻る
  \end{enumerate}
\end{algorithm}


\begin{algorithm}[tb]
  \caption{提案手法 2. DARTS + TDGA ($\bm{\alpha}$)}
  \label{alg2}
  \begin{enumerate}
    \item DARTSで事前学習したモデルの重みを引き継いだ初期個体を生成
    \item 個体 $\bm{\alpha}_i$ を $\displaystyle \nabla_{\bm{\alpha}} \mathcal{L}_{\mathrm{valid}}(\bm{w}, \bm{\alpha}_i)$ で更新
    \item 適応度 $\displaystyle \mathcal{L}_{\mathrm{test}}(\bm{w}, \bm{\alpha}_i)$ で個体 $\bm{\alpha}_i$ を評価
    \item エリート個体選択
    \item 交叉で子個体群生成
    \item 親個体群と子個体群の突然変異
    \item エリート個体と親個体群, 子個体群に熱力学的選択によって次世代とする
    \item 収束するまで 2. に戻る
  \end{enumerate}
\end{algorithm}

\begin{algorithm}[tb]
  \caption{提案手法 3. DARTS + TDGA}
  \label{alg3}
  \begin{enumerate}
    \item DARTSで事前学習したモデルの重みを引き継いだ初期個体を生成
    \item 適応度 $\displaystyle \mathcal{L}_{\mathrm{test}}(\bm{w}, \bm{\alpha}_i)$ で個体 $\bm{\alpha}_i$ を評価
    \item エリート個体選択
    \item 交叉で子個体群生成
    \item 親個体群と子個体群の突然変異
    \item エリート個体と親個体群, 子個体群に熱力学的選択によって次世代とする
    \item 収束するまで 2. に戻る
  \end{enumerate}
\end{algorithm}


% $P$は個体群,
% $\bar{\bm{\alpha}}$ は各個体の平均,
% $\bm{\alpha}_$ は実験1で有効だった構成手法Bで隣接行列にサンプリングした $\bm{\alpha}$ である.


\subsubsection{性能評価}

性能評価の段階では, 提案手法による探索結果をもとに,
学習後最終世代の個体の性能を実験 1 と同じ条件で学習し評価した.
