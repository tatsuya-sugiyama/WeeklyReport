\newpage
\changeindent{0cm}
\section{ショートカット探索}
\label{sec:pred}
\changeindent{2cm}

DARTSで柔軟なアーキテクチャを探索するため,
深層畳み込みネットワークのVGG19\cite{Simonyan15}のショートカット接続を探索する.
VGG19は分岐がない単純なネットワーク構造であるため, ベースモデルに適しているとして選択した.

VGG19は16層の畳み込み層と3層の線形結合層を持つ.
表 \ref{tab:vgg} は, VGG19の畳み込みニューラルネットワーク(Convolutional Neural Network: CNN) 部分の構造を示している. 構成する関数は, フィルターサイズが 3 x 3 の畳み込み層(Conv2d), Batch Normalization(BN), 活性化関数(Rectified Linear Unit: ReLU), ストライドが 2 の Max Pooling (MaxPool)である.
このVGG19に対し層を飛ばして接続するショートカットの数と位置を求め,
性能を向上させることを目的とする.

\begin{table}[t]
  \begin{center}
    \caption{VGG19 の構造}
		\vspace{-1mm}
    例として入力する画像を(32, 32, 3)次元としている.
		\vspace{1mm}
		\vspace{3mm}
    \begin{tabular}{|c|c|c|l|}
    \hline
    \textbf{index} & \textbf{image size} & \textbf{channels} & \multicolumn{1}{c|}{\textbf{applied function}} \\ \hline
    input & 32 x 32 & 3   & \multicolumn{1}{c|}{-}         \\ \hline
    1     & 32 x 32 & 64  & 3x3\_Conv2d, BN, ReLU          \\ \hline
    2     & 16 x 16 & 64  & 3x3\_Conv2d, BN, ReLU, MaxPool \\ \hline
    3     & 16 x 16 & 128 & 3x3\_Conv2d, BN, ReLU          \\ \hline
    4     & 8 x 8   & 128 & 3x3\_Conv2d, BN, ReLU, MaxPool \\ \hline
    5     & 8 x 8   & 256 & 3x3\_Conv2d, BN, ReLU          \\ \hline
    6     & 8 x 8   & 256 & 3x3\_Conv2d, BN, ReLU          \\ \hline
    7     & 8 x 8   & 256 & 3x3\_Conv2d, BN, ReLU          \\ \hline
    8     & 4 x 4   & 256 & 3x3\_Conv2d, BN, ReLU, MaxPool \\ \hline
    9     & 4 x 4   & 512 & 3x3\_Conv2d, BN, ReLU          \\ \hline
    10    & 4 x 4   & 512 & 3x3\_Conv2d, BN, ReLU          \\ \hline
    11    & 4 x 4   & 512 & 3x3\_Conv2d, BN, ReLU          \\ \hline
    12    & 2 x 2   & 512 & 3x3\_Conv2d, BN, ReLU, MaxPool \\ \hline
    13    & 2 x 2   & 512 & 3x3\_Conv2d, BN, ReLU          \\ \hline
    14    & 2 x 2   & 512 & 3x3\_Conv2d, BN, ReLU          \\ \hline
    15    & 2 x 2   & 512 & 3x3\_Conv2d, BN, ReLU          \\ \hline
    16    & 1 x 1   & 512 & 3x3\_Conv2d, BN, ReLU, MaxPool \\ \hline
    \end{tabular}
    \label{tab:vgg}
  \end{center}
\end{table}


モデル中の潜在的特徴は高さ・幅・チャンネル数を持つデータであるが,
特徴の次元は場所によって異なるため, ショートカットは次元を変換する必要がある.
したがってショートカット関数は以下のように設定した.
\begin{enumerate}
  \item 次元が同じ場合：恒等関数
  \item チャンネル数が違う場合：Pointwise Convolution
  \item 高さと幅が半分の場合：Factorized Reduce
  \item それ以外の場合：ショートカットを定義しない
\end{enumerate}
ショートカットに使用する関数の制限によってショートカット位置の候補は61であるため,
探索空間は$2^{61}$である.
演算子の種類は固定することで, アーキテクチャ $\alpha$ は畳み込み部に相当するグラフの重みをもつ隣接行列と定義した.


%4.1
\changeindent{0cm}
\subsection{提案手法:DARTS}
\label{sec:pred.01}
\changeindent{2cm}

ショートカットの本数も探索するため,
$\alpha$ に対する重み補正 $\beta$ を (\ref{equ:cut}) 式 で定義する.
\begin{equation}
  \label{equ:cut}
  x_i = f^{\mathrm{c}}_{i-1, i}(x_{i-1}) + \beta_i \sum_{j \in S_i} \alpha_{ij} f^{\mathrm{s}}_{j, i} (x_j)
\end{equation}
ここで $f^{\mathrm{c}}(・)$, $f^{\mathrm{s}}(・)$ は, VGGの畳み込み関数とショートカット関数,
$S_i$ はノード $i$ とショートカットで接続する先行(predecessor)ノードのインデックス集合である.

ただし$\beta=0$で勾配の更新ができなくなるので,
\begin{equation}
  \label{equ:beta}
  \hat{\beta} = \begin{cases}
    \exp(\beta - 1) & (\beta \leq 1) \\
    \log(\beta) + 1 & (\mathrm{otherwise})
  \end{cases}
\end{equation}
で0とならないように補正した $\hat{\beta}$ を用いた.

学習の手順を以下に示す.
\begin{enumerate}
  \item 探索：アーキテクチャ $\alpha$ の訓練
  \item 構成：$\alpha$ からネットワークを構成
  \item 評価：得られたネットワークをバックプロパゲーションにより訓練し, テストデータで性能を評価
\end{enumerate}

構成手法は複数考えられるため,
\begin{itemize}
  \item 構成手法A : predecessorsの中で大きい順に採択
  \item 構成手法B : 閾値以上のエッジを採択
\end{itemize}
で実験した.


\changeindent{0cm}
\subsubsection{実験概要}
\label{sec:pred.01_03}
\changeindent{2cm}


\begin{table}[tb]
  \begin{center}
    \caption{実験1の設定}
  	\vspace{3mm}
    \begin{tabular}{|c|c|} \hline
      Loss & Cross Entropy Loss \\ \hline
      batch size & 64 \\ \hline\hline
      Step & Architecture Search \\ \hline
      Optim($w$) & SGD(lr=0.001, momentum=0.9) \\ \hline
      Optim($\alpha$) & Adam(lr=0.003, $\beta$=(0.5, 0.999)) \\ \hline
      data size & train : valid : test = 25000 : 25000 : 10000\\ \hline\hline
      Step & Evaluation \\ \hline
      Optim($w$) & SGD(lr=0.0090131, momentum=0.9) \\ \hline
      Scheduler($w$) & Step($\gamma$=0.23440, stepsize=100) \\ \hline
      data size & train : valid : test = 50000 : 0 : 10000\\ \hline
    \end{tabular}
    \label{tab:setting1}
  \end{center}
\end{table}

表 \ref{tab:setting1} に探索段階と評価段階の実験設定を示す.
探索段階はDARTSを参考に, 評価段階はoptunaで最適化した値を使用した.
データセットは, 訓練画像が 32 pixel 四方で訓練データを50000枚持つ CIFAR-10 を利用して,
10クラス分類問題を解いた.

探索時間は, 150 epochとし, 50 epochごとにその時点の $\alpha$ の性能を評価した.
構成段階では手法A, Bに加えて比較のため,
ショートカット数が同じとなる条件でランダムに選択する手法でも実験する.
各手法において10回試行して統計的な性能を比較した.



\changeindent{0cm}
\subsection{提案手法:DARTS+TDGA}
\label{sec:pred.02}
\changeindent{2cm}

実験1では $\alpha$ の学習度によって重み $w$ の学習しやすさに偏りがあったため,
収束するグラフ構造にばらつきが見られた.

そこで個体表現を $\alpha$ とした遺伝的アルゴリズムによって,
アーキテクチャの多様性を維持しつつ, 安定的な
ネットワーク構造の学習を図った.
しかし単純に個体数を増やすと, 計算コストが定数倍されるので,
重み $w$ は全体で共有する One-Shot モデルを利用することで,
高速化した.

% GAには評価関数の全順序性が必要となるので,（これは間違い）
各パラメータ集合の学習ステップを分離し個体間で不平等がないように設計した.

% \begin{enumerate}
%   \item 一様乱数で初期個体生成
%   % \item 重み$w$を$\displaystyle \sum_{i \in P} \nabla_w \mathcal{L}_{\mathrm{train}}(w^*, \alpha^\mathrm{sampled}_i)$で更新
%   \item 重み $w$ を $\displaystyle \nabla_w \mathcal{L}_{\mathrm{train}}(w^*, \bar{\alpha})$ で更新
%   \item 個体 $\alpha_i$ を $\displaystyle \nabla_\alpha \mathcal{L}_{\mathrm{valid}}(w^*, \alpha_i)$ で更新
%   \item 適応度 $\displaystyle \mathcal{L}_{\mathrm{test}}(w, \alpha^\mathrm{smp})$ で個体 $\alpha$ を評価・選択
%   \item 交叉・突然変異
%   \item 収束するまで 2. に戻る
% \end{enumerate}

\begin{enumerate}
  \item DARTSで事前学習したモデルの重みを引き継いだ初期個体を生成
  \item エリート個体選択
  \item 個体 $\alpha_i$ を $\displaystyle \nabla_\alpha \mathcal{L}_{\mathrm{valid}}(w^*, \alpha_i)$ で更新
  \item 適応度 $\displaystyle \mathcal{L}_{\mathrm{test}}(w, \alpha^\mathrm{smp})$ で個体 $\alpha$ を評価
  \item 交叉で子個体群生成
  \item 親個体群と子個体群の突然変異
  \item エリート個体と親個体, 子個体に熱力学的選択をして次世代とする
  \item 収束するまで 2. に戻る
\end{enumerate}
ただし
% $P$は個体群,
$\bar{\alpha}$ は各個体の平均,
$\alpha^\mathrm{smp}$ は実験1で有効だった構成手法Bで隣接行列にサンプリングした $\alpha$ である.

学習後最終世代の個体の性能を実験1と同じ条件で評価した.


\changeindent{0cm}
\subsubsection{実験概要}
\label{sec:pred.02_03}
\changeindent{2cm}


表 \ref{tab:setting2}, \ref{tab:setting_ga}にモデルとGAの実験設定を示した.
モデルの重み $w$ はImage Netで訓練された事前学習の重みを畳み込み層の部分に適用した.
初期収束を防ぐため, 交叉にはエッジに相当する遺伝子座ごとに 0.5 の確率で操作する一様交叉を使用した.
突然変異には遺伝子座ごとに 0.1 の確率で $\mu=0$, $\gamma=0.2$ となるガウス分布からの摂動を与えた.


\begin{table}[t]
  \begin{center}
    \caption{実験2の設定}
  	\vspace{3mm}
    \begin{tabular}{|c|c|} \hline
      % Optim($w$) & SGD(lr=0.001, momentum=0.9) \\ \hline
      Optim($\alpha$) & Adam(lr=0.001, $\beta$=(0.5, 0.999)) \\ \hline
      % Loss & Cross Entropy Loss \\ \hline
      % batch size & 64 \\ \hline
      data size & train : valid = 0 : 25000 \\ \hline
    \end{tabular}
    \label{tab:setting2}
  \end{center}
\end{table}

\begin{table}[t]
  \begin{center}
    \caption{GAの設定}
  	\vspace{3mm}
    \begin{tabular}{|c|c|} \hline
      個体数 & 10 \\ \hline
      世代数 & 150 \\ \hline \hline
      選択 & 熱力学的選択 \\ \hline
      サイズ & 2 \\ \hline \hline
      交叉 & 一様交叉 \\ \hline
      交叉率 & 0.8 \\ \hline \hline
      変異 & ガウス分布 \\ \hline
      変異率 & 0.2 \\ \hline
    \end{tabular}
    \label{tab:setting_ga}
  \end{center}
\end{table}
