{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NAS test.ipynb","provenance":[],"collapsed_sections":["-x0m5_f1_bPM","MJcIT6CC--14","6c6pY6qT_Hcc","m2K-51qdYUJu","4hjG8JhgNykr","-nlV1X9RN5MB","5_9lzCS6oeWk"],"toc_visible":true,"authorship_tag":"ABX9TyONJVW/en0Ihq7Bij/9EMC/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rrZoN0VtYK09","colab_type":"text"},"source":["# Define"]},{"cell_type":"markdown","metadata":{"id":"-x0m5_f1_bPM","colab_type":"text"},"source":["## import"]},{"cell_type":"code","metadata":{"id":"tBu5wn0eenjg","colab_type":"code","colab":{}},"source":["import torch #基本モジュール\n","from torch.autograd import Variable #自動微分用\n","import torch.nn as nn #ネットワーク構築用\n","import torch.optim as optim #最適化関数\n","import torch.nn.functional as F #ネットワーク用の様々な関数\n","import torch.utils.data #データセット読み込み関連\n","import torchvision #画像関連\n","from torch import Tensor\n","from torchvision import datasets, models, transforms #画像用データセット諸々\n","\n","import numpy as np\n","import argparse\n","import json\n","from logging.config import dictConfig\n","from logging import getLogger\n","import os\n","import time\n","from google.colab import files\n","import itertools"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MJcIT6CC--14","colab_type":"text"},"source":["## Components"]},{"cell_type":"code","metadata":{"id":"IUEe2wW3lddC","colab_type":"code","colab":{}},"source":["class Zero(nn.Module):\n","  def __init__(self, stride):\n","    super(Zero, self).__init__()\n","    self.stride = stride\n","\n","  def forward(self, x):\n","    if self.stride == 1:\n","      return x.mul(0.)\n","    return x[:,:,::self.stride,::self.stride].mul(0.)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H_5KTW9GTxGp","colab_type":"code","colab":{}},"source":["class FactorizedReduce(nn.Module):\n","  def __init__(self, C_in, C_out, affine=True):\n","    super(FactorizedReduce, self).__init__()\n","    assert C_out % 2 == 0\n","    self.relu = nn.ReLU(inplace=False)\n","    self.conv_1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n","    self.conv_2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False) \n","    self.bn = nn.BatchNorm2d(C_out, affine=affine)\n","\n","  def forward(self, x):\n","    x = self.relu(x)\n","    # strideの偶奇による情報ロスを防ぐ\n","    out = torch.cat([self.conv_1(x), self.conv_2(x[:,:,1:,1:])], dim=1)\n","    out = self.bn(out)\n","    return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZYRjWX1_F-WL","colab_type":"code","colab":{}},"source":["class ReLUConvBN(nn.Module):\n","\n","  def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n","    super(ReLUConvBN, self).__init__()\n","    self.op = nn.Sequential(\n","      nn.ReLU(inplace=False),\n","      nn.Conv2d(C_in, C_out, kernel_size, stride=stride, padding=padding, bias=False),\n","      nn.BatchNorm2d(C_out, affine=affine)\n","    )\n","\n","  def forward(self, x):\n","    return self.op(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ELnkSTWdgCbm","colab_type":"code","colab":{}},"source":["class DilConv(nn.Module):\n","    \n","  def __init__(self, C_in, C_out, kernel_size, stride, padding, dilation, affine=True):\n","    super(DilConv, self).__init__()\n","    self.op = nn.Sequential(\n","      nn.ReLU(inplace=False),\n","      nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=C_in, bias=False),\n","      nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),\n","      nn.BatchNorm2d(C_out, affine=affine),\n","      )\n","\n","  def forward(self, x):\n","    return self.op(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r2LW87UogSFL","colab_type":"code","colab":{}},"source":["class SepConv(nn.Module):\n","    \n","  def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n","    super(SepConv, self).__init__()\n","    self.op = nn.Sequential(\n","      nn.ReLU(inplace=False),\n","      nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, padding=padding, groups=C_in, bias=False),\n","      nn.Conv2d(C_in, C_in, kernel_size=1, padding=0, bias=False),\n","      nn.BatchNorm2d(C_in, affine=affine),\n","      nn.ReLU(inplace=False),\n","      nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=1, padding=padding, groups=C_in, bias=False),\n","      nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),\n","      nn.BatchNorm2d(C_out, affine=affine),\n","      )\n","\n","  def forward(self, x):\n","    return self.op(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XBw-skZelvsj","colab_type":"code","colab":{}},"source":["class Edge(nn.Module):\n","  def __init__(self, operators, theta=None):\n","    super(Edge, self).__init__()\n","    self.operators = operators\n","\n","  def forward(self, input: Tensor, theta: Tensor) -> Tensor:\n","    return sum(t * op(input) for t, op in zip(theta, self.operators))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"deRuJrGRamyP","colab_type":"code","colab":{}},"source":["class Graph():\n","  def __init__(self, node_num : int, input : int=1, output : int=1):\n","    self.node_num = node_num\n","    self.input = input\n","    self.output = output\n","    self.middle = node_num - input - output\n","    self._graph = self._make_graph(node_num)\n","    self._order = self._ordered_edges(1)\n","  \n","  def edges(self):\n","    return self._graph\n","\n","  def _ordered_edges(self, agg : int):\n","    g = self._graph\n","    r = [i for i in range(self.node_num)][self.input:-self.output]\n","    return [[idx for idx, t in enumerate(g) if t[agg] == i] for i in r]\n","\n","  # edge indices to each intermediate node\n","  def ordered_edges(self):\n","    return self._order\n","\n","  def size(self):\n","    return len(self._graph)\n","\n","  def _make_graph(self, num : int):\n","    l = [i for i in range(num - self.output)]\n","    return [(s, e) for (s, e) in itertools.combinations(l, 2) if e >= self.input]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"95bRwYB4sMCt","colab_type":"code","colab":{}},"source":["import sys\n","from graphviz import Digraph\n","\n","def plot(graph, theta, multi, filename):\n","  g = Digraph(\n","      format='png', #pdf\n","      edge_attr=dict(fontsize='20', fontname=\"times\"),\n","      node_attr=dict(style='filled', shape='rect', align='center', fontsize='20', height='0.5', width='0.5', penwidth='2', fontname=\"times\"),\n","      engine='dot')\n","  g.body.extend(['rankdir=LR'])\n","\n","  for i in range(graph.node_num):\n","    color = 'lightblue'\n","    color = 'darkseagreen2' if graph.input > i else color\n","    color = 'palegoldenrod' if graph.node_num - graph.output <= i else color\n","    g.node(str(i), fillcolor=color)\n","  \n","  for idx, (s, e) in enumerate(graph.edges()):\n","    op = [CANDIDATE[i] for i, p in enumerate(theta[idx]) if p >= 1.0]\n","    if len(op) == 0: continue\n","    g.edge(str(s), str(e), label=op[0], fillcolor=\"gray\")\n","\n","  for i in range(graph.node_num - 1 - multi, graph.node_num - 1):\n","    g.edge(str(i), str(graph.node_num - graph.output), fillcolor=\"gray\")\n","\n","  g.render(filename, view=True)\n","  return g"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6c6pY6qT_Hcc","colab_type":"text"},"source":["## Operator"]},{"cell_type":"code","metadata":{"id":"XXFettOx-yz4","colab_type":"code","colab":{}},"source":["OPS = {\n","  'none' : lambda C, stride, affine: Zero(stride),\n","  'skip_connect' : lambda C, stride, affine: nn.Identity() if stride == 1 else FactorizedReduce(C, C, affine=affine),\n","  'avg_pool_3x3' : lambda C, stride, affine: nn.Sequential(nn.AvgPool2d(3, stride=stride, padding=1, count_include_pad=False), nn.BatchNorm2d(C, affine=False)),\n","  'max_pool_3x3' : lambda C, stride, affine: nn.Sequential(nn.MaxPool2d(3, stride=stride, padding=1), nn.BatchNorm2d(C, affine=False)),\n","  'conv_3x3' : lambda C, stride, affine: nn.Conv2d(C, C, 3, stride=stride, padding=1),\n","  'conv_5x5' : lambda C, stride, affine: nn.Conv2d(C, C, 5, stride=stride, padding=2),\n","  'ReLUConvBN' : lambda C, stride, affine: ReLUConvBN(C, C, 3, stride=stride, padding=1),\n","  'sep_conv_3x3' : lambda C, stride, affine: SepConv(C, C, 3, stride, 1, affine=affine),\n","  'sep_conv_5x5' : lambda C, stride, affine: SepConv(C, C, 5, stride, 2, affine=affine),\n","  'sep_conv_7x7' : lambda C, stride, affine: SepConv(C, C, 7, stride, 3, affine=affine),\n","  'dil_conv_3x3' : lambda C, stride, affine: DilConv(C, C, 3, stride, 2, 2, affine=affine),\n","  'dil_conv_5x5' : lambda C, stride, affine: DilConv(C, C, 5, stride, 4, 2, affine=affine),\n","  'conv_7x1_1x7' : lambda C, stride, affine: nn.Sequential(\n","    nn.ReLU(inplace=False),\n","    nn.Conv2d(C, C, (1,7), stride=(1, stride), padding=(0, 3), bias=False),\n","    nn.Conv2d(C, C, (7,1), stride=(stride, 1), padding=(3, 0), bias=False),\n","    nn.BatchNorm2d(C, affine=affine)\n","    ),\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SNkCVvfykW9i","colab_type":"code","colab":{}},"source":["CANDIDATE = [\n","  'conv_3x3',\n","  'conv_5x5',\n","  # 'ReLUConvBN',\n","  'avg_pool_3x3',\n","  'max_pool_3x3',\n","  'skip_connect',\n","  'none',\n","]\n","CANDIDATE = [\n","  'none',\n","  'skip_connect',\n","  # 'avg_pool_3x3',\n","  'max_pool_3x3',\n","  'sep_conv_3x3',\n","  'sep_conv_5x5',\n","  # 'sep_conv_7x7',\n","  'dil_conv_3x3',\n","  'dil_conv_5x5',\n","  # 'conv_7x1_1x7',\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g9Rou6oTlKx-","colab_type":"text"},"source":["## module"]},{"cell_type":"code","metadata":{"id":"PTuASgaXt5YA","colab_type":"code","colab":{}},"source":["from typing import List\n","class Cell(nn.Module):\n","  def __init__(self, names, graph, cs, multi, reduces=[False], search=True):\n","    super(Cell, self).__init__()\n","    assert len(cs) == len(reduces) + 1\n","    c = cs[-1]\n","    self.reduce = reduces[-1]\n","    self.pres = nn.ModuleList([\n","      FactorizedReduce(c_prev, c, affine=False) if reduce else\n","      ReLUConvBN(c_prev, c, 1, 1, 0, affine=False)\n","      for c_prev, reduce in zip(cs[:-1], reduces[:-1] + [False])\n","    ])\n","\n","    self.search = search\n","    self.graph = graph\n","    self.multi = multi\n","    self.edges = nn.ModuleList(\n","      [Edge(self._mix_operators(names, c, r)) for r in self.graph.edges()]\n","    )\n","\n","  def _mix_operators(self, names, c, r):\n","    modules = []\n","    stride = 2 if self.reduce and r[0] < self.graph.input else 1\n","    affine = not self.search\n","    for name in names:\n","      modules += [OPS[name](c, stride, affine)]\n","    return nn.ModuleList(modules)\n","\n","  def forward(self, inputs: List[Tensor], theta: Tensor) -> Tensor:\n","    nodes = [p(i) for p, i in zip(self.pres, inputs)]\n","\n","    refs = self.graph.edges()\n","    for idc in self.graph.ordered_edges():\n","      output = sum(self.edges[idx](nodes[refs[idx][0]], theta[idx]) for idx in idc)\n","      nodes += [output]\n","    \n","    return torch.cat(nodes[-self.multi:], dim=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F96DEDZt068O","colab_type":"code","colab":{}},"source":["\n","# TODO : check point (epoch)\n","# TODO : show layer shape\n","# TODO : replace to networkx\n","class CellNetwork(nn.Module):\n","  def __init__(self, depth = 4, node_num = 4, input = 1, init_channel = 16, class_num = 10, multi = -1, search = True):\n","    super(CellNetwork, self).__init__()\n","    self.depth = depth\n","    self.search = search\n","    self.onehot = False\n","    self.graph = Graph(node_num, input)\n","    self.multi = self.graph.middle if multi <= 0 else multi\n","    self.init_modules(3, self.multi, class_num, init_channel)\n","    self.init_theta()\n","    \n","    print(self.graph.edges())\n","    print(CANDIDATE)\n","\n","  def is_reduce(self, idx):\n","    return self.depth//3 == idx or 2*self.depth//3 == idx\n","\n","  def init_theta(self, zero=True, delta=1e-3):\n","    if zero:\n","      normal_theta = torch.zeros(self.graph.size(), len(CANDIDATE), requires_grad=True)\n","      reduce_theta = torch.zeros(self.graph.size(), len(CANDIDATE), requires_grad=True)\n","    else:\n","      normal_theta = delta * torch.randn(self.graph.size(), len(CANDIDATE), requires_grad=True)\n","      reduce_theta = delta * torch.randn(self.graph.size(), len(CANDIDATE), requires_grad=True)\n","    self.thetas = [normal_theta, reduce_theta]\n","\n","  def init_modules(self, c, multi, class_num, ini_c):\n","    # stem\n","    # c_n = c * multi\n","    c_n = ini_c\n","    self.stem = nn.Sequential(\n","      nn.Conv2d(c, c_n, 3, padding=1, bias=False),\n","      nn.BatchNorm2d(c_n)\n","    )\n","    c = ini_c\n","\n","    # cells\n","    input = self.graph.input\n","    reduces = [False for _ in range(input)]\n","    cs = [c_n for _ in range(input)]\n","    self.cells = nn.ModuleList()\n","    for i in range(self.depth):\n","      reduces += [self.is_reduce(i)]\n","      if self.is_reduce(i):\n","        c *= 2\n","      \n","      cell = Cell(CANDIDATE, self.graph, cs[-input:] + [c], multi, reduces=reduces[-input:], search=self.search)\n","      self.cells += [cell]\n","      cs += [multi * c]\n","    \n","    # classify\n","    self.pooling = nn.AdaptiveAvgPool2d(1)\n","    self.linear = nn.Linear(cs[-1], class_num)\n","\n","  def forward(self, input) -> Tensor:\n","    s = self.stem(input)\n","    input_num = self.graph.input\n","    stas = [s] * input_num\n","\n","    for idx, cell in enumerate(self.cells):\n","      theta = self.thetas[1] if cell.reduce else self.thetas[0]\n","      weights = theta if self.onehot else F.softmax(theta, dim=-1)\n","      s = cell(stas[-input_num:], weights)\n","      stas += [s]\n","\n","    out = self.pooling(s)\n","    return self.linear(out.view(out.size(0), -1))\n","  \n","  def to(self, *args, **kwargs):\n","    with torch.no_grad():\n","      for idx, theta in enumerate(self.thetas):\n","        self.thetas[idx] = theta.to(*args, **kwargs)\n","\n","    return super(CellNetwork, self).to(*args, **kwargs)\n","\n","  def learn_theta(self, is_learn: bool):\n","    for theta in self.thetas:\n","      theta.requires_grad = is_learn\n","\n","  def sampling(self, inplace=True):\n","    def _sampling(theta, degree=2, graph=self.graph, ignore=CANDIDATE.index('none')):\n","      with torch.no_grad():\n","        for t in theta:\n","          t[ignore] = t.min(0).values\n","          max = t.max(0)\n","          t[:] = 0.0\n","          t[max.indices] = max.values\n","\n","        for edges in graph.ordered_edges():\n","          values = [(e, theta[e].argmax(), theta[e].max(0).values) for e in edges]\n","          select = min(len(values), degree)\n","          edges = sorted(values, key=lambda x: -x[2])[:select]\n","          \n","          for (e, o, v) in values:\n","            theta[e][o] = 0.0\n","          for (e, o, v) in edges:\n","            theta[e][o] = 1.0\n","            \n","    if inplace:\n","      for theta in self.thetas:\n","        _sampling(theta)\n","        \n","      self.learn_theta(False)\n","      self.onehot = True\n","    else:\n","      thetas = [theta.detach().clone() for theta in self.thetas]\n","      for theta in thetas:\n","        _sampling(theta)\n","      return thetas\n","  \n","  def plot(self, prefix):\n","    thetas = self.sampling(inplace=False)\n","    return [plot(self.graph, theta, self.multi, prefix + name)\n","      for theta, name in zip(thetas, [\"normal\", \"reduce\"])]\n","\n","  def log(self):\n","    print(\"Network\")\n","    for theta in self.thetas:\n","      print(theta)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m2K-51qdYUJu","colab_type":"text"},"source":["# Unit Test"]},{"cell_type":"code","metadata":{"id":"psHgz5EKnAdA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"executionInfo":{"status":"ok","timestamp":1596727948977,"user_tz":-540,"elapsed":4215,"user":{"displayName":"Stand Rights","photoUrl":"","userId":"00208011058625639860"}},"outputId":"db97655a-71da-4f34-8dc8-97aaf6b44925"},"source":["import unittest\n","\n","def tensor_equal(x, y):\n","  return (torch.sum(x == y) == x.view(-1).shape[0]).item()\n","\n","class TestEdge(unittest.TestCase):\n","  def test_id(self):\n","    input = torch.randn(1, 3, 32, 32)\n","    operators = [nn.Identity(), None]\n","    model = Edge(operators, theta=torch.tensor([1.0, 0.0]))\n","    output = model(input)\n","    self.assertEqual(tensor_equal(input, output), True)\n","\n","if __name__ == '__main__':\n","    unittest.main(argv=['first-arg-is-ignored'], exit=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["E\n","======================================================================\n","ERROR: test_id (__main__.TestEdge)\n","----------------------------------------------------------------------\n","Traceback (most recent call last):\n","  File \"<ipython-input-14-7c4fb3f2b357>\", line 11, in test_id\n","    output = model(input)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n","    result = self.forward(*input, **kwargs)\n","TypeError: forward() missing 1 required positional argument: 'theta'\n","\n","----------------------------------------------------------------------\n","Ran 1 test in 0.016s\n","\n","FAILED (errors=1)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"TZoZKiScYCUK","colab_type":"text"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"4hjG8JhgNykr","colab_type":"text"},"source":["## training"]},{"cell_type":"code","metadata":{"id":"dVXnK1V40BVT","colab_type":"code","colab":{}},"source":["def train(model, device, train_loader, optimizer, optimizerB, criterion, logger, class_array):\n","    model.train()\n","    class_array = torch.LongTensor(class_array).to(device)\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        if optimizerB != None: optimizerB.zero_grad()\n","        output = model(data)\n","        reg = torch.zeros(output.shape[0], 10).to(device)\n","        reg.index_add_(1, class_array, output)\n","        loss = criterion(reg, target)\n","        loss.backward(retain_graph=True)\n","        optimizer.step()\n","        if optimizerB != None: optimizerB.step()\n","        \n","    return (None, loss.item())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NwZroO640CRI","colab_type":"code","colab":{}},"source":["def test(model, device, test_loader, criterion, logger, class_array):\n","    model.eval()\n","    test_loss = []\n","    correct = 0\n","    class_array = torch.tensor(class_array).to(device)\n","    with torch.no_grad():\n","        for batch_idx, (data, target) in enumerate(test_loader):\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss.append(criterion(output, target).item())\n","            pred = class_array[output.argmax(dim=1, keepdim=True)]\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","            \n","    test_loss = np.mean(np.array(test_loss))\n","    accuracy = 100. * correct / len(test_loader.dataset)\n","    \n","    return (None, (test_loss, accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-nlV1X9RN5MB","colab_type":"text"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"abKn8iiU1cxm","colab_type":"code","colab":{}},"source":["from argparse import Namespace\n","def dictspace(f):\n","  def inner(**kwds):\n","    return f(Namespace(**kwds))\n","  return inner"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hi9WNHMpdxk3","colab_type":"code","colab":{}},"source":["class Store():\n","  def __init__(self, dir=\"result\", name=\"log\"):\n","    self.dict = {}\n","    self.dir = dir\n","  \n","  def add(self, name, value):\n","    if not name in self.dict:\n","      self.dict[name] = []\n","    self.dict[name].append(value)\n","  \n","  def save(self, name=\"log\"):\n","    path = os.path.join(self.dir, name + \".txt\")\n","    with open(path, mode='w') as f:\n","      f.write(\"%s\" % self.dict)\n","\n","  def save_fig(self, metrix=\"acc\", xlabel=\"epochs\", ylabel=\"accuracy[%]\"):\n","    import matplotlib.pyplot as plt\n","    \n","    times = len(self.dict[metrix])\n","    fig = plt.figure()\n","    plt.plot(np.arange(times), self.dict[metrix])\n","    plt.xlabel(xlabel)\n","    plt.ylabel(ylabel)\n","    plt.legend()\n","    plt.show()\n","    fig.savefig(os.path.join(self.dir, \"%s_%d.png\" % (metrix, times)))\n","\n","  def __repr__(self):\n","    return \"store in %s\" % self.dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DarkkI7gat_9","colab_type":"code","colab":{}},"source":["def SaveModel(name, model, dir=\"result\"):\n","  path = os.path.join(dir, name + \".pt\")\n","  if not os.path.exists(dir):\n","    os.mkdir(dir)\n","\n","  torch.save(model.state_dict(), path)\n","  # print(os.path.join(\"/content\", result_path))\n","  # files.download(os.path.join(\"/content\", result_path))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fImVHate1k1n","colab_type":"code","colab":{}},"source":["def load_dataset(train=2000, test=500):\n","  #画像の変形処理\n","  transform = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n","  ])\n","\n","  transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","  ])\n","\n","  #CIFAR-10のtrain, testsetのロード\n","  trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                          download=True, transform=transform)\n","  testset = torchvision.datasets.CIFAR10(root='./data', train=False, \n","                                          download=True, transform=transform_test)\n","  \n","  trainset, _ = torch.utils.data.random_split(trainset, [train, 50000-train])\n","  testset, _ = torch.utils.data.random_split(testset, [test, 10000-test])\n","  return trainset, testset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G0ELeoz_26D7","colab_type":"code","colab":{}},"source":["class EarlyStopping:\n","  def __init__(self, dir=\"min\", patent=5):\n","    self.list = []\n","    self.best = 0\n","    self.patent = patent\n","    self.count = 0\n","    self.order = dir == \"max\"\n","\n","  def step(self, item):\n","    def _score(item):\n","      return item * (1 if self.order else -1)\n","\n","    if len(self.list) == 0:\n","      self.best = _score(item)\n","\n","    self.list.append(item)\n","    item = _score(item)\n","    if self.best < item:\n","      self.best = item\n","      self.count = 0\n","    else:\n","      self.count += 1\n","\n","  def is_stop(self):\n","    return self.patent <= self.count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rd3cRtNPoQ2e","colab_type":"code","colab":{}},"source":["def setup_drive():\n","  # Install a Drive FUSE wrapper.\n","  # https://github.com/astrada/google-drive-ocamlfuse\n","  !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","  !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","  !apt-get update -qq 2>&1 > /dev/null\n","  !apt-get -y install -qq google-drive-ocamlfuse fuse\n","\n","  # Generate auth tokens for Colab\n","  from google.colab import auth\n","  auth.authenticate_user()\n","\n","  # Generate creds for the Drive FUSE library.\n","  from oauth2client.client import GoogleCredentials\n","  creds = GoogleCredentials.get_application_default()\n","  import getpass\n","  !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","  vcode = getpass.getpass()\n","  !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4UmGYC3FpKBy","colab_type":"code","colab":{}},"source":["def save_dir(dir : str, drivepath = './drive/My Drive/ml'):\n","  if not \"getpass\" in sys.modules:\n","    setup_drive()\n","\n","  if dir:\n","    import subprocess\n","    res = subprocess.run([\"cp\", \"-r\", \"./\" + dir, drivepath], stdout=subprocess.PIPE)\n","    sys.stdout.write(res.stdout)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZR_CdGmoz-Ml","colab_type":"code","colab":{}},"source":["def get_time():\n","  import datetime\n","  import pytz\n","  dt_now = datetime.datetime.now(pytz.timezone('Asia/Tokyo'))\n","  return dt_now.strftime('%Y_%m_%d %H_%M %S')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oesY_H1Iy315","colab_type":"code","colab":{}},"source":["def save_heatmap(data, path):\n","  import seaborn as sns\n","  import matplotlib.pyplot as plt\n","  \n","  plt.figure()\n","  sns.heatmap(data, annot=True, fmt=\"1.2f\")\n","  plt.savefig(path)\n","  plt.close('all')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"16TLqrRKN815","colab_type":"text"},"source":["## main"]},{"cell_type":"code","metadata":{"id":"Hhjb1T4N0uvD","colab_type":"code","colab":{}},"source":["def main(description, **kwarg):\n","\n","  save_dir(\"\")\n","\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","  print(\"device is %s\" % device)\n","\n","  trainset, testset = load_dataset()\n","  kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n","\n","  class_array = [i for i in range(10)]\n","\n","  @dictspace\n","  def learning(args):\n","\n","    store = Store(dir=args.dir)\n","    store.add(\"kwargs\", args)\n","    theta_log = Store(dir=args.dir)\n","\n","    # instantiate\n","    networkarg = {\"depth\" : args.depth, \"node_num\" : args.node,\n","                  \"input\" : args.input, \"multi\" : args.multi}\n","    model = CellNetwork(**networkarg)\n","    model.to(device)\n","\n","    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=3e-4)\n","    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0.0001)\n","    optimizer_theta = optim.Adam(model.thetas, lr=args.lr_theta, betas=(0.5, 0.999), weight_decay=1e-3)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    trainset, testset = load_dataset(train=args.train_size)\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=False, **kwargs)\n","    testloader = torch.utils.data.DataLoader(testset, batch_size=args.batch_size, shuffle=False, **kwargs)\n","    \n","    # architecture search\n","    time_sta = time.time()\n","    accuracy, loss = 0, 1e10\n","    model.learn_theta(False)\n","    for epoch in range(args.epochs):\n","      \n","      if epoch == args.switch:\n","        model.learn_theta(True)\n","        \n","      # model.log()\n","\n","      (_, loss_train) = train(model, device, trainloader, optimizer, optimizer_theta, criterion, None, class_array)\n","      (_, (loss_test, acc)) = test(model, device, testloader, criterion, None, class_array)\n","      scheduler.step()\n","\n","      print('epoch %d, acc %s' % (epoch, acc))\n","      if epoch >= args.switch and epoch % 5 == 0:\n","        model.plot(os.path.join(args.dir, \"epoch%d_\" % epoch))\n","        if args.save_dir: save_dir(args.dir)\n","      store.add(\"loss\", loss_test)\n","      store.add(\"acc\", acc)\n","      theta_log.add(\"theta_n\", model.thetas[0].detach().cpu().clone().numpy())\n","      theta_log.add(\"theta_r\", model.thetas[1].detach().cpu().clone().numpy())\n","      save_heatmap(model.thetas[0].detach().cpu().clone().numpy(), os.path.join(args.dir, \"theta_%s.png\" % epoch))\n","\n","      accuracy, loss = acc, loss_test\n","      if time.time() - time_sta >= 60 * args.minutes:\n","        break \n","\n","    print(\"\\naccuracy \", accuracy, end=\", \")\n","    print(\"loss \", loss)\n","    model.plot(os.path.join(args.dir, \"complete_\"))\n","    SaveModel(\"cell\", model, dir=args.dir)\n","\n","    # relearning\n","    stop = EarlyStopping(patent=15)\n","    accuracy, loss = 0, 1e10\n","    remodel = CellNetwork(search=False, **networkarg).to(device)\n","    remodel.thetas = model.sampling(inplace=False)\n","    remodel.sampling()\n","    model = remodel\n","    optimizer = optim.SGD(model.parameters(), lr=args.relr, momentum=args.momentum, weight_decay=3e-4)\n","    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0.0001)\n","    for epoch in range(args.epochs):\n","      \n","      (_, loss_train) = train(model, device, trainloader, optimizer, None, criterion, None, class_array)\n","      (_, (loss_test, acc)) = test(model, device, testloader, criterion, None, class_array)\n","      scheduler.step()\n","\n","      print('epoch %d, acc %s' % (epoch, acc))\n","      store.add(\"loss\", loss_test)\n","      store.add(\"acc\", acc)\n","\n","      accuracy, loss = acc, loss_test\n","      stop.step(loss_test)\n","      if stop.is_stop():\n","        break\n","\n","    print(\"\\naccuracy \", accuracy, end=\", \")\n","    print(\"loss \", loss)\n","    SaveModel(\"cell\", model, dir=args.dir)\n","    print(store)\n","    store.save()\n","    theta_log.save(name=\"theta\")\n","    store.save_fig()\n","    if args.save_dir: save_dir(args.dir)\n","\n","    del model\n","\n","    return loss\n","\n","  kwarg['dir'] = get_time()\n","  if not os.path.exists(kwarg['dir']):\n","    os.mkdir(kwarg['dir'])\n","  learning(**kwarg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1vSuEIjIOWtQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"94b9d023-a1f5-49d5-8476-720768a58055"},"source":["if __name__ == '__main__':\n","  # # paper\n","  # main(\"\", lr=0.0250, lr_theta=0.0003, relr=0.0010, batch_size=64, train_size=20000, momentum=0.9, \n","  #          epochs=50, switch=0, minutes=180, depth=8, node=7, multi=4, input=2, save_dir=True)\n","\n","  # main(\"\", lr=0.0100, lr_theta=0.0002, batch_size=64, train_size=20000, momentum=0.9, \n","  #          epochs=90, switch=30, minutes=180, depth=5, node=7, multi=2)\n","  # main(\"\", lr=0.0100, lr_theta=0.0002, batch_size=64, train_size=20000, momentum=0.9, \n","  #          epochs=90, switch=10, minutes=180, depth=5, node=7, multi=2)\n","  # main(\"\", lr=0.0050, lr_theta=0.0005, batch_size=64, train_size=4000, momentum=0.9, \n","  #          epochs=15, switch=5, minutes=180, depth=4, node=4, multi=2)\n","  # annealing cosine scheduler no restart\n","  # main(\"\", lr=0.0250, lr_theta=0.0002, relr=0.0050, batch_size=64, train_size=1000, momentum=0.9, \n","  #          epochs=10, switch=0, minutes=180, depth=5, node=7, multi=4, input=2, save_dir=True)\n","  main(\"\", lr=0.0250, lr_theta=0.0002, relr=0.0050, batch_size=64, train_size=20000, momentum=0.9, \n","           epochs=50, switch=10, minutes=180, depth=6, node=7, multi=-1, input=2, save_dir=True)\n","\n","  # main(\"\", lr=0.0100, lr_theta=0.0002, relr=0.0010, batch_size=64, train_size=20000, momentum=0.9, \n","  #          epochs=50, switch=10, minutes=180, depth=5, node=7, multi=4, input=2, save_dir=True)\n","  # main(\"\", lr=0.0080, lr_theta=0.0001, relr=0.0010, batch_size=64, train_size=4000, momentum=0.9, \n","  #          epochs=30, switch=10, minutes=180, depth=5, node=7, multi=4, input=2, save_dir=True)\n","  # main(\"\", lr=0.0080, lr_theta=0.0001, relr=0.0010, batch_size=64, train_size=400, momentum=0.9, \n","  #          epochs=1, switch=0, minutes=180, depth=3, node=7, multi=4, input=2, save_dir=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["device is cuda\n","Files already downloaded and verified\n","Files already downloaded and verified\n","[(0, 2), (0, 3), (0, 4), (0, 5), (1, 2), (1, 3), (1, 4), (1, 5), (2, 3), (2, 4), (2, 5), (3, 4), (3, 5), (4, 5)]\n","['none', 'skip_connect', 'max_pool_3x3', 'sep_conv_3x3', 'sep_conv_5x5', 'dil_conv_3x3', 'dil_conv_5x5']\n","Files already downloaded and verified\n","Files already downloaded and verified\n","epoch 0, acc 48.8\n","epoch 1, acc 56.2\n","epoch 2, acc 52.2\n","epoch 3, acc 64.4\n","epoch 4, acc 62.8\n","epoch 5, acc 69.2\n","epoch 6, acc 69.6\n","epoch 7, acc 72.8\n","epoch 8, acc 73.4\n","epoch 9, acc 72.2\n","epoch 10, acc 70.8\n","epoch 11, acc 75.8\n","epoch 12, acc 76.4\n","epoch 13, acc 77.8\n","epoch 14, acc 74.2\n","epoch 15, acc 78.0\n","epoch 16, acc 75.6\n","epoch 17, acc 78.8\n","epoch 18, acc 76.8\n","epoch 19, acc 76.8\n","epoch 20, acc 79.6\n","epoch 21, acc 77.4\n","epoch 22, acc 78.0\n","epoch 23, acc 79.8\n","epoch 24, acc 77.6\n","epoch 25, acc 79.0\n","epoch 26, acc 82.4\n","epoch 27, acc 81.4\n","epoch 28, acc 80.8\n","epoch 29, acc 82.0\n","epoch 30, acc 81.8\n","epoch 31, acc 82.2\n","epoch 32, acc 83.6\n","epoch 33, acc 82.8\n","epoch 34, acc 84.0\n","epoch 35, acc 83.6\n","epoch 36, acc 84.6\n","epoch 37, acc 83.4\n","epoch 38, acc 84.0\n","epoch 39, acc 83.0\n","epoch 40, acc 85.2\n","epoch 41, acc 85.6\n","epoch 42, acc 85.0\n","epoch 43, acc 84.8\n","epoch 44, acc 85.4\n","epoch 45, acc 84.8\n","epoch 46, acc 85.4\n","epoch 47, acc 85.0\n","epoch 48, acc 84.8\n","epoch 49, acc 84.6\n","\n","accuracy  84.6, loss  0.7535911500453949\n","[(0, 2), (0, 3), (0, 4), (0, 5), (1, 2), (1, 3), (1, 4), (1, 5), (2, 3), (2, 4), (2, 5), (3, 4), (3, 5), (4, 5)]\n","['none', 'skip_connect', 'max_pool_3x3', 'sep_conv_3x3', 'sep_conv_5x5', 'dil_conv_3x3', 'dil_conv_5x5']\n","epoch 0, acc 37.6\n","epoch 1, acc 48.8\n","epoch 2, acc 53.0\n","epoch 3, acc 52.2\n","epoch 4, acc 58.0\n","epoch 5, acc 60.6\n","epoch 6, acc 62.8\n","epoch 7, acc 65.2\n","epoch 8, acc 65.4\n","epoch 9, acc 70.0\n","epoch 10, acc 69.0\n","epoch 11, acc 71.2\n","epoch 12, acc 68.8\n","epoch 13, acc 70.6\n","epoch 14, acc 72.8\n","epoch 15, acc 73.2\n","epoch 16, acc 71.6\n","epoch 17, acc 71.8\n","epoch 18, acc 72.4\n","epoch 19, acc 71.8\n","epoch 20, acc 72.2\n","epoch 21, acc 72.0\n","epoch 22, acc 74.0\n","epoch 23, acc 73.2\n","epoch 24, acc 73.2\n","epoch 25, acc 73.2\n","epoch 26, acc 75.0\n","epoch 27, acc 75.8\n","epoch 28, acc 74.0\n","epoch 29, acc 74.6\n","epoch 30, acc 76.0\n","epoch 31, acc 76.8\n","epoch 32, acc 75.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5_9lzCS6oeWk","colab_type":"text"},"source":["## viz"]},{"cell_type":"code","metadata":{"id":"HiSgQDMVsIXq","colab_type":"code","colab":{}},"source":["# # cause a crash to increase RAM\n","# [_ for _ in range(10000000000)]"],"execution_count":null,"outputs":[]}]}