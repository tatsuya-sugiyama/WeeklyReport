{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #基本モジュール\n",
    "from torch.autograd import Variable #自動微分用\n",
    "import torch.nn as nn #ネットワーク構築用\n",
    "import torch.optim as optim #最適化関数\n",
    "import torch.nn.functional as F #ネットワーク用の様々な関数\n",
    "import torch.utils.data #データセット読み込み関連\n",
    "import torchvision #画像関連\n",
    "from torchvision import datasets, models, transforms #画像用データセット諸々\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#画像の変形処理\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#CIFAR-10のtrain, testsetのロード\n",
    "#変形はtransformを適用\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "#DataLoaderの適用->これによりバッチの割り当て・シャッフルをまとめて行うことができる\n",
    "#batch_sizeでバッチサイズを指定\n",
    "#num_workersでいくつのコアでデータをロードするか指定(デフォルトはメインのみ)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "# input [100] reshape?\n",
    "# layer [256, 4, 4] conv\n",
    "# layer [128, 8, 8] conv\n",
    "# layer [64, 16, 16]\n",
    "# output [3, 32, 32] tanh\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.pc1 = nn.Linear(100, 256 * 4 * 4)\n",
    "        conv1 = nn.ConvTranspose2d(256, 128, 5, stride=2, padding=2, output_padding=1)\n",
    "        conv2 = nn.ConvTranspose2d(128, 64, 5, stride=2, padding=2, output_padding=1)\n",
    "        conv3 = nn.ConvTranspose2d(64, 3, 5, stride=2, padding=2, output_padding=1)\n",
    "        relu = nn.LeakyReLU()\n",
    "        batch1 =  nn.BatchNorm2d(128)\n",
    "        batch2 =  nn.BatchNorm2d(64)\n",
    "        batch3 =  nn.BatchNorm2d(3)\n",
    "        out = nn.Tanh()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            conv1, batch1, relu, conv2, batch2, relu, conv3, batch3, out\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.pc1(z).view(-1, 256, 4, 4)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        conv1 = nn.Conv2d(3, 3, 5, stride=2, padding=2)\n",
    "        conv2 = nn.Conv2d(3, 3, 5, stride=2, padding=2)\n",
    "        conv3 = nn.Conv2d(3, 3, 5, stride=2, padding=2)\n",
    "        pc = nn.Conv2d(3, 1, 4)\n",
    "        relu = nn.LeakyReLU()\n",
    "        batch1 =  nn.BatchNorm2d(3)\n",
    "        batch2 =  nn.BatchNorm2d(3)\n",
    "        out = nn.Sigmoid()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            conv1, batch1, relu, conv2, batch2, relu, conv3, pc, out\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデル定義\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "#Loss関数の指定\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "#Optimizerの指定\n",
    "optimizer_g = optim.Adam(generator.parameters())\n",
    "optimizer_d = optim.Adam(discriminator.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch0/3125, loss0.351315 vs 1.028077\n",
      "epoch 0, batch1/3125, loss0.369206 vs 1.006213\n",
      "epoch 0, batch2/3125, loss0.350297 vs 1.030429\n",
      "epoch 0, batch3/3125, loss0.367223 vs 0.985919\n",
      "epoch 0, batch4/3125, loss0.394143 vs 0.993026\n",
      "epoch 0, batch5/3125, loss0.387170 vs 0.972744\n",
      "epoch 0, batch6/3125, loss0.368839 vs 0.997858\n",
      "epoch 0, batch7/3125, loss0.409399 vs 0.966508\n",
      "epoch 0, batch8/3125, loss0.412364 vs 0.944446\n",
      "epoch 0, batch9/3125, loss0.458398 vs 0.922321\n",
      "epoch 0, batch10/3125, loss0.414565 vs 0.934955\n",
      "epoch 0, batch11/3125, loss0.416813 vs 0.932097\n",
      "epoch 0, batch12/3125, loss0.445249 vs 0.918775\n",
      "epoch 0, batch13/3125, loss0.445315 vs 0.924134\n",
      "epoch 0, batch14/3125, loss0.458750 vs 0.901384\n",
      "epoch 0, batch15/3125, loss0.460268 vs 0.902660\n",
      "epoch 0, batch16/3125, loss0.486586 vs 0.880614\n",
      "epoch 0, batch17/3125, loss0.493724 vs 0.870169\n",
      "epoch 0, batch18/3125, loss0.486443 vs 0.883592\n",
      "epoch 0, batch19/3125, loss0.515550 vs 0.860184\n",
      "epoch 0, batch20/3125, loss0.521310 vs 0.853265\n",
      "epoch 0, batch21/3125, loss0.535740 vs 0.851778\n",
      "epoch 0, batch22/3125, loss0.539760 vs 0.843860\n",
      "epoch 0, batch23/3125, loss0.539205 vs 0.849092\n",
      "epoch 0, batch24/3125, loss0.544197 vs 0.846133\n",
      "epoch 0, batch25/3125, loss0.566848 vs 0.825232\n",
      "epoch 0, batch26/3125, loss0.574958 vs 0.819776\n",
      "epoch 0, batch27/3125, loss0.574837 vs 0.814028\n",
      "epoch 0, batch28/3125, loss0.587985 vs 0.809901\n",
      "epoch 0, batch29/3125, loss0.597199 vs 0.808644\n",
      "epoch 0, batch30/3125, loss0.605470 vs 0.802386\n",
      "epoch 0, batch31/3125, loss0.612867 vs 0.805804\n",
      "epoch 0, batch32/3125, loss0.627317 vs 0.789257\n",
      "epoch 0, batch33/3125, loss0.629605 vs 0.793468\n",
      "epoch 0, batch34/3125, loss0.643785 vs 0.782054\n",
      "epoch 0, batch35/3125, loss0.651625 vs 0.780070\n",
      "epoch 0, batch36/3125, loss0.656521 vs 0.772045\n",
      "epoch 0, batch37/3125, loss0.665387 vs 0.771698\n",
      "epoch 0, batch38/3125, loss0.669328 vs 0.770578\n",
      "epoch 0, batch39/3125, loss0.680810 vs 0.759964\n",
      "epoch 0, batch40/3125, loss0.684035 vs 0.754248\n",
      "epoch 0, batch41/3125, loss0.685577 vs 0.765016\n",
      "epoch 0, batch42/3125, loss0.684149 vs 0.765549\n",
      "epoch 0, batch43/3125, loss0.685159 vs 0.758523\n",
      "epoch 0, batch44/3125, loss0.682506 vs 0.761379\n",
      "epoch 0, batch45/3125, loss0.682005 vs 0.761585\n",
      "epoch 0, batch46/3125, loss0.676719 vs 0.765833\n",
      "epoch 0, batch47/3125, loss0.675042 vs 0.772146\n",
      "epoch 0, batch48/3125, loss0.680668 vs 0.760145\n",
      "epoch 0, batch49/3125, loss0.674308 vs 0.764818\n",
      "epoch 0, batch50/3125, loss0.673315 vs 0.766209\n",
      "epoch 0, batch51/3125, loss0.669910 vs 0.764076\n",
      "epoch 0, batch52/3125, loss0.669172 vs 0.767246\n",
      "epoch 0, batch53/3125, loss0.660353 vs 0.773849\n",
      "epoch 0, batch54/3125, loss0.659001 vs 0.763861\n",
      "epoch 0, batch55/3125, loss0.657519 vs 0.761748\n",
      "epoch 0, batch56/3125, loss0.653464 vs 0.767053\n",
      "epoch 0, batch57/3125, loss0.655669 vs 0.769425\n",
      "epoch 0, batch58/3125, loss0.654098 vs 0.770722\n",
      "epoch 0, batch59/3125, loss0.657602 vs 0.767910\n",
      "epoch 0, batch60/3125, loss0.660775 vs 0.761949\n",
      "epoch 0, batch61/3125, loss0.659213 vs 0.765203\n",
      "epoch 0, batch62/3125, loss0.662968 vs 0.766124\n",
      "epoch 0, batch63/3125, loss0.665763 vs 0.760540\n",
      "epoch 0, batch64/3125, loss0.667004 vs 0.763090\n",
      "epoch 0, batch65/3125, loss0.668871 vs 0.756329\n",
      "epoch 0, batch66/3125, loss0.670268 vs 0.763252\n",
      "epoch 0, batch67/3125, loss0.675957 vs 0.761526\n",
      "epoch 0, batch68/3125, loss0.679361 vs 0.747074\n",
      "epoch 0, batch69/3125, loss0.682815 vs 0.749485\n",
      "epoch 0, batch70/3125, loss0.686961 vs 0.751226\n",
      "epoch 0, batch71/3125, loss0.687255 vs 0.748212\n",
      "epoch 0, batch72/3125, loss0.692606 vs 0.748499\n",
      "epoch 0, batch73/3125, loss0.692896 vs 0.745762\n",
      "epoch 0, batch74/3125, loss0.694671 vs 0.743635\n",
      "epoch 0, batch75/3125, loss0.697425 vs 0.743206\n",
      "epoch 0, batch76/3125, loss0.707247 vs 0.742318\n",
      "epoch 0, batch77/3125, loss0.704415 vs 0.738272\n",
      "epoch 0, batch78/3125, loss0.713403 vs 0.737534\n",
      "epoch 0, batch79/3125, loss0.713186 vs 0.733675\n",
      "epoch 0, batch80/3125, loss0.714116 vs 0.733493\n",
      "epoch 0, batch81/3125, loss0.722972 vs 0.720471\n",
      "epoch 0, batch82/3125, loss0.724743 vs 0.721187\n",
      "epoch 0, batch83/3125, loss0.724034 vs 0.724911\n",
      "epoch 0, batch84/3125, loss0.728603 vs 0.722428\n",
      "epoch 0, batch85/3125, loss0.725303 vs 0.726473\n",
      "epoch 0, batch86/3125, loss0.730617 vs 0.725295\n",
      "epoch 0, batch87/3125, loss0.730169 vs 0.725521\n",
      "epoch 0, batch88/3125, loss0.723899 vs 0.725739\n",
      "epoch 0, batch89/3125, loss0.726951 vs 0.720888\n",
      "epoch 0, batch90/3125, loss0.731382 vs 0.724573\n",
      "epoch 0, batch91/3125, loss0.727671 vs 0.718954\n",
      "epoch 0, batch92/3125, loss0.731148 vs 0.720958\n",
      "epoch 0, batch93/3125, loss0.730248 vs 0.718467\n",
      "epoch 0, batch94/3125, loss0.731940 vs 0.716915\n",
      "epoch 0, batch95/3125, loss0.728445 vs 0.719284\n",
      "epoch 0, batch96/3125, loss0.733695 vs 0.716829\n",
      "epoch 0, batch97/3125, loss0.731306 vs 0.718647\n",
      "epoch 0, batch98/3125, loss0.729086 vs 0.716877\n",
      "epoch 0, batch99/3125, loss0.730612 vs 0.714253\n",
      "epoch 0, batch100/3125, loss0.724390 vs 0.717382\n",
      "epoch 0, batch101/3125, loss0.724851 vs 0.721655\n",
      "epoch 0, batch102/3125, loss0.721972 vs 0.717872\n",
      "epoch 0, batch103/3125, loss0.726085 vs 0.716878\n",
      "epoch 0, batch104/3125, loss0.719629 vs 0.718699\n",
      "epoch 0, batch105/3125, loss0.714369 vs 0.718682\n",
      "epoch 0, batch106/3125, loss0.709709 vs 0.725403\n",
      "epoch 0, batch107/3125, loss0.706438 vs 0.720067\n",
      "epoch 0, batch108/3125, loss0.703089 vs 0.726374\n",
      "epoch 0, batch109/3125, loss0.692877 vs 0.724932\n",
      "epoch 0, batch110/3125, loss0.691301 vs 0.731777\n",
      "epoch 0, batch111/3125, loss0.682920 vs 0.734593\n",
      "epoch 0, batch112/3125, loss0.679870 vs 0.732328\n",
      "epoch 0, batch113/3125, loss0.672666 vs 0.735753\n",
      "epoch 0, batch114/3125, loss0.665222 vs 0.742702\n",
      "epoch 0, batch115/3125, loss0.660446 vs 0.752509\n",
      "epoch 0, batch116/3125, loss0.663377 vs 0.749318\n",
      "epoch 0, batch117/3125, loss0.662741 vs 0.745719\n",
      "epoch 0, batch118/3125, loss0.665033 vs 0.744761\n",
      "epoch 0, batch119/3125, loss0.666940 vs 0.736524\n",
      "epoch 0, batch120/3125, loss0.669381 vs 0.734580\n",
      "epoch 0, batch121/3125, loss0.670293 vs 0.743035\n",
      "epoch 0, batch122/3125, loss0.671674 vs 0.740837\n",
      "epoch 0, batch123/3125, loss0.677925 vs 0.735334\n",
      "epoch 0, batch124/3125, loss0.679601 vs 0.735694\n",
      "epoch 0, batch125/3125, loss0.680470 vs 0.734842\n",
      "epoch 0, batch126/3125, loss0.683024 vs 0.724651\n",
      "epoch 0, batch127/3125, loss0.688191 vs 0.729250\n",
      "epoch 0, batch128/3125, loss0.685669 vs 0.730429\n",
      "epoch 0, batch129/3125, loss0.690791 vs 0.726643\n",
      "epoch 0, batch130/3125, loss0.689192 vs 0.725838\n",
      "epoch 0, batch131/3125, loss0.689722 vs 0.723964\n",
      "epoch 0, batch132/3125, loss0.691696 vs 0.721561\n",
      "epoch 0, batch133/3125, loss0.693402 vs 0.721733\n",
      "epoch 0, batch134/3125, loss0.692178 vs 0.722922\n",
      "epoch 0, batch135/3125, loss0.691515 vs 0.717920\n",
      "epoch 0, batch136/3125, loss0.684132 vs 0.726050\n",
      "epoch 0, batch137/3125, loss0.686073 vs 0.722054\n",
      "epoch 0, batch138/3125, loss0.690843 vs 0.721930\n",
      "epoch 0, batch139/3125, loss0.687135 vs 0.723901\n",
      "epoch 0, batch140/3125, loss0.685085 vs 0.724918\n",
      "epoch 0, batch141/3125, loss0.687163 vs 0.718134\n",
      "epoch 0, batch142/3125, loss0.682538 vs 0.721086\n",
      "epoch 0, batch143/3125, loss0.678889 vs 0.725453\n",
      "epoch 0, batch144/3125, loss0.680889 vs 0.725571\n",
      "epoch 0, batch145/3125, loss0.678815 vs 0.724516\n",
      "epoch 0, batch146/3125, loss0.679975 vs 0.722476\n",
      "epoch 0, batch147/3125, loss0.675691 vs 0.729313\n",
      "epoch 0, batch148/3125, loss0.675524 vs 0.723802\n",
      "epoch 0, batch149/3125, loss0.676731 vs 0.723368\n",
      "epoch 0, batch150/3125, loss0.677651 vs 0.727827\n",
      "epoch 0, batch151/3125, loss0.684034 vs 0.719681\n",
      "epoch 0, batch152/3125, loss0.679937 vs 0.723183\n",
      "epoch 0, batch153/3125, loss0.682528 vs 0.720130\n",
      "epoch 0, batch154/3125, loss0.687693 vs 0.719179\n",
      "epoch 0, batch155/3125, loss0.692297 vs 0.715730\n",
      "epoch 0, batch156/3125, loss0.693692 vs 0.711210\n",
      "epoch 0, batch157/3125, loss0.698056 vs 0.710824\n",
      "epoch 0, batch158/3125, loss0.703215 vs 0.711316\n",
      "epoch 0, batch159/3125, loss0.706296 vs 0.708684\n",
      "epoch 0, batch160/3125, loss0.708366 vs 0.705014\n",
      "epoch 0, batch161/3125, loss0.711736 vs 0.706317\n",
      "epoch 0, batch162/3125, loss0.716965 vs 0.702161\n",
      "epoch 0, batch163/3125, loss0.718253 vs 0.704353\n",
      "epoch 0, batch164/3125, loss0.719610 vs 0.695174\n",
      "epoch 0, batch165/3125, loss0.723616 vs 0.696994\n",
      "epoch 0, batch166/3125, loss0.725865 vs 0.693188\n",
      "epoch 0, batch167/3125, loss0.730048 vs 0.697860\n",
      "epoch 0, batch168/3125, loss0.729774 vs 0.702123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch169/3125, loss0.727194 vs 0.692473\n",
      "epoch 0, batch170/3125, loss0.722783 vs 0.703479\n",
      "epoch 0, batch171/3125, loss0.718220 vs 0.695258\n",
      "epoch 0, batch172/3125, loss0.713538 vs 0.698288\n",
      "epoch 0, batch173/3125, loss0.707654 vs 0.709992\n",
      "epoch 0, batch174/3125, loss0.701174 vs 0.703226\n",
      "epoch 0, batch175/3125, loss0.693240 vs 0.713136\n",
      "epoch 0, batch176/3125, loss0.688583 vs 0.713919\n",
      "epoch 0, batch177/3125, loss0.678131 vs 0.723100\n",
      "epoch 0, batch178/3125, loss0.668854 vs 0.719102\n",
      "epoch 0, batch179/3125, loss0.659923 vs 0.723920\n",
      "epoch 0, batch180/3125, loss0.652393 vs 0.718799\n",
      "epoch 0, batch181/3125, loss0.641119 vs 0.734658\n",
      "epoch 0, batch182/3125, loss0.635338 vs 0.736421\n",
      "epoch 0, batch183/3125, loss0.633072 vs 0.742476\n",
      "epoch 0, batch184/3125, loss0.632770 vs 0.738190\n",
      "epoch 0, batch185/3125, loss0.634247 vs 0.747009\n",
      "epoch 0, batch186/3125, loss0.630301 vs 0.739177\n",
      "epoch 0, batch187/3125, loss0.629357 vs 0.751808\n",
      "epoch 0, batch188/3125, loss0.632941 vs 0.734642\n",
      "epoch 0, batch189/3125, loss0.632832 vs 0.742411\n",
      "epoch 0, batch190/3125, loss0.634758 vs 0.735807\n",
      "epoch 0, batch191/3125, loss0.639208 vs 0.741924\n",
      "epoch 0, batch192/3125, loss0.639997 vs 0.734912\n",
      "epoch 0, batch193/3125, loss0.645341 vs 0.730618\n",
      "epoch 0, batch194/3125, loss0.648618 vs 0.728899\n",
      "epoch 0, batch195/3125, loss0.653417 vs 0.731074\n",
      "epoch 0, batch196/3125, loss0.657059 vs 0.727212\n",
      "epoch 0, batch197/3125, loss0.659186 vs 0.724632\n",
      "epoch 0, batch198/3125, loss0.668028 vs 0.729389\n",
      "epoch 0, batch199/3125, loss0.669409 vs 0.718608\n",
      "epoch 0, batch200/3125, loss0.677037 vs 0.715030\n",
      "epoch 0, batch201/3125, loss0.681728 vs 0.720452\n",
      "epoch 0, batch202/3125, loss0.686182 vs 0.716910\n",
      "epoch 0, batch203/3125, loss0.690043 vs 0.711065\n",
      "epoch 0, batch204/3125, loss0.694003 vs 0.705356\n",
      "epoch 0, batch205/3125, loss0.696615 vs 0.708759\n",
      "epoch 0, batch206/3125, loss0.697252 vs 0.708015\n",
      "epoch 0, batch207/3125, loss0.699930 vs 0.711139\n",
      "epoch 0, batch208/3125, loss0.701238 vs 0.705967\n",
      "epoch 0, batch209/3125, loss0.702229 vs 0.709799\n",
      "epoch 0, batch210/3125, loss0.708367 vs 0.707075\n",
      "epoch 0, batch211/3125, loss0.709172 vs 0.706756\n",
      "epoch 0, batch212/3125, loss0.710671 vs 0.701742\n",
      "epoch 0, batch213/3125, loss0.714820 vs 0.703205\n",
      "epoch 0, batch214/3125, loss0.711554 vs 0.705313\n",
      "epoch 0, batch215/3125, loss0.714536 vs 0.699538\n",
      "epoch 0, batch216/3125, loss0.714417 vs 0.704628\n",
      "epoch 0, batch217/3125, loss0.712615 vs 0.693213\n",
      "epoch 0, batch218/3125, loss0.710039 vs 0.703268\n",
      "epoch 0, batch219/3125, loss0.708399 vs 0.706782\n",
      "epoch 0, batch220/3125, loss0.702073 vs 0.709174\n",
      "epoch 0, batch221/3125, loss0.702690 vs 0.705091\n",
      "epoch 0, batch222/3125, loss0.696901 vs 0.709519\n",
      "epoch 0, batch223/3125, loss0.691014 vs 0.715179\n",
      "epoch 0, batch224/3125, loss0.684975 vs 0.711896\n",
      "epoch 0, batch225/3125, loss0.689224 vs 0.713717\n",
      "epoch 0, batch226/3125, loss0.683356 vs 0.718735\n",
      "epoch 0, batch227/3125, loss0.684870 vs 0.713212\n",
      "epoch 0, batch228/3125, loss0.684887 vs 0.713753\n",
      "epoch 0, batch229/3125, loss0.686267 vs 0.713299\n",
      "epoch 0, batch230/3125, loss0.687368 vs 0.714619\n",
      "epoch 0, batch231/3125, loss0.686454 vs 0.712381\n",
      "epoch 0, batch232/3125, loss0.686222 vs 0.715101\n",
      "epoch 0, batch233/3125, loss0.690759 vs 0.705319\n",
      "epoch 0, batch234/3125, loss0.691289 vs 0.706443\n",
      "epoch 0, batch235/3125, loss0.690427 vs 0.707074\n",
      "epoch 0, batch236/3125, loss0.693333 vs 0.708011\n",
      "epoch 0, batch237/3125, loss0.693476 vs 0.703360\n",
      "epoch 0, batch238/3125, loss0.695384 vs 0.704557\n",
      "epoch 0, batch239/3125, loss0.697660 vs 0.704122\n",
      "epoch 0, batch240/3125, loss0.698774 vs 0.700164\n",
      "epoch 0, batch241/3125, loss0.698754 vs 0.701683\n",
      "epoch 0, batch242/3125, loss0.694173 vs 0.697831\n",
      "epoch 0, batch243/3125, loss0.696870 vs 0.699269\n",
      "epoch 0, batch244/3125, loss0.694002 vs 0.699779\n",
      "epoch 0, batch245/3125, loss0.691804 vs 0.699525\n",
      "epoch 0, batch246/3125, loss0.688488 vs 0.704356\n",
      "epoch 0, batch247/3125, loss0.687351 vs 0.700691\n",
      "epoch 0, batch248/3125, loss0.685660 vs 0.703775\n",
      "epoch 0, batch249/3125, loss0.681818 vs 0.702995\n",
      "epoch 0, batch250/3125, loss0.673652 vs 0.706036\n",
      "epoch 0, batch251/3125, loss0.670635 vs 0.705498\n",
      "epoch 0, batch252/3125, loss0.668257 vs 0.708541\n",
      "epoch 0, batch253/3125, loss0.668713 vs 0.707018\n",
      "epoch 0, batch254/3125, loss0.670449 vs 0.705429\n",
      "epoch 0, batch255/3125, loss0.666467 vs 0.711484\n",
      "epoch 0, batch256/3125, loss0.667256 vs 0.703449\n",
      "epoch 0, batch257/3125, loss0.663294 vs 0.710168\n",
      "epoch 0, batch258/3125, loss0.658433 vs 0.710233\n",
      "epoch 0, batch259/3125, loss0.647528 vs 0.716944\n",
      "epoch 0, batch260/3125, loss0.637886 vs 0.721014\n",
      "epoch 0, batch261/3125, loss0.632732 vs 0.727574\n",
      "epoch 0, batch262/3125, loss0.635688 vs 0.722187\n",
      "epoch 0, batch263/3125, loss0.635352 vs 0.721584\n",
      "epoch 0, batch264/3125, loss0.636471 vs 0.719201\n",
      "epoch 0, batch265/3125, loss0.638440 vs 0.718801\n",
      "epoch 0, batch266/3125, loss0.639175 vs 0.720175\n",
      "epoch 0, batch267/3125, loss0.639592 vs 0.717247\n",
      "epoch 0, batch268/3125, loss0.639976 vs 0.722788\n",
      "epoch 0, batch269/3125, loss0.641279 vs 0.719966\n",
      "epoch 0, batch270/3125, loss0.642408 vs 0.717626\n",
      "epoch 0, batch271/3125, loss0.643730 vs 0.716108\n",
      "epoch 0, batch272/3125, loss0.645828 vs 0.717013\n",
      "epoch 0, batch273/3125, loss0.645085 vs 0.717359\n",
      "epoch 0, batch274/3125, loss0.646902 vs 0.718359\n",
      "epoch 0, batch275/3125, loss0.649068 vs 0.717206\n",
      "epoch 0, batch276/3125, loss0.651763 vs 0.714620\n",
      "epoch 0, batch277/3125, loss0.653466 vs 0.710365\n",
      "epoch 0, batch278/3125, loss0.655278 vs 0.716770\n",
      "epoch 0, batch279/3125, loss0.656606 vs 0.707975\n",
      "epoch 0, batch280/3125, loss0.658571 vs 0.713312\n",
      "epoch 0, batch281/3125, loss0.659893 vs 0.710801\n",
      "epoch 0, batch282/3125, loss0.663256 vs 0.712927\n",
      "epoch 0, batch283/3125, loss0.665508 vs 0.709843\n",
      "epoch 0, batch284/3125, loss0.667110 vs 0.708217\n",
      "epoch 0, batch285/3125, loss0.668372 vs 0.711658\n",
      "epoch 0, batch286/3125, loss0.669584 vs 0.707608\n",
      "epoch 0, batch287/3125, loss0.672438 vs 0.705727\n",
      "epoch 0, batch288/3125, loss0.675471 vs 0.701772\n",
      "epoch 0, batch289/3125, loss0.674550 vs 0.705368\n",
      "epoch 0, batch290/3125, loss0.678297 vs 0.706665\n",
      "epoch 0, batch291/3125, loss0.680012 vs 0.705496\n",
      "epoch 0, batch292/3125, loss0.681438 vs 0.702577\n",
      "epoch 0, batch293/3125, loss0.682646 vs 0.702155\n",
      "epoch 0, batch294/3125, loss0.684157 vs 0.702497\n",
      "epoch 0, batch295/3125, loss0.683584 vs 0.704550\n",
      "epoch 0, batch296/3125, loss0.685909 vs 0.706416\n",
      "epoch 0, batch297/3125, loss0.687107 vs 0.702908\n",
      "epoch 0, batch298/3125, loss0.688237 vs 0.702224\n",
      "epoch 0, batch299/3125, loss0.687769 vs 0.704224\n",
      "epoch 0, batch300/3125, loss0.687060 vs 0.709510\n",
      "epoch 0, batch301/3125, loss0.687540 vs 0.702788\n",
      "epoch 0, batch302/3125, loss0.689575 vs 0.701315\n",
      "epoch 0, batch303/3125, loss0.690743 vs 0.701494\n",
      "epoch 0, batch304/3125, loss0.692731 vs 0.702068\n",
      "epoch 0, batch305/3125, loss0.694069 vs 0.703062\n",
      "epoch 0, batch306/3125, loss0.695841 vs 0.696387\n",
      "epoch 0, batch307/3125, loss0.697629 vs 0.701790\n",
      "epoch 0, batch308/3125, loss0.700149 vs 0.699217\n",
      "epoch 0, batch309/3125, loss0.701756 vs 0.696657\n",
      "epoch 0, batch310/3125, loss0.703659 vs 0.698074\n",
      "epoch 0, batch311/3125, loss0.706110 vs 0.692499\n",
      "epoch 0, batch312/3125, loss0.707695 vs 0.693301\n",
      "epoch 0, batch313/3125, loss0.710640 vs 0.691513\n",
      "epoch 0, batch314/3125, loss0.713137 vs 0.690833\n",
      "epoch 0, batch315/3125, loss0.714845 vs 0.686343\n",
      "epoch 0, batch316/3125, loss0.717010 vs 0.687756\n",
      "epoch 0, batch317/3125, loss0.718951 vs 0.689520\n",
      "epoch 0, batch318/3125, loss0.722825 vs 0.682524\n",
      "epoch 0, batch319/3125, loss0.725287 vs 0.688710\n",
      "epoch 0, batch320/3125, loss0.727080 vs 0.683651\n",
      "epoch 0, batch321/3125, loss0.730436 vs 0.683624\n",
      "epoch 0, batch322/3125, loss0.731738 vs 0.676821\n",
      "epoch 0, batch323/3125, loss0.733042 vs 0.678855\n",
      "epoch 0, batch324/3125, loss0.735449 vs 0.675763\n",
      "epoch 0, batch325/3125, loss0.736446 vs 0.682472\n",
      "epoch 0, batch326/3125, loss0.737009 vs 0.679084\n",
      "epoch 0, batch327/3125, loss0.740283 vs 0.678223\n",
      "epoch 0, batch328/3125, loss0.742258 vs 0.676159\n",
      "epoch 0, batch329/3125, loss0.743532 vs 0.672159\n",
      "epoch 0, batch330/3125, loss0.742954 vs 0.667036\n",
      "epoch 0, batch331/3125, loss0.740470 vs 0.671240\n",
      "epoch 0, batch332/3125, loss0.737438 vs 0.677675\n",
      "epoch 0, batch333/3125, loss0.739590 vs 0.672226\n",
      "epoch 0, batch334/3125, loss0.734786 vs 0.676442\n",
      "epoch 0, batch335/3125, loss0.733870 vs 0.670272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch336/3125, loss0.727612 vs 0.668810\n",
      "epoch 0, batch337/3125, loss0.725739 vs 0.678604\n",
      "epoch 0, batch338/3125, loss0.722607 vs 0.672090\n",
      "epoch 0, batch339/3125, loss0.719462 vs 0.679131\n",
      "epoch 0, batch340/3125, loss0.709117 vs 0.679688\n",
      "epoch 0, batch341/3125, loss0.710622 vs 0.683856\n",
      "epoch 0, batch342/3125, loss0.698518 vs 0.691499\n",
      "epoch 0, batch343/3125, loss0.700238 vs 0.697086\n",
      "epoch 0, batch344/3125, loss0.691933 vs 0.680220\n",
      "epoch 0, batch345/3125, loss0.685243 vs 0.692387\n",
      "epoch 0, batch346/3125, loss0.686278 vs 0.687990\n",
      "epoch 0, batch347/3125, loss0.681989 vs 0.686250\n",
      "epoch 0, batch348/3125, loss0.674850 vs 0.687187\n",
      "epoch 0, batch349/3125, loss0.671148 vs 0.692507\n",
      "epoch 0, batch350/3125, loss0.666930 vs 0.698576\n",
      "epoch 0, batch351/3125, loss0.668789 vs 0.690871\n",
      "epoch 0, batch352/3125, loss0.667391 vs 0.688939\n",
      "epoch 0, batch353/3125, loss0.667970 vs 0.692920\n",
      "epoch 0, batch354/3125, loss0.670497 vs 0.709496\n",
      "epoch 0, batch355/3125, loss0.667954 vs 0.682828\n",
      "epoch 0, batch356/3125, loss0.672247 vs 0.677212\n",
      "epoch 0, batch357/3125, loss0.671933 vs 0.686932\n",
      "epoch 0, batch358/3125, loss0.677687 vs 0.676353\n",
      "epoch 0, batch359/3125, loss0.679424 vs 0.677930\n",
      "epoch 0, batch360/3125, loss0.679801 vs 0.711325\n",
      "epoch 0, batch361/3125, loss0.684712 vs 0.663213\n",
      "epoch 0, batch362/3125, loss0.686152 vs 0.672918\n",
      "epoch 0, batch363/3125, loss0.687580 vs 0.670847\n",
      "epoch 0, batch364/3125, loss0.693075 vs 0.662498\n",
      "epoch 0, batch365/3125, loss0.692665 vs 0.689550\n",
      "epoch 0, batch366/3125, loss0.690263 vs 0.662586\n",
      "epoch 0, batch367/3125, loss0.688334 vs 0.683455\n",
      "epoch 0, batch368/3125, loss0.692097 vs 0.686369\n",
      "epoch 0, batch369/3125, loss0.691427 vs 0.662594\n",
      "epoch 0, batch370/3125, loss0.685439 vs 0.696043\n",
      "epoch 0, batch371/3125, loss0.680447 vs 0.689261\n",
      "epoch 0, batch372/3125, loss0.672776 vs 0.663920\n",
      "epoch 0, batch373/3125, loss0.663644 vs 0.679410\n",
      "epoch 0, batch374/3125, loss0.644581 vs 0.684911\n",
      "epoch 0, batch375/3125, loss0.615273 vs 0.730237\n",
      "epoch 0, batch376/3125, loss0.572143 vs 0.747139\n",
      "epoch 0, batch377/3125, loss0.529459 vs 0.787887\n",
      "epoch 0, batch378/3125, loss0.511922 vs 0.782842\n",
      "epoch 0, batch379/3125, loss0.496641 vs 0.771702\n",
      "epoch 0, batch380/3125, loss0.496901 vs 0.806988\n",
      "epoch 0, batch381/3125, loss0.503996 vs 0.792998\n",
      "epoch 0, batch382/3125, loss0.516242 vs 0.821149\n",
      "epoch 0, batch383/3125, loss0.529827 vs 0.779073\n",
      "epoch 0, batch384/3125, loss0.548038 vs 0.787216\n",
      "epoch 0, batch385/3125, loss0.567543 vs 0.759301\n",
      "epoch 0, batch386/3125, loss0.588848 vs 0.756631\n",
      "epoch 0, batch387/3125, loss0.610123 vs 0.739413\n",
      "epoch 0, batch388/3125, loss0.630358 vs 0.745880\n",
      "epoch 0, batch389/3125, loss0.653705 vs 0.749653\n",
      "epoch 0, batch390/3125, loss0.669360 vs 0.733755\n",
      "epoch 0, batch391/3125, loss0.687188 vs 0.734998\n",
      "epoch 0, batch392/3125, loss0.702305 vs 0.731673\n",
      "epoch 0, batch393/3125, loss0.718153 vs 0.729581\n",
      "epoch 0, batch394/3125, loss0.727060 vs 0.721201\n",
      "epoch 0, batch395/3125, loss0.739691 vs 0.719727\n",
      "epoch 0, batch396/3125, loss0.750016 vs 0.721439\n",
      "epoch 0, batch397/3125, loss0.761607 vs 0.716483\n",
      "epoch 0, batch398/3125, loss0.773645 vs 0.711733\n",
      "epoch 0, batch399/3125, loss0.785265 vs 0.712520\n",
      "epoch 0, batch400/3125, loss0.792592 vs 0.710417\n",
      "epoch 0, batch401/3125, loss0.802178 vs 0.707743\n",
      "epoch 0, batch402/3125, loss0.809726 vs 0.704932\n",
      "epoch 0, batch403/3125, loss0.815693 vs 0.700084\n",
      "epoch 0, batch404/3125, loss0.822407 vs 0.696591\n",
      "epoch 0, batch405/3125, loss0.828010 vs 0.703166\n",
      "epoch 0, batch406/3125, loss0.831429 vs 0.693732\n",
      "epoch 0, batch407/3125, loss0.834636 vs 0.698331\n",
      "epoch 0, batch408/3125, loss0.835762 vs 0.699869\n",
      "epoch 0, batch409/3125, loss0.835879 vs 0.689865\n",
      "epoch 0, batch410/3125, loss0.837275 vs 0.688521\n",
      "epoch 0, batch411/3125, loss0.834839 vs 0.689964\n",
      "epoch 0, batch412/3125, loss0.828523 vs 0.684283\n",
      "epoch 0, batch413/3125, loss0.822226 vs 0.679589\n",
      "epoch 0, batch414/3125, loss0.817851 vs 0.690621\n",
      "epoch 0, batch415/3125, loss0.805722 vs 0.682317\n",
      "epoch 0, batch416/3125, loss0.800223 vs 0.669254\n",
      "epoch 0, batch417/3125, loss0.787451 vs 0.676676\n",
      "epoch 0, batch418/3125, loss0.777343 vs 0.677854\n",
      "epoch 0, batch419/3125, loss0.768804 vs 0.673928\n",
      "epoch 0, batch420/3125, loss0.758910 vs 0.678274\n",
      "epoch 0, batch421/3125, loss0.748241 vs 0.680008\n",
      "epoch 0, batch422/3125, loss0.737590 vs 0.679887\n",
      "epoch 0, batch423/3125, loss0.721426 vs 0.687662\n",
      "epoch 0, batch424/3125, loss0.707148 vs 0.694655\n",
      "epoch 0, batch425/3125, loss0.696725 vs 0.697688\n",
      "epoch 0, batch426/3125, loss0.693049 vs 0.696552\n",
      "epoch 0, batch427/3125, loss0.685504 vs 0.698654\n",
      "epoch 0, batch428/3125, loss0.683164 vs 0.696127\n",
      "epoch 0, batch429/3125, loss0.677416 vs 0.699245\n",
      "epoch 0, batch430/3125, loss0.677980 vs 0.703160\n",
      "epoch 0, batch431/3125, loss0.676058 vs 0.696379\n",
      "epoch 0, batch432/3125, loss0.678028 vs 0.701253\n",
      "epoch 0, batch433/3125, loss0.675117 vs 0.694910\n",
      "epoch 0, batch434/3125, loss0.680118 vs 0.700224\n",
      "epoch 0, batch435/3125, loss0.682784 vs 0.695731\n",
      "epoch 0, batch436/3125, loss0.685262 vs 0.692980\n",
      "epoch 0, batch437/3125, loss0.687600 vs 0.689049\n",
      "epoch 0, batch438/3125, loss0.693863 vs 0.693466\n",
      "epoch 0, batch439/3125, loss0.690283 vs 0.690896\n",
      "epoch 0, batch440/3125, loss0.693177 vs 0.692563\n",
      "epoch 0, batch441/3125, loss0.693707 vs 0.686289\n",
      "epoch 0, batch442/3125, loss0.693426 vs 0.689447\n",
      "epoch 0, batch443/3125, loss0.690811 vs 0.691745\n",
      "epoch 0, batch444/3125, loss0.687541 vs 0.695176\n",
      "epoch 0, batch445/3125, loss0.685152 vs 0.698102\n",
      "epoch 0, batch446/3125, loss0.674919 vs 0.701975\n",
      "epoch 0, batch447/3125, loss0.667782 vs 0.708311\n",
      "epoch 0, batch448/3125, loss0.656396 vs 0.716061\n",
      "epoch 0, batch449/3125, loss0.654844 vs 0.713830\n",
      "epoch 0, batch450/3125, loss0.645219 vs 0.729002\n",
      "epoch 0, batch451/3125, loss0.646184 vs 0.719675\n",
      "epoch 0, batch452/3125, loss0.645874 vs 0.719387\n",
      "epoch 0, batch453/3125, loss0.641100 vs 0.727464\n",
      "epoch 0, batch454/3125, loss0.647627 vs 0.724988\n",
      "epoch 0, batch455/3125, loss0.651905 vs 0.720972\n",
      "epoch 0, batch456/3125, loss0.661850 vs 0.717356\n",
      "epoch 0, batch457/3125, loss0.666560 vs 0.726319\n",
      "epoch 0, batch458/3125, loss0.674464 vs 0.714684\n",
      "epoch 0, batch459/3125, loss0.684419 vs 0.711925\n",
      "epoch 0, batch460/3125, loss0.687810 vs 0.716299\n",
      "epoch 0, batch461/3125, loss0.699276 vs 0.697972\n",
      "epoch 0, batch462/3125, loss0.703220 vs 0.701825\n",
      "epoch 0, batch463/3125, loss0.701992 vs 0.707875\n",
      "epoch 0, batch464/3125, loss0.708211 vs 0.708324\n",
      "epoch 0, batch465/3125, loss0.713427 vs 0.701006\n",
      "epoch 0, batch466/3125, loss0.715095 vs 0.703338\n",
      "epoch 0, batch467/3125, loss0.726280 vs 0.699153\n",
      "epoch 0, batch468/3125, loss0.735620 vs 0.692642\n",
      "epoch 0, batch469/3125, loss0.745703 vs 0.687328\n",
      "epoch 0, batch470/3125, loss0.756966 vs 0.685588\n",
      "epoch 0, batch471/3125, loss0.769229 vs 0.697185\n",
      "epoch 0, batch472/3125, loss0.770969 vs 0.687252\n",
      "epoch 0, batch473/3125, loss0.779292 vs 0.677264\n",
      "epoch 0, batch474/3125, loss0.784491 vs 0.670129\n",
      "epoch 0, batch475/3125, loss0.799120 vs 0.669586\n",
      "epoch 0, batch476/3125, loss0.805135 vs 0.667104\n",
      "epoch 0, batch477/3125, loss0.805252 vs 0.671985\n",
      "epoch 0, batch478/3125, loss0.820295 vs 0.660650\n",
      "epoch 0, batch479/3125, loss0.828352 vs 0.655316\n",
      "epoch 0, batch480/3125, loss0.829460 vs 0.657648\n",
      "epoch 0, batch481/3125, loss0.815212 vs 0.665932\n",
      "epoch 0, batch482/3125, loss0.814505 vs 0.663000\n",
      "epoch 0, batch483/3125, loss0.833451 vs 0.659598\n",
      "epoch 0, batch484/3125, loss0.832380 vs 0.654158\n",
      "epoch 0, batch485/3125, loss0.803156 vs 0.663428\n",
      "epoch 0, batch486/3125, loss0.799916 vs 0.668324\n",
      "epoch 0, batch487/3125, loss0.784907 vs 0.667286\n",
      "epoch 0, batch488/3125, loss0.770963 vs 0.672407\n",
      "epoch 0, batch489/3125, loss0.753820 vs 0.691205\n",
      "epoch 0, batch490/3125, loss0.735235 vs 0.689549\n",
      "epoch 0, batch491/3125, loss0.736152 vs 0.708502\n",
      "epoch 0, batch492/3125, loss0.720760 vs 0.705044\n",
      "epoch 0, batch493/3125, loss0.716804 vs 0.709815\n",
      "epoch 0, batch494/3125, loss0.697421 vs 0.719293\n",
      "epoch 0, batch495/3125, loss0.685662 vs 0.724246\n",
      "epoch 0, batch496/3125, loss0.665157 vs 0.739913\n",
      "epoch 0, batch497/3125, loss0.657941 vs 0.741200\n",
      "epoch 0, batch498/3125, loss0.641692 vs 0.749425\n",
      "epoch 0, batch499/3125, loss0.628581 vs 0.750207\n",
      "epoch 0, batch500/3125, loss0.632014 vs 0.742295\n",
      "epoch 0, batch501/3125, loss0.626251 vs 0.735371\n",
      "epoch 0, batch502/3125, loss0.618619 vs 0.753055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch503/3125, loss0.616125 vs 0.754467\n",
      "epoch 0, batch504/3125, loss0.617960 vs 0.750324\n",
      "epoch 0, batch505/3125, loss0.616274 vs 0.769526\n",
      "epoch 0, batch506/3125, loss0.622403 vs 0.785701\n",
      "epoch 0, batch507/3125, loss0.610477 vs 0.776591\n",
      "epoch 0, batch508/3125, loss0.620184 vs 0.754143\n",
      "epoch 0, batch509/3125, loss0.618746 vs 0.733567\n",
      "epoch 0, batch510/3125, loss0.616921 vs 0.764644\n",
      "epoch 0, batch511/3125, loss0.617928 vs 0.756597\n",
      "epoch 0, batch512/3125, loss0.621627 vs 0.747213\n",
      "epoch 0, batch513/3125, loss0.616730 vs 0.749250\n",
      "epoch 0, batch514/3125, loss0.624305 vs 0.756073\n",
      "epoch 0, batch515/3125, loss0.624351 vs 0.744381\n",
      "epoch 0, batch516/3125, loss0.620777 vs 0.751109\n",
      "epoch 0, batch517/3125, loss0.625525 vs 0.751195\n",
      "epoch 0, batch518/3125, loss0.626569 vs 0.741080\n",
      "epoch 0, batch519/3125, loss0.628033 vs 0.739348\n",
      "epoch 0, batch520/3125, loss0.633797 vs 0.743570\n",
      "epoch 0, batch521/3125, loss0.641131 vs 0.736515\n",
      "epoch 0, batch522/3125, loss0.645296 vs 0.738671\n",
      "epoch 0, batch523/3125, loss0.650724 vs 0.736736\n",
      "epoch 0, batch524/3125, loss0.658754 vs 0.726493\n",
      "epoch 0, batch525/3125, loss0.664096 vs 0.726172\n",
      "epoch 0, batch526/3125, loss0.670861 vs 0.709797\n",
      "epoch 0, batch527/3125, loss0.676257 vs 0.716963\n",
      "epoch 0, batch528/3125, loss0.686081 vs 0.717681\n",
      "epoch 0, batch529/3125, loss0.687668 vs 0.714942\n",
      "epoch 0, batch530/3125, loss0.695089 vs 0.706908\n",
      "epoch 0, batch531/3125, loss0.703672 vs 0.703069\n",
      "epoch 0, batch532/3125, loss0.708885 vs 0.703199\n",
      "epoch 0, batch533/3125, loss0.713451 vs 0.700908\n",
      "epoch 0, batch534/3125, loss0.723254 vs 0.693085\n",
      "epoch 0, batch535/3125, loss0.731233 vs 0.688222\n",
      "epoch 0, batch536/3125, loss0.735937 vs 0.688054\n",
      "epoch 0, batch537/3125, loss0.740897 vs 0.678910\n",
      "epoch 0, batch538/3125, loss0.746641 vs 0.680583\n",
      "epoch 0, batch539/3125, loss0.756497 vs 0.676191\n",
      "epoch 0, batch540/3125, loss0.759686 vs 0.669841\n",
      "epoch 0, batch541/3125, loss0.767350 vs 0.664307\n",
      "epoch 0, batch542/3125, loss0.776672 vs 0.665073\n",
      "epoch 0, batch543/3125, loss0.784987 vs 0.657644\n",
      "epoch 0, batch544/3125, loss0.787611 vs 0.658667\n",
      "epoch 0, batch545/3125, loss0.795271 vs 0.652156\n",
      "epoch 0, batch546/3125, loss0.799983 vs 0.651272\n",
      "epoch 0, batch547/3125, loss0.809906 vs 0.645675\n",
      "epoch 0, batch548/3125, loss0.813168 vs 0.646848\n",
      "epoch 0, batch549/3125, loss0.819887 vs 0.639835\n",
      "epoch 0, batch550/3125, loss0.819935 vs 0.634537\n",
      "epoch 0, batch551/3125, loss0.816729 vs 0.637683\n",
      "epoch 0, batch552/3125, loss0.815250 vs 0.639512\n",
      "epoch 0, batch553/3125, loss0.817768 vs 0.642976\n",
      "epoch 0, batch554/3125, loss0.811619 vs 0.634089\n",
      "epoch 0, batch555/3125, loss0.801316 vs 0.639732\n",
      "epoch 0, batch556/3125, loss0.781304 vs 0.652455\n",
      "epoch 0, batch557/3125, loss0.772403 vs 0.644572\n",
      "epoch 0, batch558/3125, loss0.763602 vs 0.656732\n",
      "epoch 0, batch559/3125, loss0.734455 vs 0.670338\n",
      "epoch 0, batch560/3125, loss0.731397 vs 0.663484\n",
      "epoch 0, batch561/3125, loss0.719239 vs 0.685819\n",
      "epoch 0, batch562/3125, loss0.700427 vs 0.680303\n",
      "epoch 0, batch563/3125, loss0.681929 vs 0.683442\n",
      "epoch 0, batch564/3125, loss0.675710 vs 0.687658\n",
      "epoch 0, batch565/3125, loss0.671812 vs 0.694396\n",
      "epoch 0, batch566/3125, loss0.667720 vs 0.698635\n",
      "epoch 0, batch567/3125, loss0.659306 vs 0.703181\n",
      "epoch 0, batch568/3125, loss0.640378 vs 0.725334\n",
      "epoch 0, batch569/3125, loss0.652856 vs 0.705097\n",
      "epoch 0, batch570/3125, loss0.666385 vs 0.702905\n",
      "epoch 0, batch571/3125, loss0.659440 vs 0.728039\n",
      "epoch 0, batch572/3125, loss0.656561 vs 0.691113\n",
      "epoch 0, batch573/3125, loss0.665892 vs 0.693581\n",
      "epoch 0, batch574/3125, loss0.663653 vs 0.682403\n",
      "epoch 0, batch575/3125, loss0.664549 vs 0.712390\n",
      "epoch 0, batch576/3125, loss0.675530 vs 0.693276\n",
      "epoch 0, batch577/3125, loss0.680714 vs 0.713572\n",
      "epoch 0, batch578/3125, loss0.680884 vs 0.712565\n",
      "epoch 0, batch579/3125, loss0.689277 vs 0.690886\n",
      "epoch 0, batch580/3125, loss0.693471 vs 0.677255\n",
      "epoch 0, batch581/3125, loss0.704538 vs 0.687161\n",
      "epoch 0, batch582/3125, loss0.707035 vs 0.688347\n",
      "epoch 0, batch583/3125, loss0.701763 vs 0.684164\n",
      "epoch 0, batch584/3125, loss0.718351 vs 0.678107\n",
      "epoch 0, batch585/3125, loss0.729500 vs 0.666748\n",
      "epoch 0, batch586/3125, loss0.729037 vs 0.687602\n",
      "epoch 0, batch587/3125, loss0.731295 vs 0.670485\n",
      "epoch 0, batch588/3125, loss0.733510 vs 0.682090\n",
      "epoch 0, batch589/3125, loss0.724953 vs 0.682747\n",
      "epoch 0, batch590/3125, loss0.723283 vs 0.685371\n",
      "epoch 0, batch591/3125, loss0.738551 vs 0.681972\n",
      "epoch 0, batch592/3125, loss0.737328 vs 0.680760\n",
      "epoch 0, batch593/3125, loss0.754691 vs 0.665431\n",
      "epoch 0, batch594/3125, loss0.747362 vs 0.688825\n",
      "epoch 0, batch595/3125, loss0.744714 vs 0.663283\n",
      "epoch 0, batch596/3125, loss0.750558 vs 0.688682\n",
      "epoch 0, batch597/3125, loss0.730637 vs 0.694667\n",
      "epoch 0, batch598/3125, loss0.712321 vs 0.700772\n",
      "epoch 0, batch599/3125, loss0.710310 vs 0.697657\n",
      "epoch 0, batch600/3125, loss0.711464 vs 0.698205\n",
      "epoch 0, batch601/3125, loss0.704230 vs 0.711320\n",
      "epoch 0, batch602/3125, loss0.698562 vs 0.714848\n",
      "epoch 0, batch603/3125, loss0.695104 vs 0.714113\n",
      "epoch 0, batch604/3125, loss0.694406 vs 0.716443\n",
      "epoch 0, batch605/3125, loss0.689124 vs 0.734822\n",
      "epoch 0, batch606/3125, loss0.682741 vs 0.720485\n",
      "epoch 0, batch607/3125, loss0.670434 vs 0.723282\n",
      "epoch 0, batch608/3125, loss0.666275 vs 0.727303\n",
      "epoch 0, batch609/3125, loss0.658983 vs 0.734053\n",
      "epoch 0, batch610/3125, loss0.655150 vs 0.737712\n",
      "epoch 0, batch611/3125, loss0.661041 vs 0.739196\n",
      "epoch 0, batch612/3125, loss0.644425 vs 0.751904\n",
      "epoch 0, batch613/3125, loss0.646998 vs 0.760004\n",
      "epoch 0, batch614/3125, loss0.633623 vs 0.760544\n",
      "epoch 0, batch615/3125, loss0.642305 vs 0.747732\n",
      "epoch 0, batch616/3125, loss0.642307 vs 0.756937\n",
      "epoch 0, batch617/3125, loss0.643786 vs 0.742146\n",
      "epoch 0, batch618/3125, loss0.653890 vs 0.745263\n",
      "epoch 0, batch619/3125, loss0.651490 vs 0.737230\n",
      "epoch 0, batch620/3125, loss0.656294 vs 0.741222\n",
      "epoch 0, batch621/3125, loss0.673081 vs 0.724426\n",
      "epoch 0, batch622/3125, loss0.678163 vs 0.731564\n",
      "epoch 0, batch623/3125, loss0.698709 vs 0.715598\n",
      "epoch 0, batch624/3125, loss0.710294 vs 0.717710\n",
      "epoch 0, batch625/3125, loss0.722148 vs 0.710305\n",
      "epoch 0, batch626/3125, loss0.729625 vs 0.700065\n",
      "epoch 0, batch627/3125, loss0.741099 vs 0.704635\n",
      "epoch 0, batch628/3125, loss0.744938 vs 0.706617\n",
      "epoch 0, batch629/3125, loss0.769949 vs 0.697949\n",
      "epoch 0, batch630/3125, loss0.786945 vs 0.677050\n",
      "epoch 0, batch631/3125, loss0.796327 vs 0.681751\n",
      "epoch 0, batch632/3125, loss0.825756 vs 0.678679\n",
      "epoch 0, batch633/3125, loss0.833986 vs 0.661827\n",
      "epoch 0, batch634/3125, loss0.843056 vs 0.665148\n",
      "epoch 0, batch635/3125, loss0.843284 vs 0.651169\n",
      "epoch 0, batch636/3125, loss0.843225 vs 0.647125\n",
      "epoch 0, batch637/3125, loss0.845188 vs 0.654461\n",
      "epoch 0, batch638/3125, loss0.846828 vs 0.641054\n",
      "epoch 0, batch639/3125, loss0.852027 vs 0.653372\n",
      "epoch 0, batch640/3125, loss0.853440 vs 0.651426\n",
      "epoch 0, batch641/3125, loss0.857265 vs 0.646651\n",
      "epoch 0, batch642/3125, loss0.873489 vs 0.631340\n",
      "epoch 0, batch643/3125, loss0.871783 vs 0.642705\n",
      "epoch 0, batch644/3125, loss0.890687 vs 0.642845\n",
      "epoch 0, batch645/3125, loss0.887038 vs 0.620918\n",
      "epoch 0, batch646/3125, loss0.887404 vs 0.626626\n",
      "epoch 0, batch647/3125, loss0.893349 vs 0.625774\n",
      "epoch 0, batch648/3125, loss0.896996 vs 0.639424\n",
      "epoch 0, batch649/3125, loss0.904428 vs 0.625616\n",
      "epoch 0, batch650/3125, loss0.902296 vs 0.620213\n",
      "epoch 0, batch651/3125, loss0.908648 vs 0.623585\n",
      "epoch 0, batch652/3125, loss0.892539 vs 0.615265\n",
      "epoch 0, batch653/3125, loss0.900457 vs 0.615915\n",
      "epoch 0, batch654/3125, loss0.889099 vs 0.597303\n",
      "epoch 0, batch655/3125, loss0.889054 vs 0.619821\n",
      "epoch 0, batch656/3125, loss0.902602 vs 0.628217\n",
      "epoch 0, batch657/3125, loss0.893101 vs 0.606108\n",
      "epoch 0, batch658/3125, loss0.872778 vs 0.607813\n",
      "epoch 0, batch659/3125, loss0.871586 vs 0.615450\n",
      "epoch 0, batch660/3125, loss0.890408 vs 0.613446\n",
      "epoch 0, batch661/3125, loss0.886860 vs 0.599515\n",
      "epoch 0, batch662/3125, loss0.876360 vs 0.610273\n",
      "epoch 0, batch663/3125, loss0.885616 vs 0.612423\n",
      "epoch 0, batch664/3125, loss0.883860 vs 0.605130\n",
      "epoch 0, batch665/3125, loss0.872550 vs 0.596574\n",
      "epoch 0, batch666/3125, loss0.870748 vs 0.603089\n",
      "epoch 0, batch667/3125, loss0.888387 vs 0.618337\n",
      "epoch 0, batch668/3125, loss0.883857 vs 0.611213\n",
      "epoch 0, batch669/3125, loss0.870787 vs 0.605231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch670/3125, loss0.873734 vs 0.602039\n",
      "epoch 0, batch671/3125, loss0.876556 vs 0.613187\n",
      "epoch 0, batch672/3125, loss0.879726 vs 0.618883\n",
      "epoch 0, batch673/3125, loss0.875889 vs 0.619223\n",
      "epoch 0, batch674/3125, loss0.879724 vs 0.593682\n",
      "epoch 0, batch675/3125, loss0.852143 vs 0.621951\n",
      "epoch 0, batch676/3125, loss0.854031 vs 0.628927\n",
      "epoch 0, batch677/3125, loss0.854962 vs 0.630547\n",
      "epoch 0, batch678/3125, loss0.856077 vs 0.602278\n",
      "epoch 0, batch679/3125, loss0.854980 vs 0.609248\n",
      "epoch 0, batch680/3125, loss0.846259 vs 0.633608\n",
      "epoch 0, batch681/3125, loss0.835921 vs 0.627037\n",
      "epoch 0, batch682/3125, loss0.835920 vs 0.603821\n",
      "epoch 0, batch683/3125, loss0.828570 vs 0.638448\n",
      "epoch 0, batch684/3125, loss0.816882 vs 0.637848\n",
      "epoch 0, batch685/3125, loss0.811159 vs 0.623969\n",
      "epoch 0, batch686/3125, loss0.799242 vs 0.661276\n",
      "epoch 0, batch687/3125, loss0.781074 vs 0.659362\n",
      "epoch 0, batch688/3125, loss0.762626 vs 0.640435\n",
      "epoch 0, batch689/3125, loss0.752848 vs 0.690076\n",
      "epoch 0, batch690/3125, loss0.740341 vs 0.681738\n",
      "epoch 0, batch691/3125, loss0.720227 vs 0.692128\n",
      "epoch 0, batch692/3125, loss0.689588 vs 0.736515\n",
      "epoch 0, batch693/3125, loss0.672721 vs 0.702041\n",
      "epoch 0, batch694/3125, loss0.652668 vs 0.759251\n",
      "epoch 0, batch695/3125, loss0.626034 vs 0.749677\n",
      "epoch 0, batch696/3125, loss0.590753 vs 0.799020\n",
      "epoch 0, batch697/3125, loss0.595854 vs 0.766235\n",
      "epoch 0, batch698/3125, loss0.578849 vs 0.805331\n",
      "epoch 0, batch699/3125, loss0.554239 vs 0.782744\n",
      "epoch 0, batch700/3125, loss0.542620 vs 0.802212\n",
      "epoch 0, batch701/3125, loss0.540888 vs 0.766864\n",
      "epoch 0, batch702/3125, loss0.537529 vs 0.806414\n",
      "epoch 0, batch703/3125, loss0.544218 vs 0.836934\n",
      "epoch 0, batch704/3125, loss0.546658 vs 0.773355\n",
      "epoch 0, batch705/3125, loss0.556983 vs 0.778308\n",
      "epoch 0, batch706/3125, loss0.568993 vs 0.772425\n",
      "epoch 0, batch707/3125, loss0.583243 vs 0.757828\n",
      "epoch 0, batch708/3125, loss0.587834 vs 0.725358\n",
      "epoch 0, batch709/3125, loss0.599546 vs 0.737281\n",
      "epoch 0, batch710/3125, loss0.615727 vs 0.740740\n",
      "epoch 0, batch711/3125, loss0.617224 vs 0.748076\n",
      "epoch 0, batch712/3125, loss0.624300 vs 0.733459\n",
      "epoch 0, batch713/3125, loss0.632626 vs 0.767175\n",
      "epoch 0, batch714/3125, loss0.642574 vs 0.709788\n",
      "epoch 0, batch715/3125, loss0.651573 vs 0.747517\n",
      "epoch 0, batch716/3125, loss0.648608 vs 0.736177\n",
      "epoch 0, batch717/3125, loss0.647600 vs 0.718594\n",
      "epoch 0, batch718/3125, loss0.645059 vs 0.720713\n",
      "epoch 0, batch719/3125, loss0.643559 vs 0.727601\n",
      "epoch 0, batch720/3125, loss0.649811 vs 0.734153\n",
      "epoch 0, batch721/3125, loss0.649166 vs 0.716689\n",
      "epoch 0, batch722/3125, loss0.650892 vs 0.732696\n",
      "epoch 0, batch723/3125, loss0.651001 vs 0.734886\n",
      "epoch 0, batch724/3125, loss0.655209 vs 0.738802\n",
      "epoch 0, batch725/3125, loss0.655976 vs 0.708711\n",
      "epoch 0, batch726/3125, loss0.660325 vs 0.744551\n",
      "epoch 0, batch727/3125, loss0.665194 vs 0.721146\n",
      "epoch 0, batch728/3125, loss0.672444 vs 0.711901\n",
      "epoch 0, batch729/3125, loss0.676567 vs 0.699753\n",
      "epoch 0, batch730/3125, loss0.677403 vs 0.710312\n",
      "epoch 0, batch731/3125, loss0.677416 vs 0.707622\n",
      "epoch 0, batch732/3125, loss0.684040 vs 0.714343\n",
      "epoch 0, batch733/3125, loss0.689559 vs 0.705438\n",
      "epoch 0, batch734/3125, loss0.688684 vs 0.701258\n",
      "epoch 0, batch735/3125, loss0.691437 vs 0.698552\n",
      "epoch 0, batch736/3125, loss0.694781 vs 0.701779\n",
      "epoch 0, batch737/3125, loss0.696594 vs 0.719279\n",
      "epoch 0, batch738/3125, loss0.704432 vs 0.690201\n",
      "epoch 0, batch739/3125, loss0.702357 vs 0.700696\n",
      "epoch 0, batch740/3125, loss0.701492 vs 0.705198\n",
      "epoch 0, batch741/3125, loss0.709191 vs 0.704532\n",
      "epoch 0, batch742/3125, loss0.708760 vs 0.693354\n",
      "epoch 0, batch743/3125, loss0.710598 vs 0.699131\n",
      "epoch 0, batch744/3125, loss0.711505 vs 0.687503\n",
      "epoch 0, batch745/3125, loss0.714032 vs 0.691349\n",
      "epoch 0, batch746/3125, loss0.717571 vs 0.679297\n",
      "epoch 0, batch747/3125, loss0.718410 vs 0.686938\n",
      "epoch 0, batch748/3125, loss0.718230 vs 0.688941\n",
      "epoch 0, batch749/3125, loss0.722897 vs 0.684098\n",
      "epoch 0, batch750/3125, loss0.719669 vs 0.687082\n",
      "epoch 0, batch751/3125, loss0.717155 vs 0.692898\n",
      "epoch 0, batch752/3125, loss0.721774 vs 0.674268\n",
      "epoch 0, batch753/3125, loss0.722708 vs 0.681412\n",
      "epoch 0, batch754/3125, loss0.723092 vs 0.678804\n",
      "epoch 0, batch755/3125, loss0.725758 vs 0.666241\n",
      "epoch 0, batch756/3125, loss0.721822 vs 0.684829\n",
      "epoch 0, batch757/3125, loss0.724833 vs 0.666429\n",
      "epoch 0, batch758/3125, loss0.725038 vs 0.670311\n",
      "epoch 0, batch759/3125, loss0.725793 vs 0.669006\n",
      "epoch 0, batch760/3125, loss0.731402 vs 0.666774\n",
      "epoch 0, batch761/3125, loss0.732296 vs 0.665570\n",
      "epoch 0, batch762/3125, loss0.734236 vs 0.666031\n",
      "epoch 0, batch763/3125, loss0.741070 vs 0.667447\n",
      "epoch 0, batch764/3125, loss0.747795 vs 0.666981\n",
      "epoch 0, batch765/3125, loss0.755684 vs 0.654593\n",
      "epoch 0, batch766/3125, loss0.762441 vs 0.657000\n",
      "epoch 0, batch767/3125, loss0.773562 vs 0.655951\n",
      "epoch 0, batch768/3125, loss0.784327 vs 0.650234\n",
      "epoch 0, batch769/3125, loss0.795291 vs 0.652531\n",
      "epoch 0, batch770/3125, loss0.805687 vs 0.653370\n",
      "epoch 0, batch771/3125, loss0.817432 vs 0.641404\n",
      "epoch 0, batch772/3125, loss0.825925 vs 0.634524\n",
      "epoch 0, batch773/3125, loss0.831208 vs 0.639213\n",
      "epoch 0, batch774/3125, loss0.835366 vs 0.630259\n",
      "epoch 0, batch775/3125, loss0.840526 vs 0.641366\n",
      "epoch 0, batch776/3125, loss0.838640 vs 0.676013\n",
      "epoch 0, batch777/3125, loss0.826456 vs 0.646906\n",
      "epoch 0, batch778/3125, loss0.828767 vs 0.648957\n",
      "epoch 0, batch779/3125, loss0.814311 vs 0.659666\n",
      "epoch 0, batch780/3125, loss0.785914 vs 0.636067\n",
      "epoch 0, batch781/3125, loss0.772450 vs 0.666210\n",
      "epoch 0, batch782/3125, loss0.742445 vs 0.674563\n",
      "epoch 0, batch783/3125, loss0.699195 vs 0.702698\n",
      "epoch 0, batch784/3125, loss0.662651 vs 0.715953\n",
      "epoch 0, batch785/3125, loss0.626036 vs 0.748609\n",
      "epoch 0, batch786/3125, loss0.593606 vs 0.763217\n",
      "epoch 0, batch787/3125, loss0.578887 vs 0.751899\n",
      "epoch 0, batch788/3125, loss0.558900 vs 0.764966\n",
      "epoch 0, batch789/3125, loss0.556918 vs 0.794176\n",
      "epoch 0, batch790/3125, loss0.559723 vs 0.760062\n",
      "epoch 0, batch791/3125, loss0.564501 vs 0.740590\n",
      "epoch 0, batch792/3125, loss0.571600 vs 0.757045\n",
      "epoch 0, batch793/3125, loss0.583833 vs 0.742068\n",
      "epoch 0, batch794/3125, loss0.593672 vs 0.733826\n",
      "epoch 0, batch795/3125, loss0.607447 vs 0.704136\n",
      "epoch 0, batch796/3125, loss0.622299 vs 0.711467\n",
      "epoch 0, batch797/3125, loss0.632731 vs 0.688187\n",
      "epoch 0, batch798/3125, loss0.644520 vs 0.724397\n",
      "epoch 0, batch799/3125, loss0.654782 vs 0.684965\n",
      "epoch 0, batch800/3125, loss0.662289 vs 0.677400\n",
      "epoch 0, batch801/3125, loss0.668669 vs 0.667751\n",
      "epoch 0, batch802/3125, loss0.681529 vs 0.686262\n",
      "epoch 0, batch803/3125, loss0.688448 vs 0.695466\n",
      "epoch 0, batch804/3125, loss0.693078 vs 0.650170\n",
      "epoch 0, batch805/3125, loss0.696969 vs 0.673986\n",
      "epoch 0, batch806/3125, loss0.694685 vs 0.651835\n",
      "epoch 0, batch807/3125, loss0.691466 vs 0.669180\n",
      "epoch 0, batch808/3125, loss0.691619 vs 0.669021\n",
      "epoch 0, batch809/3125, loss0.686821 vs 0.657627\n",
      "epoch 0, batch810/3125, loss0.688111 vs 0.650202\n",
      "epoch 0, batch811/3125, loss0.680963 vs 0.666901\n",
      "epoch 0, batch812/3125, loss0.664634 vs 0.690413\n",
      "epoch 0, batch813/3125, loss0.655764 vs 0.699641\n",
      "epoch 0, batch814/3125, loss0.637800 vs 0.690728\n",
      "epoch 0, batch815/3125, loss0.613365 vs 0.696687\n",
      "epoch 0, batch816/3125, loss0.593345 vs 0.691144\n",
      "epoch 0, batch817/3125, loss0.560940 vs 0.714197\n",
      "epoch 0, batch818/3125, loss0.538630 vs 0.744771\n",
      "epoch 0, batch819/3125, loss0.493800 vs 0.767301\n",
      "epoch 0, batch820/3125, loss0.468701 vs 0.805642\n",
      "epoch 0, batch821/3125, loss0.445448 vs 0.813443\n",
      "epoch 0, batch822/3125, loss0.433982 vs 0.832269\n",
      "epoch 0, batch823/3125, loss0.430344 vs 0.830626\n",
      "epoch 0, batch824/3125, loss0.422067 vs 0.834878\n",
      "epoch 0, batch825/3125, loss0.427634 vs 0.842669\n",
      "epoch 0, batch826/3125, loss0.435975 vs 0.818942\n",
      "epoch 0, batch827/3125, loss0.431735 vs 0.837935\n",
      "epoch 0, batch828/3125, loss0.444453 vs 0.837962\n",
      "epoch 0, batch829/3125, loss0.452764 vs 0.848311\n",
      "epoch 0, batch830/3125, loss0.463697 vs 0.814538\n",
      "epoch 0, batch831/3125, loss0.476373 vs 0.820798\n",
      "epoch 0, batch832/3125, loss0.486440 vs 0.800994\n",
      "epoch 0, batch833/3125, loss0.499479 vs 0.792917\n",
      "epoch 0, batch834/3125, loss0.517214 vs 0.803980\n",
      "epoch 0, batch835/3125, loss0.534607 vs 0.783165\n",
      "epoch 0, batch836/3125, loss0.547161 vs 0.777221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch837/3125, loss0.562739 vs 0.758953\n",
      "epoch 0, batch838/3125, loss0.580777 vs 0.771114\n",
      "epoch 0, batch839/3125, loss0.589790 vs 0.772877\n",
      "epoch 0, batch840/3125, loss0.604410 vs 0.746437\n",
      "epoch 0, batch841/3125, loss0.617985 vs 0.749029\n",
      "epoch 0, batch842/3125, loss0.631235 vs 0.735968\n",
      "epoch 0, batch843/3125, loss0.641826 vs 0.742466\n",
      "epoch 0, batch844/3125, loss0.651029 vs 0.723550\n",
      "epoch 0, batch845/3125, loss0.668165 vs 0.729784\n",
      "epoch 0, batch846/3125, loss0.675598 vs 0.723440\n",
      "epoch 0, batch847/3125, loss0.683447 vs 0.717912\n",
      "epoch 0, batch848/3125, loss0.698117 vs 0.722677\n",
      "epoch 0, batch849/3125, loss0.709967 vs 0.702456\n",
      "epoch 0, batch850/3125, loss0.722660 vs 0.708402\n",
      "epoch 0, batch851/3125, loss0.733836 vs 0.703364\n",
      "epoch 0, batch852/3125, loss0.745275 vs 0.700453\n",
      "epoch 0, batch853/3125, loss0.751785 vs 0.682356\n",
      "epoch 0, batch854/3125, loss0.760481 vs 0.712287\n",
      "epoch 0, batch855/3125, loss0.768226 vs 0.694195\n",
      "epoch 0, batch856/3125, loss0.776745 vs 0.692823\n",
      "epoch 0, batch857/3125, loss0.781704 vs 0.692401\n",
      "epoch 0, batch858/3125, loss0.786206 vs 0.657104\n",
      "epoch 0, batch859/3125, loss0.791952 vs 0.685335\n",
      "epoch 0, batch860/3125, loss0.796713 vs 0.658456\n",
      "epoch 0, batch861/3125, loss0.796321 vs 0.665434\n",
      "epoch 0, batch862/3125, loss0.797073 vs 0.672673\n",
      "epoch 0, batch863/3125, loss0.793502 vs 0.657024\n",
      "epoch 0, batch864/3125, loss0.791677 vs 0.661329\n",
      "epoch 0, batch865/3125, loss0.792772 vs 0.669021\n",
      "epoch 0, batch866/3125, loss0.787437 vs 0.669085\n",
      "epoch 0, batch867/3125, loss0.785368 vs 0.664642\n",
      "epoch 0, batch868/3125, loss0.779163 vs 0.692533\n",
      "epoch 0, batch869/3125, loss0.764642 vs 0.678531\n",
      "epoch 0, batch870/3125, loss0.762745 vs 0.664196\n",
      "epoch 0, batch871/3125, loss0.757879 vs 0.661001\n",
      "epoch 0, batch872/3125, loss0.749025 vs 0.660884\n",
      "epoch 0, batch873/3125, loss0.744456 vs 0.682387\n",
      "epoch 0, batch874/3125, loss0.735907 vs 0.688128\n",
      "epoch 0, batch875/3125, loss0.734353 vs 0.682640\n",
      "epoch 0, batch876/3125, loss0.719866 vs 0.688287\n",
      "epoch 0, batch877/3125, loss0.706045 vs 0.688240\n",
      "epoch 0, batch878/3125, loss0.700156 vs 0.693350\n",
      "epoch 0, batch879/3125, loss0.698383 vs 0.705893\n",
      "epoch 0, batch880/3125, loss0.676654 vs 0.706276\n",
      "epoch 0, batch881/3125, loss0.680714 vs 0.725342\n",
      "epoch 0, batch882/3125, loss0.679496 vs 0.707990\n",
      "epoch 0, batch883/3125, loss0.660264 vs 0.713386\n",
      "epoch 0, batch884/3125, loss0.660863 vs 0.716229\n",
      "epoch 0, batch885/3125, loss0.663280 vs 0.721633\n",
      "epoch 0, batch886/3125, loss0.660510 vs 0.718468\n",
      "epoch 0, batch887/3125, loss0.659022 vs 0.729143\n",
      "epoch 0, batch888/3125, loss0.655647 vs 0.706717\n",
      "epoch 0, batch889/3125, loss0.658232 vs 0.718230\n",
      "epoch 0, batch890/3125, loss0.660169 vs 0.722012\n",
      "epoch 0, batch891/3125, loss0.663365 vs 0.718095\n",
      "epoch 0, batch892/3125, loss0.663577 vs 0.717058\n",
      "epoch 0, batch893/3125, loss0.663206 vs 0.709291\n",
      "epoch 0, batch894/3125, loss0.664391 vs 0.710452\n",
      "epoch 0, batch895/3125, loss0.666731 vs 0.711598\n",
      "epoch 0, batch896/3125, loss0.668526 vs 0.725653\n",
      "epoch 0, batch897/3125, loss0.669332 vs 0.690523\n",
      "epoch 0, batch898/3125, loss0.673052 vs 0.703965\n",
      "epoch 0, batch899/3125, loss0.674652 vs 0.695516\n",
      "epoch 0, batch900/3125, loss0.674415 vs 0.735071\n",
      "epoch 0, batch901/3125, loss0.678243 vs 0.698300\n",
      "epoch 0, batch902/3125, loss0.678094 vs 0.701124\n",
      "epoch 0, batch903/3125, loss0.678617 vs 0.692105\n",
      "epoch 0, batch904/3125, loss0.681001 vs 0.692220\n",
      "epoch 0, batch905/3125, loss0.681196 vs 0.705761\n",
      "epoch 0, batch906/3125, loss0.682433 vs 0.703302\n",
      "epoch 0, batch907/3125, loss0.681722 vs 0.700483\n",
      "epoch 0, batch908/3125, loss0.682380 vs 0.683492\n",
      "epoch 0, batch909/3125, loss0.683131 vs 0.703070\n",
      "epoch 0, batch910/3125, loss0.682809 vs 0.696177\n",
      "epoch 0, batch911/3125, loss0.682871 vs 0.698676\n",
      "epoch 0, batch912/3125, loss0.682900 vs 0.696630\n",
      "epoch 0, batch913/3125, loss0.685703 vs 0.688969\n",
      "epoch 0, batch914/3125, loss0.687855 vs 0.677851\n",
      "epoch 0, batch915/3125, loss0.688456 vs 0.705004\n",
      "epoch 0, batch916/3125, loss0.690766 vs 0.676567\n",
      "epoch 0, batch917/3125, loss0.691486 vs 0.682232\n",
      "epoch 0, batch918/3125, loss0.692877 vs 0.696600\n",
      "epoch 0, batch919/3125, loss0.695518 vs 0.673517\n",
      "epoch 0, batch920/3125, loss0.694078 vs 0.684315\n",
      "epoch 0, batch921/3125, loss0.696990 vs 0.675417\n",
      "epoch 0, batch922/3125, loss0.695765 vs 0.690724\n",
      "epoch 0, batch923/3125, loss0.695811 vs 0.701886\n",
      "epoch 0, batch924/3125, loss0.694657 vs 0.677226\n",
      "epoch 0, batch925/3125, loss0.695400 vs 0.681488\n",
      "epoch 0, batch926/3125, loss0.693537 vs 0.690548\n",
      "epoch 0, batch927/3125, loss0.692879 vs 0.684121\n",
      "epoch 0, batch928/3125, loss0.691707 vs 0.681539\n",
      "epoch 0, batch929/3125, loss0.685653 vs 0.689674\n",
      "epoch 0, batch930/3125, loss0.687387 vs 0.680386\n",
      "epoch 0, batch931/3125, loss0.683863 vs 0.689021\n",
      "epoch 0, batch932/3125, loss0.682080 vs 0.698214\n",
      "epoch 0, batch933/3125, loss0.679000 vs 0.699400\n",
      "epoch 0, batch934/3125, loss0.672693 vs 0.719732\n",
      "epoch 0, batch935/3125, loss0.669682 vs 0.685606\n",
      "epoch 0, batch936/3125, loss0.668942 vs 0.701646\n",
      "epoch 0, batch937/3125, loss0.665558 vs 0.693933\n",
      "epoch 0, batch938/3125, loss0.660890 vs 0.700215\n",
      "epoch 0, batch939/3125, loss0.660666 vs 0.707879\n",
      "epoch 0, batch940/3125, loss0.654835 vs 0.693819\n",
      "epoch 0, batch941/3125, loss0.654398 vs 0.703367\n",
      "epoch 0, batch942/3125, loss0.648375 vs 0.703237\n",
      "epoch 0, batch943/3125, loss0.644381 vs 0.703722\n",
      "epoch 0, batch944/3125, loss0.638948 vs 0.717558\n",
      "epoch 0, batch945/3125, loss0.633750 vs 0.716328\n",
      "epoch 0, batch946/3125, loss0.636333 vs 0.724499\n",
      "epoch 0, batch947/3125, loss0.630471 vs 0.701841\n",
      "epoch 0, batch948/3125, loss0.622865 vs 0.722672\n",
      "epoch 0, batch949/3125, loss0.618763 vs 0.735341\n",
      "epoch 0, batch950/3125, loss0.611944 vs 0.732045\n",
      "epoch 0, batch951/3125, loss0.610118 vs 0.730800\n",
      "epoch 0, batch952/3125, loss0.607682 vs 0.723773\n",
      "epoch 0, batch953/3125, loss0.607286 vs 0.729865\n",
      "epoch 0, batch954/3125, loss0.608650 vs 0.725636\n",
      "epoch 0, batch955/3125, loss0.612945 vs 0.731757\n",
      "epoch 0, batch956/3125, loss0.611080 vs 0.733832\n",
      "epoch 0, batch957/3125, loss0.611755 vs 0.721478\n",
      "epoch 0, batch958/3125, loss0.613639 vs 0.727190\n",
      "epoch 0, batch959/3125, loss0.616124 vs 0.733958\n",
      "epoch 0, batch960/3125, loss0.617290 vs 0.733740\n",
      "epoch 0, batch961/3125, loss0.617518 vs 0.741466\n",
      "epoch 0, batch962/3125, loss0.619917 vs 0.722344\n",
      "epoch 0, batch963/3125, loss0.620937 vs 0.725119\n",
      "epoch 0, batch964/3125, loss0.619169 vs 0.723563\n",
      "epoch 0, batch965/3125, loss0.620576 vs 0.720573\n",
      "epoch 0, batch966/3125, loss0.621627 vs 0.721291\n",
      "epoch 0, batch967/3125, loss0.621440 vs 0.724179\n",
      "epoch 0, batch968/3125, loss0.622845 vs 0.725694\n",
      "epoch 0, batch969/3125, loss0.623963 vs 0.719150\n",
      "epoch 0, batch970/3125, loss0.626963 vs 0.725998\n",
      "epoch 0, batch971/3125, loss0.628713 vs 0.720902\n",
      "epoch 0, batch972/3125, loss0.631978 vs 0.728218\n",
      "epoch 0, batch973/3125, loss0.636215 vs 0.711646\n",
      "epoch 0, batch974/3125, loss0.638660 vs 0.716227\n",
      "epoch 0, batch975/3125, loss0.642599 vs 0.702658\n",
      "epoch 0, batch976/3125, loss0.645967 vs 0.731218\n",
      "epoch 0, batch977/3125, loss0.650070 vs 0.706293\n",
      "epoch 0, batch978/3125, loss0.653453 vs 0.705346\n",
      "epoch 0, batch979/3125, loss0.656198 vs 0.695882\n",
      "epoch 0, batch980/3125, loss0.659621 vs 0.695882\n",
      "epoch 0, batch981/3125, loss0.662112 vs 0.695952\n",
      "epoch 0, batch982/3125, loss0.665076 vs 0.694849\n",
      "epoch 0, batch983/3125, loss0.666099 vs 0.690536\n",
      "epoch 0, batch984/3125, loss0.666691 vs 0.694844\n",
      "epoch 0, batch985/3125, loss0.667134 vs 0.694118\n",
      "epoch 0, batch986/3125, loss0.666971 vs 0.694192\n",
      "epoch 0, batch987/3125, loss0.667415 vs 0.693670\n",
      "epoch 0, batch988/3125, loss0.666825 vs 0.702016\n",
      "epoch 0, batch989/3125, loss0.667759 vs 0.690711\n",
      "epoch 0, batch990/3125, loss0.668570 vs 0.686328\n",
      "epoch 0, batch991/3125, loss0.667638 vs 0.689675\n",
      "epoch 0, batch992/3125, loss0.666669 vs 0.691051\n",
      "epoch 0, batch993/3125, loss0.666628 vs 0.692175\n",
      "epoch 0, batch994/3125, loss0.664047 vs 0.690966\n",
      "epoch 0, batch995/3125, loss0.664039 vs 0.702032\n",
      "epoch 0, batch996/3125, loss0.663179 vs 0.692541\n",
      "epoch 0, batch997/3125, loss0.660827 vs 0.699689\n",
      "epoch 0, batch998/3125, loss0.658099 vs 0.694178\n",
      "epoch 0, batch999/3125, loss0.656756 vs 0.694796\n",
      "epoch 0, batch1000/3125, loss0.658935 vs 0.695920\n",
      "epoch 0, batch1001/3125, loss0.657229 vs 0.704164\n",
      "epoch 0, batch1002/3125, loss0.658481 vs 0.693987\n",
      "epoch 0, batch1003/3125, loss0.657180 vs 0.693679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch1004/3125, loss0.658488 vs 0.695910\n",
      "epoch 0, batch1005/3125, loss0.656453 vs 0.697136\n",
      "epoch 0, batch1006/3125, loss0.656863 vs 0.699016\n",
      "epoch 0, batch1007/3125, loss0.656617 vs 0.689900\n",
      "epoch 0, batch1008/3125, loss0.655782 vs 0.691795\n",
      "epoch 0, batch1009/3125, loss0.655264 vs 0.698885\n",
      "epoch 0, batch1010/3125, loss0.655400 vs 0.710662\n",
      "epoch 0, batch1011/3125, loss0.654945 vs 0.695100\n",
      "epoch 0, batch1012/3125, loss0.654166 vs 0.693339\n",
      "epoch 0, batch1013/3125, loss0.653848 vs 0.697614\n",
      "epoch 0, batch1014/3125, loss0.655178 vs 0.700433\n",
      "epoch 0, batch1015/3125, loss0.653128 vs 0.696547\n",
      "epoch 0, batch1016/3125, loss0.651823 vs 0.698709\n",
      "epoch 0, batch1017/3125, loss0.651321 vs 0.703621\n",
      "epoch 0, batch1018/3125, loss0.649552 vs 0.702234\n",
      "epoch 0, batch1019/3125, loss0.649226 vs 0.706136\n",
      "epoch 0, batch1020/3125, loss0.649584 vs 0.701922\n",
      "epoch 0, batch1021/3125, loss0.648416 vs 0.702965\n",
      "epoch 0, batch1022/3125, loss0.648264 vs 0.709415\n",
      "epoch 0, batch1023/3125, loss0.647926 vs 0.697988\n",
      "epoch 0, batch1024/3125, loss0.647248 vs 0.708467\n",
      "epoch 0, batch1025/3125, loss0.647311 vs 0.702018\n",
      "epoch 0, batch1026/3125, loss0.646344 vs 0.705683\n",
      "epoch 0, batch1027/3125, loss0.645596 vs 0.704724\n",
      "epoch 0, batch1028/3125, loss0.645841 vs 0.716723\n",
      "epoch 0, batch1029/3125, loss0.645566 vs 0.707639\n",
      "epoch 0, batch1030/3125, loss0.645566 vs 0.706531\n",
      "epoch 0, batch1031/3125, loss0.644283 vs 0.704739\n",
      "epoch 0, batch1032/3125, loss0.644725 vs 0.703386\n",
      "epoch 0, batch1033/3125, loss0.645091 vs 0.703522\n",
      "epoch 0, batch1034/3125, loss0.643860 vs 0.700513\n",
      "epoch 0, batch1035/3125, loss0.644385 vs 0.703117\n",
      "epoch 0, batch1036/3125, loss0.643506 vs 0.706528\n",
      "epoch 0, batch1037/3125, loss0.643761 vs 0.701623\n",
      "epoch 0, batch1038/3125, loss0.643137 vs 0.710204\n",
      "epoch 0, batch1039/3125, loss0.642941 vs 0.702752\n",
      "epoch 0, batch1040/3125, loss0.643588 vs 0.703903\n",
      "epoch 0, batch1041/3125, loss0.643427 vs 0.702451\n",
      "epoch 0, batch1042/3125, loss0.643326 vs 0.704655\n",
      "epoch 0, batch1043/3125, loss0.643252 vs 0.705377\n",
      "epoch 0, batch1044/3125, loss0.644078 vs 0.713450\n",
      "epoch 0, batch1045/3125, loss0.642827 vs 0.707637\n",
      "epoch 0, batch1046/3125, loss0.643489 vs 0.713241\n",
      "epoch 0, batch1047/3125, loss0.644770 vs 0.706045\n",
      "epoch 0, batch1048/3125, loss0.644668 vs 0.702157\n",
      "epoch 0, batch1049/3125, loss0.643836 vs 0.705107\n",
      "epoch 0, batch1050/3125, loss0.645062 vs 0.711241\n",
      "epoch 0, batch1051/3125, loss0.645875 vs 0.702331\n",
      "epoch 0, batch1052/3125, loss0.645886 vs 0.699974\n",
      "epoch 0, batch1053/3125, loss0.646608 vs 0.706161\n",
      "epoch 0, batch1054/3125, loss0.645351 vs 0.698815\n",
      "epoch 0, batch1055/3125, loss0.645801 vs 0.705265\n",
      "epoch 0, batch1056/3125, loss0.646527 vs 0.702817\n",
      "epoch 0, batch1057/3125, loss0.647153 vs 0.703531\n",
      "epoch 0, batch1058/3125, loss0.646797 vs 0.700483\n",
      "epoch 0, batch1059/3125, loss0.648327 vs 0.699096\n",
      "epoch 0, batch1060/3125, loss0.647307 vs 0.704321\n",
      "epoch 0, batch1061/3125, loss0.647263 vs 0.701330\n",
      "epoch 0, batch1062/3125, loss0.648229 vs 0.705016\n",
      "epoch 0, batch1063/3125, loss0.649303 vs 0.701919\n",
      "epoch 0, batch1064/3125, loss0.648383 vs 0.706252\n",
      "epoch 0, batch1065/3125, loss0.649616 vs 0.703958\n",
      "epoch 0, batch1066/3125, loss0.650216 vs 0.707689\n",
      "epoch 0, batch1067/3125, loss0.650289 vs 0.704967\n",
      "epoch 0, batch1068/3125, loss0.651458 vs 0.705268\n",
      "epoch 0, batch1069/3125, loss0.653236 vs 0.702417\n",
      "epoch 0, batch1070/3125, loss0.653960 vs 0.703555\n",
      "epoch 0, batch1071/3125, loss0.654379 vs 0.703783\n",
      "epoch 0, batch1072/3125, loss0.654948 vs 0.702979\n",
      "epoch 0, batch1073/3125, loss0.657169 vs 0.706393\n",
      "epoch 0, batch1074/3125, loss0.657302 vs 0.713948\n",
      "epoch 0, batch1075/3125, loss0.659917 vs 0.707684\n",
      "epoch 0, batch1076/3125, loss0.661415 vs 0.706699\n",
      "epoch 0, batch1077/3125, loss0.662203 vs 0.699261\n",
      "epoch 0, batch1078/3125, loss0.664231 vs 0.706358\n",
      "epoch 0, batch1079/3125, loss0.666114 vs 0.700611\n",
      "epoch 0, batch1080/3125, loss0.667626 vs 0.705754\n",
      "epoch 0, batch1081/3125, loss0.668885 vs 0.701807\n",
      "epoch 0, batch1082/3125, loss0.670276 vs 0.700710\n",
      "epoch 0, batch1083/3125, loss0.673561 vs 0.699534\n",
      "epoch 0, batch1084/3125, loss0.675506 vs 0.701712\n",
      "epoch 0, batch1085/3125, loss0.677950 vs 0.702198\n",
      "epoch 0, batch1086/3125, loss0.679137 vs 0.698376\n",
      "epoch 0, batch1087/3125, loss0.680270 vs 0.702733\n",
      "epoch 0, batch1088/3125, loss0.683421 vs 0.696597\n",
      "epoch 0, batch1089/3125, loss0.686276 vs 0.694557\n",
      "epoch 0, batch1090/3125, loss0.687935 vs 0.695230\n",
      "epoch 0, batch1091/3125, loss0.690473 vs 0.693005\n",
      "epoch 0, batch1092/3125, loss0.692775 vs 0.695034\n",
      "epoch 0, batch1093/3125, loss0.695892 vs 0.691771\n",
      "epoch 0, batch1094/3125, loss0.698349 vs 0.689473\n",
      "epoch 0, batch1095/3125, loss0.701063 vs 0.688523\n",
      "epoch 0, batch1096/3125, loss0.704510 vs 0.687884\n",
      "epoch 0, batch1097/3125, loss0.705748 vs 0.686013\n",
      "epoch 0, batch1098/3125, loss0.709195 vs 0.686675\n",
      "epoch 0, batch1099/3125, loss0.711083 vs 0.683619\n",
      "epoch 0, batch1100/3125, loss0.714150 vs 0.683157\n",
      "epoch 0, batch1101/3125, loss0.715442 vs 0.681802\n",
      "epoch 0, batch1102/3125, loss0.718097 vs 0.681066\n",
      "epoch 0, batch1103/3125, loss0.720020 vs 0.686602\n",
      "epoch 0, batch1104/3125, loss0.722431 vs 0.679551\n",
      "epoch 0, batch1105/3125, loss0.724231 vs 0.679772\n",
      "epoch 0, batch1106/3125, loss0.725692 vs 0.678771\n",
      "epoch 0, batch1107/3125, loss0.728345 vs 0.679755\n",
      "epoch 0, batch1108/3125, loss0.729520 vs 0.675671\n",
      "epoch 0, batch1109/3125, loss0.729542 vs 0.676564\n",
      "epoch 0, batch1110/3125, loss0.729824 vs 0.675785\n",
      "epoch 0, batch1111/3125, loss0.731134 vs 0.676698\n",
      "epoch 0, batch1112/3125, loss0.731343 vs 0.671448\n",
      "epoch 0, batch1113/3125, loss0.730996 vs 0.672575\n",
      "epoch 0, batch1114/3125, loss0.730285 vs 0.676487\n",
      "epoch 0, batch1115/3125, loss0.731035 vs 0.669080\n",
      "epoch 0, batch1116/3125, loss0.728320 vs 0.672039\n",
      "epoch 0, batch1117/3125, loss0.725298 vs 0.678252\n",
      "epoch 0, batch1118/3125, loss0.722523 vs 0.676691\n",
      "epoch 0, batch1119/3125, loss0.719295 vs 0.679748\n",
      "epoch 0, batch1120/3125, loss0.718766 vs 0.677295\n",
      "epoch 0, batch1121/3125, loss0.713555 vs 0.678009\n",
      "epoch 0, batch1122/3125, loss0.710643 vs 0.683280\n",
      "epoch 0, batch1123/3125, loss0.707673 vs 0.683683\n",
      "epoch 0, batch1124/3125, loss0.702596 vs 0.685806\n",
      "epoch 0, batch1125/3125, loss0.699291 vs 0.681778\n",
      "epoch 0, batch1126/3125, loss0.695272 vs 0.687691\n",
      "epoch 0, batch1127/3125, loss0.691382 vs 0.690796\n",
      "epoch 0, batch1128/3125, loss0.687280 vs 0.686531\n",
      "epoch 0, batch1129/3125, loss0.686834 vs 0.688096\n",
      "epoch 0, batch1130/3125, loss0.683654 vs 0.690503\n",
      "epoch 0, batch1131/3125, loss0.678050 vs 0.696815\n",
      "epoch 0, batch1132/3125, loss0.676367 vs 0.691143\n",
      "epoch 0, batch1133/3125, loss0.675075 vs 0.698824\n",
      "epoch 0, batch1134/3125, loss0.673091 vs 0.715001\n",
      "epoch 0, batch1135/3125, loss0.671331 vs 0.700587\n",
      "epoch 0, batch1136/3125, loss0.670459 vs 0.696754\n",
      "epoch 0, batch1137/3125, loss0.665746 vs 0.709679\n",
      "epoch 0, batch1138/3125, loss0.662865 vs 0.709001\n",
      "epoch 0, batch1139/3125, loss0.662052 vs 0.699896\n",
      "epoch 0, batch1140/3125, loss0.659536 vs 0.704025\n",
      "epoch 0, batch1141/3125, loss0.662449 vs 0.702349\n",
      "epoch 0, batch1142/3125, loss0.661928 vs 0.708836\n",
      "epoch 0, batch1143/3125, loss0.658518 vs 0.706748\n",
      "epoch 0, batch1144/3125, loss0.658916 vs 0.709565\n",
      "epoch 0, batch1145/3125, loss0.657066 vs 0.706901\n",
      "epoch 0, batch1146/3125, loss0.657959 vs 0.703428\n",
      "epoch 0, batch1147/3125, loss0.657427 vs 0.704889\n",
      "epoch 0, batch1148/3125, loss0.658256 vs 0.712783\n",
      "epoch 0, batch1149/3125, loss0.658305 vs 0.710364\n",
      "epoch 0, batch1150/3125, loss0.658465 vs 0.705984\n",
      "epoch 0, batch1151/3125, loss0.659237 vs 0.702601\n",
      "epoch 0, batch1152/3125, loss0.660661 vs 0.710066\n",
      "epoch 0, batch1153/3125, loss0.660806 vs 0.710718\n",
      "epoch 0, batch1154/3125, loss0.661304 vs 0.703174\n",
      "epoch 0, batch1155/3125, loss0.660513 vs 0.706696\n",
      "epoch 0, batch1156/3125, loss0.662563 vs 0.711462\n",
      "epoch 0, batch1157/3125, loss0.664432 vs 0.711675\n",
      "epoch 0, batch1158/3125, loss0.666116 vs 0.705349\n",
      "epoch 0, batch1159/3125, loss0.666843 vs 0.703579\n",
      "epoch 0, batch1160/3125, loss0.670483 vs 0.702501\n",
      "epoch 0, batch1161/3125, loss0.671753 vs 0.706849\n",
      "epoch 0, batch1162/3125, loss0.673110 vs 0.705346\n",
      "epoch 0, batch1163/3125, loss0.675293 vs 0.705027\n",
      "epoch 0, batch1164/3125, loss0.676803 vs 0.701160\n",
      "epoch 0, batch1165/3125, loss0.677917 vs 0.703412\n",
      "epoch 0, batch1166/3125, loss0.680316 vs 0.696728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch1167/3125, loss0.683243 vs 0.695996\n",
      "epoch 0, batch1168/3125, loss0.684510 vs 0.698816\n",
      "epoch 0, batch1169/3125, loss0.683345 vs 0.696856\n",
      "epoch 0, batch1170/3125, loss0.686377 vs 0.694806\n",
      "epoch 0, batch1171/3125, loss0.690341 vs 0.698029\n",
      "epoch 0, batch1172/3125, loss0.690001 vs 0.702957\n",
      "epoch 0, batch1173/3125, loss0.692164 vs 0.693830\n",
      "epoch 0, batch1174/3125, loss0.692800 vs 0.692767\n",
      "epoch 0, batch1175/3125, loss0.695077 vs 0.689118\n",
      "epoch 0, batch1176/3125, loss0.695695 vs 0.689545\n",
      "epoch 0, batch1177/3125, loss0.698347 vs 0.692364\n",
      "epoch 0, batch1178/3125, loss0.699806 vs 0.690811\n",
      "epoch 0, batch1179/3125, loss0.699487 vs 0.694152\n",
      "epoch 0, batch1180/3125, loss0.699178 vs 0.688630\n",
      "epoch 0, batch1181/3125, loss0.701934 vs 0.688067\n",
      "epoch 0, batch1182/3125, loss0.701504 vs 0.688391\n",
      "epoch 0, batch1183/3125, loss0.701974 vs 0.691896\n",
      "epoch 0, batch1184/3125, loss0.702047 vs 0.689286\n",
      "epoch 0, batch1185/3125, loss0.703447 vs 0.689653\n",
      "epoch 0, batch1186/3125, loss0.705093 vs 0.688876\n",
      "epoch 0, batch1187/3125, loss0.705690 vs 0.687489\n",
      "epoch 0, batch1188/3125, loss0.700306 vs 0.690668\n",
      "epoch 0, batch1189/3125, loss0.704037 vs 0.691300\n",
      "epoch 0, batch1190/3125, loss0.703673 vs 0.691570\n",
      "epoch 0, batch1191/3125, loss0.703214 vs 0.692737\n",
      "epoch 0, batch1192/3125, loss0.706164 vs 0.683978\n",
      "epoch 0, batch1193/3125, loss0.705717 vs 0.689255\n",
      "epoch 0, batch1194/3125, loss0.703310 vs 0.686819\n",
      "epoch 0, batch1195/3125, loss0.701512 vs 0.689558\n",
      "epoch 0, batch1196/3125, loss0.701429 vs 0.691023\n",
      "epoch 0, batch1197/3125, loss0.699009 vs 0.694359\n",
      "epoch 0, batch1198/3125, loss0.699257 vs 0.690122\n",
      "epoch 0, batch1199/3125, loss0.696694 vs 0.698309\n",
      "epoch 0, batch1200/3125, loss0.695512 vs 0.692286\n",
      "epoch 0, batch1201/3125, loss0.689156 vs 0.691770\n",
      "epoch 0, batch1202/3125, loss0.689834 vs 0.701384\n",
      "epoch 0, batch1203/3125, loss0.688935 vs 0.704592\n",
      "epoch 0, batch1204/3125, loss0.687505 vs 0.697635\n",
      "epoch 0, batch1205/3125, loss0.689052 vs 0.699681\n",
      "epoch 0, batch1206/3125, loss0.683691 vs 0.703913\n",
      "epoch 0, batch1207/3125, loss0.687488 vs 0.702234\n",
      "epoch 0, batch1208/3125, loss0.684922 vs 0.706458\n",
      "epoch 0, batch1209/3125, loss0.688753 vs 0.697041\n",
      "epoch 0, batch1210/3125, loss0.686249 vs 0.706052\n",
      "epoch 0, batch1211/3125, loss0.684282 vs 0.713303\n",
      "epoch 0, batch1212/3125, loss0.680728 vs 0.715234\n",
      "epoch 0, batch1213/3125, loss0.681657 vs 0.701230\n",
      "epoch 0, batch1214/3125, loss0.682837 vs 0.704708\n",
      "epoch 0, batch1215/3125, loss0.680250 vs 0.706512\n",
      "epoch 0, batch1216/3125, loss0.682305 vs 0.709081\n",
      "epoch 0, batch1217/3125, loss0.681277 vs 0.707182\n",
      "epoch 0, batch1218/3125, loss0.681820 vs 0.708142\n",
      "epoch 0, batch1219/3125, loss0.682353 vs 0.702204\n",
      "epoch 0, batch1220/3125, loss0.683458 vs 0.706975\n",
      "epoch 0, batch1221/3125, loss0.684021 vs 0.710248\n",
      "epoch 0, batch1222/3125, loss0.683389 vs 0.712608\n",
      "epoch 0, batch1223/3125, loss0.683366 vs 0.711417\n",
      "epoch 0, batch1224/3125, loss0.686925 vs 0.709402\n",
      "epoch 0, batch1225/3125, loss0.686871 vs 0.708021\n",
      "epoch 0, batch1226/3125, loss0.688970 vs 0.710509\n",
      "epoch 0, batch1227/3125, loss0.690427 vs 0.708657\n",
      "epoch 0, batch1228/3125, loss0.692615 vs 0.710214\n",
      "epoch 0, batch1229/3125, loss0.695813 vs 0.705305\n",
      "epoch 0, batch1230/3125, loss0.697194 vs 0.704840\n",
      "epoch 0, batch1231/3125, loss0.699551 vs 0.708009\n",
      "epoch 0, batch1232/3125, loss0.700836 vs 0.704342\n",
      "epoch 0, batch1233/3125, loss0.703060 vs 0.700900\n",
      "epoch 0, batch1234/3125, loss0.704686 vs 0.701606\n",
      "epoch 0, batch1235/3125, loss0.706081 vs 0.704370\n",
      "epoch 0, batch1236/3125, loss0.708147 vs 0.700219\n",
      "epoch 0, batch1237/3125, loss0.709067 vs 0.701972\n",
      "epoch 0, batch1238/3125, loss0.709517 vs 0.703607\n",
      "epoch 0, batch1239/3125, loss0.711189 vs 0.698371\n",
      "epoch 0, batch1240/3125, loss0.712588 vs 0.700496\n",
      "epoch 0, batch1241/3125, loss0.713672 vs 0.696884\n",
      "epoch 0, batch1242/3125, loss0.714967 vs 0.701947\n",
      "epoch 0, batch1243/3125, loss0.714300 vs 0.692253\n",
      "epoch 0, batch1244/3125, loss0.715663 vs 0.698291\n",
      "epoch 0, batch1245/3125, loss0.716536 vs 0.694784\n",
      "epoch 0, batch1246/3125, loss0.716648 vs 0.698854\n",
      "epoch 0, batch1247/3125, loss0.718244 vs 0.692848\n",
      "epoch 0, batch1248/3125, loss0.717699 vs 0.698138\n",
      "epoch 0, batch1249/3125, loss0.718729 vs 0.693611\n",
      "epoch 0, batch1250/3125, loss0.718619 vs 0.694098\n",
      "epoch 0, batch1251/3125, loss0.719565 vs 0.694118\n",
      "epoch 0, batch1252/3125, loss0.718887 vs 0.700850\n",
      "epoch 0, batch1253/3125, loss0.718641 vs 0.694974\n",
      "epoch 0, batch1254/3125, loss0.717431 vs 0.694931\n",
      "epoch 0, batch1255/3125, loss0.718407 vs 0.693135\n",
      "epoch 0, batch1256/3125, loss0.719018 vs 0.689815\n",
      "epoch 0, batch1257/3125, loss0.718368 vs 0.694078\n",
      "epoch 0, batch1258/3125, loss0.717425 vs 0.693912\n",
      "epoch 0, batch1259/3125, loss0.717673 vs 0.697622\n",
      "epoch 0, batch1260/3125, loss0.717015 vs 0.691016\n",
      "epoch 0, batch1261/3125, loss0.715451 vs 0.692537\n",
      "epoch 0, batch1262/3125, loss0.715813 vs 0.694473\n",
      "epoch 0, batch1263/3125, loss0.713244 vs 0.691461\n",
      "epoch 0, batch1264/3125, loss0.711590 vs 0.695422\n",
      "epoch 0, batch1265/3125, loss0.711135 vs 0.694318\n",
      "epoch 0, batch1266/3125, loss0.711134 vs 0.689966\n",
      "epoch 0, batch1267/3125, loss0.709749 vs 0.694504\n",
      "epoch 0, batch1268/3125, loss0.708524 vs 0.694002\n",
      "epoch 0, batch1269/3125, loss0.707648 vs 0.690366\n",
      "epoch 0, batch1270/3125, loss0.706545 vs 0.694051\n",
      "epoch 0, batch1271/3125, loss0.703974 vs 0.693961\n",
      "epoch 0, batch1272/3125, loss0.703693 vs 0.696367\n",
      "epoch 0, batch1273/3125, loss0.702333 vs 0.695028\n",
      "epoch 0, batch1274/3125, loss0.700755 vs 0.690664\n",
      "epoch 0, batch1275/3125, loss0.699309 vs 0.693232\n",
      "epoch 0, batch1276/3125, loss0.698268 vs 0.689806\n",
      "epoch 0, batch1277/3125, loss0.697211 vs 0.692938\n",
      "epoch 0, batch1278/3125, loss0.696304 vs 0.696991\n",
      "epoch 0, batch1279/3125, loss0.693737 vs 0.694882\n",
      "epoch 0, batch1280/3125, loss0.693410 vs 0.695769\n",
      "epoch 0, batch1281/3125, loss0.691650 vs 0.692740\n",
      "epoch 0, batch1282/3125, loss0.690779 vs 0.696229\n",
      "epoch 0, batch1283/3125, loss0.689969 vs 0.695749\n",
      "epoch 0, batch1284/3125, loss0.688307 vs 0.693974\n",
      "epoch 0, batch1285/3125, loss0.687391 vs 0.694198\n",
      "epoch 0, batch1286/3125, loss0.685958 vs 0.698763\n",
      "epoch 0, batch1287/3125, loss0.684906 vs 0.696542\n",
      "epoch 0, batch1288/3125, loss0.682987 vs 0.692935\n",
      "epoch 0, batch1289/3125, loss0.683695 vs 0.696602\n",
      "epoch 0, batch1290/3125, loss0.681732 vs 0.696244\n",
      "epoch 0, batch1291/3125, loss0.679387 vs 0.695931\n",
      "epoch 0, batch1292/3125, loss0.678983 vs 0.694465\n",
      "epoch 0, batch1293/3125, loss0.678486 vs 0.697263\n",
      "epoch 0, batch1294/3125, loss0.677634 vs 0.694807\n",
      "epoch 0, batch1295/3125, loss0.676142 vs 0.694694\n",
      "epoch 0, batch1296/3125, loss0.675566 vs 0.693432\n",
      "epoch 0, batch1297/3125, loss0.675316 vs 0.695579\n",
      "epoch 0, batch1298/3125, loss0.674659 vs 0.693134\n",
      "epoch 0, batch1299/3125, loss0.673728 vs 0.690643\n",
      "epoch 0, batch1300/3125, loss0.673651 vs 0.695843\n",
      "epoch 0, batch1301/3125, loss0.673719 vs 0.690380\n",
      "epoch 0, batch1302/3125, loss0.672030 vs 0.695706\n",
      "epoch 0, batch1303/3125, loss0.672094 vs 0.690571\n",
      "epoch 0, batch1304/3125, loss0.671610 vs 0.689477\n",
      "epoch 0, batch1305/3125, loss0.671694 vs 0.691634\n",
      "epoch 0, batch1306/3125, loss0.671659 vs 0.689113\n",
      "epoch 0, batch1307/3125, loss0.671444 vs 0.691456\n",
      "epoch 0, batch1308/3125, loss0.670247 vs 0.689834\n",
      "epoch 0, batch1309/3125, loss0.670213 vs 0.688167\n",
      "epoch 0, batch1310/3125, loss0.669813 vs 0.687653\n",
      "epoch 0, batch1311/3125, loss0.670025 vs 0.691129\n",
      "epoch 0, batch1312/3125, loss0.669380 vs 0.688286\n",
      "epoch 0, batch1313/3125, loss0.669510 vs 0.685800\n",
      "epoch 0, batch1314/3125, loss0.669176 vs 0.694231\n",
      "epoch 0, batch1315/3125, loss0.669109 vs 0.689086\n",
      "epoch 0, batch1316/3125, loss0.667763 vs 0.692758\n",
      "epoch 0, batch1317/3125, loss0.667706 vs 0.686086\n",
      "epoch 0, batch1318/3125, loss0.667049 vs 0.688611\n",
      "epoch 0, batch1319/3125, loss0.666160 vs 0.690437\n",
      "epoch 0, batch1320/3125, loss0.665057 vs 0.684808\n",
      "epoch 0, batch1321/3125, loss0.664862 vs 0.688832\n",
      "epoch 0, batch1322/3125, loss0.664006 vs 0.691622\n",
      "epoch 0, batch1323/3125, loss0.662610 vs 0.688680\n",
      "epoch 0, batch1324/3125, loss0.660884 vs 0.681039\n",
      "epoch 0, batch1325/3125, loss0.659723 vs 0.695329\n",
      "epoch 0, batch1326/3125, loss0.657615 vs 0.686388\n",
      "epoch 0, batch1327/3125, loss0.655967 vs 0.685022\n",
      "epoch 0, batch1328/3125, loss0.653446 vs 0.684746\n",
      "epoch 0, batch1329/3125, loss0.651637 vs 0.690313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch1330/3125, loss0.649027 vs 0.696290\n",
      "epoch 0, batch1331/3125, loss0.645612 vs 0.698823\n",
      "epoch 0, batch1332/3125, loss0.642898 vs 0.691755\n",
      "epoch 0, batch1333/3125, loss0.638432 vs 0.697426\n",
      "epoch 0, batch1334/3125, loss0.637343 vs 0.694197\n",
      "epoch 0, batch1335/3125, loss0.631594 vs 0.703834\n",
      "epoch 0, batch1336/3125, loss0.629014 vs 0.706536\n",
      "epoch 0, batch1337/3125, loss0.624333 vs 0.709349\n",
      "epoch 0, batch1338/3125, loss0.621392 vs 0.701724\n",
      "epoch 0, batch1339/3125, loss0.619854 vs 0.710807\n",
      "epoch 0, batch1340/3125, loss0.617218 vs 0.713360\n",
      "epoch 0, batch1341/3125, loss0.615155 vs 0.713093\n",
      "epoch 0, batch1342/3125, loss0.614470 vs 0.713004\n",
      "epoch 0, batch1343/3125, loss0.612798 vs 0.719730\n",
      "epoch 0, batch1344/3125, loss0.611635 vs 0.721325\n",
      "epoch 0, batch1345/3125, loss0.612531 vs 0.716045\n",
      "epoch 0, batch1346/3125, loss0.613998 vs 0.719832\n",
      "epoch 0, batch1347/3125, loss0.615596 vs 0.719686\n",
      "epoch 0, batch1348/3125, loss0.617751 vs 0.723676\n",
      "epoch 0, batch1349/3125, loss0.620776 vs 0.715079\n",
      "epoch 0, batch1350/3125, loss0.623080 vs 0.713977\n",
      "epoch 0, batch1351/3125, loss0.627196 vs 0.713068\n",
      "epoch 0, batch1352/3125, loss0.630969 vs 0.712455\n",
      "epoch 0, batch1353/3125, loss0.634594 vs 0.714309\n",
      "epoch 0, batch1354/3125, loss0.639265 vs 0.707212\n",
      "epoch 0, batch1355/3125, loss0.642699 vs 0.711309\n",
      "epoch 0, batch1356/3125, loss0.647433 vs 0.711054\n",
      "epoch 0, batch1357/3125, loss0.651736 vs 0.709473\n",
      "epoch 0, batch1358/3125, loss0.656173 vs 0.706641\n",
      "epoch 0, batch1359/3125, loss0.660645 vs 0.705835\n",
      "epoch 0, batch1360/3125, loss0.664616 vs 0.703736\n",
      "epoch 0, batch1361/3125, loss0.668386 vs 0.704100\n",
      "epoch 0, batch1362/3125, loss0.672331 vs 0.707384\n",
      "epoch 0, batch1363/3125, loss0.676067 vs 0.702201\n",
      "epoch 0, batch1364/3125, loss0.679360 vs 0.706567\n",
      "epoch 0, batch1365/3125, loss0.682870 vs 0.698906\n",
      "epoch 0, batch1366/3125, loss0.686219 vs 0.697159\n",
      "epoch 0, batch1367/3125, loss0.689703 vs 0.702550\n",
      "epoch 0, batch1368/3125, loss0.693281 vs 0.697414\n",
      "epoch 0, batch1369/3125, loss0.696120 vs 0.699067\n",
      "epoch 0, batch1370/3125, loss0.699372 vs 0.698013\n",
      "epoch 0, batch1371/3125, loss0.702518 vs 0.700233\n",
      "epoch 0, batch1372/3125, loss0.706042 vs 0.696721\n",
      "epoch 0, batch1373/3125, loss0.708824 vs 0.696376\n",
      "epoch 0, batch1374/3125, loss0.712315 vs 0.695724\n",
      "epoch 0, batch1375/3125, loss0.714894 vs 0.701463\n",
      "epoch 0, batch1376/3125, loss0.717840 vs 0.690873\n",
      "epoch 0, batch1377/3125, loss0.719927 vs 0.689579\n",
      "epoch 0, batch1378/3125, loss0.722739 vs 0.689789\n",
      "epoch 0, batch1379/3125, loss0.725106 vs 0.688189\n",
      "epoch 0, batch1380/3125, loss0.726282 vs 0.687393\n",
      "epoch 0, batch1381/3125, loss0.728724 vs 0.686749\n",
      "epoch 0, batch1382/3125, loss0.730366 vs 0.687559\n",
      "epoch 0, batch1383/3125, loss0.732262 vs 0.692964\n",
      "epoch 0, batch1384/3125, loss0.733963 vs 0.687465\n",
      "epoch 0, batch1385/3125, loss0.735400 vs 0.687499\n",
      "epoch 0, batch1386/3125, loss0.737075 vs 0.689268\n",
      "epoch 0, batch1387/3125, loss0.738632 vs 0.683075\n",
      "epoch 0, batch1388/3125, loss0.740343 vs 0.682071\n",
      "epoch 0, batch1389/3125, loss0.741576 vs 0.681329\n",
      "epoch 0, batch1390/3125, loss0.742756 vs 0.690939\n",
      "epoch 0, batch1391/3125, loss0.744507 vs 0.679364\n",
      "epoch 0, batch1392/3125, loss0.745007 vs 0.685116\n",
      "epoch 0, batch1393/3125, loss0.745947 vs 0.680550\n",
      "epoch 0, batch1394/3125, loss0.747005 vs 0.681915\n",
      "epoch 0, batch1395/3125, loss0.747351 vs 0.686645\n",
      "epoch 0, batch1396/3125, loss0.747900 vs 0.680607\n",
      "epoch 0, batch1397/3125, loss0.748614 vs 0.677113\n",
      "epoch 0, batch1398/3125, loss0.749069 vs 0.688163\n",
      "epoch 0, batch1399/3125, loss0.748531 vs 0.685513\n",
      "epoch 0, batch1400/3125, loss0.748560 vs 0.675493\n",
      "epoch 0, batch1401/3125, loss0.748308 vs 0.678230\n",
      "epoch 0, batch1402/3125, loss0.748202 vs 0.685160\n",
      "epoch 0, batch1403/3125, loss0.748577 vs 0.679156\n",
      "epoch 0, batch1404/3125, loss0.748367 vs 0.689810\n",
      "epoch 0, batch1405/3125, loss0.747733 vs 0.681957\n",
      "epoch 0, batch1406/3125, loss0.746982 vs 0.684319\n",
      "epoch 0, batch1407/3125, loss0.745860 vs 0.688523\n",
      "epoch 0, batch1408/3125, loss0.745723 vs 0.678557\n",
      "epoch 0, batch1409/3125, loss0.744826 vs 0.675271\n",
      "epoch 0, batch1410/3125, loss0.743117 vs 0.674370\n",
      "epoch 0, batch1411/3125, loss0.741824 vs 0.683706\n",
      "epoch 0, batch1412/3125, loss0.741628 vs 0.688093\n",
      "epoch 0, batch1413/3125, loss0.738986 vs 0.677652\n",
      "epoch 0, batch1414/3125, loss0.737667 vs 0.687307\n",
      "epoch 0, batch1415/3125, loss0.735636 vs 0.689190\n",
      "epoch 0, batch1416/3125, loss0.733782 vs 0.684292\n",
      "epoch 0, batch1417/3125, loss0.732378 vs 0.682397\n",
      "epoch 0, batch1418/3125, loss0.729514 vs 0.686897\n",
      "epoch 0, batch1419/3125, loss0.727582 vs 0.681529\n",
      "epoch 0, batch1420/3125, loss0.725531 vs 0.684158\n",
      "epoch 0, batch1421/3125, loss0.723152 vs 0.679960\n",
      "epoch 0, batch1422/3125, loss0.721417 vs 0.687964\n",
      "epoch 0, batch1423/3125, loss0.718948 vs 0.682548\n",
      "epoch 0, batch1424/3125, loss0.716322 vs 0.684037\n",
      "epoch 0, batch1425/3125, loss0.714727 vs 0.684189\n",
      "epoch 0, batch1426/3125, loss0.712689 vs 0.692648\n",
      "epoch 0, batch1427/3125, loss0.710416 vs 0.698002\n",
      "epoch 0, batch1428/3125, loss0.709518 vs 0.689122\n",
      "epoch 0, batch1429/3125, loss0.707923 vs 0.689526\n",
      "epoch 0, batch1430/3125, loss0.706310 vs 0.689239\n",
      "epoch 0, batch1431/3125, loss0.705485 vs 0.685782\n",
      "epoch 0, batch1432/3125, loss0.703828 vs 0.695935\n",
      "epoch 0, batch1433/3125, loss0.702666 vs 0.693001\n",
      "epoch 0, batch1434/3125, loss0.701563 vs 0.693035\n",
      "epoch 0, batch1435/3125, loss0.700635 vs 0.695949\n",
      "epoch 0, batch1436/3125, loss0.699890 vs 0.690873\n",
      "epoch 0, batch1437/3125, loss0.698878 vs 0.689571\n",
      "epoch 0, batch1438/3125, loss0.698342 vs 0.700589\n",
      "epoch 0, batch1439/3125, loss0.698193 vs 0.692573\n",
      "epoch 0, batch1440/3125, loss0.697251 vs 0.693376\n",
      "epoch 0, batch1441/3125, loss0.696121 vs 0.702185\n",
      "epoch 0, batch1442/3125, loss0.695195 vs 0.695433\n",
      "epoch 0, batch1443/3125, loss0.694900 vs 0.692429\n",
      "epoch 0, batch1444/3125, loss0.693740 vs 0.689597\n",
      "epoch 0, batch1445/3125, loss0.693042 vs 0.699139\n",
      "epoch 0, batch1446/3125, loss0.692364 vs 0.691852\n",
      "epoch 0, batch1447/3125, loss0.691915 vs 0.692853\n",
      "epoch 0, batch1448/3125, loss0.691094 vs 0.694808\n",
      "epoch 0, batch1449/3125, loss0.690525 vs 0.692615\n",
      "epoch 0, batch1450/3125, loss0.690127 vs 0.691402\n",
      "epoch 0, batch1451/3125, loss0.689092 vs 0.692610\n",
      "epoch 0, batch1452/3125, loss0.689089 vs 0.700121\n",
      "epoch 0, batch1453/3125, loss0.688321 vs 0.693284\n",
      "epoch 0, batch1454/3125, loss0.687911 vs 0.692307\n",
      "epoch 0, batch1455/3125, loss0.687588 vs 0.692728\n",
      "epoch 0, batch1456/3125, loss0.687342 vs 0.693449\n",
      "epoch 0, batch1457/3125, loss0.686956 vs 0.694406\n",
      "epoch 0, batch1458/3125, loss0.686281 vs 0.696471\n",
      "epoch 0, batch1459/3125, loss0.685602 vs 0.693712\n",
      "epoch 0, batch1460/3125, loss0.685786 vs 0.695089\n",
      "epoch 0, batch1461/3125, loss0.685270 vs 0.694701\n",
      "epoch 0, batch1462/3125, loss0.684694 vs 0.697189\n",
      "epoch 0, batch1463/3125, loss0.684062 vs 0.697372\n",
      "epoch 0, batch1464/3125, loss0.683750 vs 0.693605\n",
      "epoch 0, batch1465/3125, loss0.683077 vs 0.698145\n",
      "epoch 0, batch1466/3125, loss0.682442 vs 0.696913\n",
      "epoch 0, batch1467/3125, loss0.682337 vs 0.697749\n",
      "epoch 0, batch1468/3125, loss0.681677 vs 0.697282\n",
      "epoch 0, batch1469/3125, loss0.681284 vs 0.702922\n",
      "epoch 0, batch1470/3125, loss0.681041 vs 0.696001\n",
      "epoch 0, batch1471/3125, loss0.680622 vs 0.699696\n",
      "epoch 0, batch1472/3125, loss0.679738 vs 0.698743\n",
      "epoch 0, batch1473/3125, loss0.680013 vs 0.699304\n",
      "epoch 0, batch1474/3125, loss0.679397 vs 0.699481\n",
      "epoch 0, batch1475/3125, loss0.678777 vs 0.696983\n",
      "epoch 0, batch1476/3125, loss0.678460 vs 0.698173\n",
      "epoch 0, batch1477/3125, loss0.677887 vs 0.702631\n",
      "epoch 0, batch1478/3125, loss0.677223 vs 0.698595\n",
      "epoch 0, batch1479/3125, loss0.676917 vs 0.697736\n",
      "epoch 0, batch1480/3125, loss0.676295 vs 0.699501\n",
      "epoch 0, batch1481/3125, loss0.676054 vs 0.699180\n",
      "epoch 0, batch1482/3125, loss0.675440 vs 0.699433\n",
      "epoch 0, batch1483/3125, loss0.675157 vs 0.699368\n",
      "epoch 0, batch1484/3125, loss0.674600 vs 0.701365\n",
      "epoch 0, batch1485/3125, loss0.674432 vs 0.698612\n",
      "epoch 0, batch1486/3125, loss0.674564 vs 0.698975\n",
      "epoch 0, batch1487/3125, loss0.673886 vs 0.698352\n",
      "epoch 0, batch1488/3125, loss0.673519 vs 0.702766\n",
      "epoch 0, batch1489/3125, loss0.673552 vs 0.698121\n",
      "epoch 0, batch1490/3125, loss0.673506 vs 0.701505\n",
      "epoch 0, batch1491/3125, loss0.673279 vs 0.698920\n",
      "epoch 0, batch1492/3125, loss0.673008 vs 0.701026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch1493/3125, loss0.673396 vs 0.698233\n",
      "epoch 0, batch1494/3125, loss0.673061 vs 0.697635\n",
      "epoch 0, batch1495/3125, loss0.673079 vs 0.698134\n",
      "epoch 0, batch1496/3125, loss0.673123 vs 0.698090\n",
      "epoch 0, batch1497/3125, loss0.673224 vs 0.697243\n",
      "epoch 0, batch1498/3125, loss0.673204 vs 0.698309\n",
      "epoch 0, batch1499/3125, loss0.673240 vs 0.697255\n",
      "epoch 0, batch1500/3125, loss0.673196 vs 0.699462\n",
      "epoch 0, batch1501/3125, loss0.673506 vs 0.698599\n",
      "epoch 0, batch1502/3125, loss0.673491 vs 0.697249\n",
      "epoch 0, batch1503/3125, loss0.673897 vs 0.698262\n",
      "epoch 0, batch1504/3125, loss0.673714 vs 0.698082\n",
      "epoch 0, batch1505/3125, loss0.674008 vs 0.695885\n",
      "epoch 0, batch1506/3125, loss0.674006 vs 0.696610\n",
      "epoch 0, batch1507/3125, loss0.674456 vs 0.697308\n",
      "epoch 0, batch1508/3125, loss0.674702 vs 0.696199\n",
      "epoch 0, batch1509/3125, loss0.674937 vs 0.695662\n",
      "epoch 0, batch1510/3125, loss0.675293 vs 0.696237\n",
      "epoch 0, batch1511/3125, loss0.675317 vs 0.695083\n",
      "epoch 0, batch1512/3125, loss0.675616 vs 0.694928\n",
      "epoch 0, batch1513/3125, loss0.676010 vs 0.694790\n",
      "epoch 0, batch1514/3125, loss0.676243 vs 0.696494\n",
      "epoch 0, batch1515/3125, loss0.676497 vs 0.694520\n",
      "epoch 0, batch1516/3125, loss0.676798 vs 0.693706\n",
      "epoch 0, batch1517/3125, loss0.677200 vs 0.694482\n",
      "epoch 0, batch1518/3125, loss0.677495 vs 0.693856\n",
      "epoch 0, batch1519/3125, loss0.677753 vs 0.693447\n",
      "epoch 0, batch1520/3125, loss0.678083 vs 0.693310\n",
      "epoch 0, batch1521/3125, loss0.678304 vs 0.693808\n",
      "epoch 0, batch1522/3125, loss0.678686 vs 0.693959\n",
      "epoch 0, batch1523/3125, loss0.678984 vs 0.694343\n",
      "epoch 0, batch1524/3125, loss0.679245 vs 0.692993\n",
      "epoch 0, batch1525/3125, loss0.679681 vs 0.690863\n",
      "epoch 0, batch1526/3125, loss0.679931 vs 0.695183\n",
      "epoch 0, batch1527/3125, loss0.680252 vs 0.690937\n",
      "epoch 0, batch1528/3125, loss0.680648 vs 0.690823\n",
      "epoch 0, batch1529/3125, loss0.680900 vs 0.690695\n",
      "epoch 0, batch1530/3125, loss0.681144 vs 0.690506\n",
      "epoch 0, batch1531/3125, loss0.681334 vs 0.697122\n",
      "epoch 0, batch1532/3125, loss0.681551 vs 0.692286\n",
      "epoch 0, batch1533/3125, loss0.681900 vs 0.690815\n",
      "epoch 0, batch1534/3125, loss0.682134 vs 0.693043\n",
      "epoch 0, batch1535/3125, loss0.682141 vs 0.689765\n",
      "epoch 0, batch1536/3125, loss0.682239 vs 0.690306\n",
      "epoch 0, batch1537/3125, loss0.682459 vs 0.688207\n",
      "epoch 0, batch1538/3125, loss0.682202 vs 0.688683\n",
      "epoch 0, batch1539/3125, loss0.682705 vs 0.691338\n",
      "epoch 0, batch1540/3125, loss0.682717 vs 0.689777\n",
      "epoch 0, batch1541/3125, loss0.682886 vs 0.690737\n",
      "epoch 0, batch1542/3125, loss0.682833 vs 0.689014\n",
      "epoch 0, batch1543/3125, loss0.682649 vs 0.690463\n",
      "epoch 0, batch1544/3125, loss0.683009 vs 0.688564\n",
      "epoch 0, batch1545/3125, loss0.682723 vs 0.690358\n",
      "epoch 0, batch1546/3125, loss0.682671 vs 0.692045\n",
      "epoch 0, batch1547/3125, loss0.682552 vs 0.688813\n",
      "epoch 0, batch1548/3125, loss0.682338 vs 0.691164\n",
      "epoch 0, batch1549/3125, loss0.682394 vs 0.694090\n",
      "epoch 0, batch1550/3125, loss0.682470 vs 0.695222\n",
      "epoch 0, batch1551/3125, loss0.682238 vs 0.689059\n",
      "epoch 0, batch1552/3125, loss0.681676 vs 0.692529\n",
      "epoch 0, batch1553/3125, loss0.682325 vs 0.693177\n",
      "epoch 0, batch1554/3125, loss0.681836 vs 0.694109\n",
      "epoch 0, batch1555/3125, loss0.681679 vs 0.690353\n",
      "epoch 0, batch1556/3125, loss0.681552 vs 0.692018\n",
      "epoch 0, batch1557/3125, loss0.681385 vs 0.693539\n",
      "epoch 0, batch1558/3125, loss0.681877 vs 0.693850\n",
      "epoch 0, batch1559/3125, loss0.681801 vs 0.691615\n",
      "epoch 0, batch1560/3125, loss0.682019 vs 0.695566\n",
      "epoch 0, batch1561/3125, loss0.681785 vs 0.694081\n",
      "epoch 0, batch1562/3125, loss0.682043 vs 0.693420\n",
      "epoch 0, batch1563/3125, loss0.682309 vs 0.691233\n",
      "epoch 0, batch1564/3125, loss0.682995 vs 0.694297\n",
      "epoch 0, batch1565/3125, loss0.682833 vs 0.693686\n",
      "epoch 0, batch1566/3125, loss0.683413 vs 0.693539\n",
      "epoch 0, batch1567/3125, loss0.683730 vs 0.692742\n",
      "epoch 0, batch1568/3125, loss0.684436 vs 0.693986\n",
      "epoch 0, batch1569/3125, loss0.684490 vs 0.694181\n",
      "epoch 0, batch1570/3125, loss0.685208 vs 0.695830\n",
      "epoch 0, batch1571/3125, loss0.685602 vs 0.693920\n",
      "epoch 0, batch1572/3125, loss0.686555 vs 0.692719\n",
      "epoch 0, batch1573/3125, loss0.687217 vs 0.693797\n",
      "epoch 0, batch1574/3125, loss0.688021 vs 0.694349\n",
      "epoch 0, batch1575/3125, loss0.688551 vs 0.694146\n",
      "epoch 0, batch1576/3125, loss0.689463 vs 0.693053\n",
      "epoch 0, batch1577/3125, loss0.690182 vs 0.693852\n",
      "epoch 0, batch1578/3125, loss0.691120 vs 0.694572\n",
      "epoch 0, batch1579/3125, loss0.691965 vs 0.694467\n",
      "epoch 0, batch1580/3125, loss0.692697 vs 0.694189\n",
      "epoch 0, batch1581/3125, loss0.693707 vs 0.693608\n",
      "epoch 0, batch1582/3125, loss0.694517 vs 0.693742\n",
      "epoch 0, batch1583/3125, loss0.695536 vs 0.692918\n",
      "epoch 0, batch1584/3125, loss0.696405 vs 0.693223\n",
      "epoch 0, batch1585/3125, loss0.697350 vs 0.694351\n",
      "epoch 0, batch1586/3125, loss0.698135 vs 0.693937\n",
      "epoch 0, batch1587/3125, loss0.699046 vs 0.693411\n",
      "epoch 0, batch1588/3125, loss0.700048 vs 0.693793\n",
      "epoch 0, batch1589/3125, loss0.700832 vs 0.692681\n",
      "epoch 0, batch1590/3125, loss0.701972 vs 0.693274\n",
      "epoch 0, batch1591/3125, loss0.702668 vs 0.693650\n",
      "epoch 0, batch1592/3125, loss0.703444 vs 0.692941\n",
      "epoch 0, batch1593/3125, loss0.704510 vs 0.693996\n",
      "epoch 0, batch1594/3125, loss0.705266 vs 0.696803\n",
      "epoch 0, batch1595/3125, loss0.706046 vs 0.692765\n",
      "epoch 0, batch1596/3125, loss0.706895 vs 0.694725\n",
      "epoch 0, batch1597/3125, loss0.707704 vs 0.693753\n",
      "epoch 0, batch1598/3125, loss0.708384 vs 0.693404\n",
      "epoch 0, batch1599/3125, loss0.709211 vs 0.691431\n",
      "epoch 0, batch1600/3125, loss0.709848 vs 0.691594\n",
      "epoch 0, batch1601/3125, loss0.710576 vs 0.691373\n",
      "epoch 0, batch1602/3125, loss0.711309 vs 0.692602\n",
      "epoch 0, batch1603/3125, loss0.711772 vs 0.691526\n",
      "epoch 0, batch1604/3125, loss0.712538 vs 0.691227\n",
      "epoch 0, batch1605/3125, loss0.712998 vs 0.692108\n",
      "epoch 0, batch1606/3125, loss0.713699 vs 0.691399\n",
      "epoch 0, batch1607/3125, loss0.714047 vs 0.692158\n",
      "epoch 0, batch1608/3125, loss0.714628 vs 0.692798\n",
      "epoch 0, batch1609/3125, loss0.715140 vs 0.691269\n",
      "epoch 0, batch1610/3125, loss0.715528 vs 0.690748\n",
      "epoch 0, batch1611/3125, loss0.715928 vs 0.690187\n",
      "epoch 0, batch1612/3125, loss0.716458 vs 0.694128\n",
      "epoch 0, batch1613/3125, loss0.716846 vs 0.690824\n",
      "epoch 0, batch1614/3125, loss0.717243 vs 0.692575\n",
      "epoch 0, batch1615/3125, loss0.717649 vs 0.691113\n",
      "epoch 0, batch1616/3125, loss0.717868 vs 0.691503\n",
      "epoch 0, batch1617/3125, loss0.718137 vs 0.689840\n",
      "epoch 0, batch1618/3125, loss0.718520 vs 0.689482\n",
      "epoch 0, batch1619/3125, loss0.718790 vs 0.690617\n",
      "epoch 0, batch1620/3125, loss0.719161 vs 0.689122\n",
      "epoch 0, batch1621/3125, loss0.719473 vs 0.690509\n",
      "epoch 0, batch1622/3125, loss0.719755 vs 0.691581\n",
      "epoch 0, batch1623/3125, loss0.720121 vs 0.688952\n",
      "epoch 0, batch1624/3125, loss0.720433 vs 0.690802\n",
      "epoch 0, batch1625/3125, loss0.720527 vs 0.690094\n",
      "epoch 0, batch1626/3125, loss0.720901 vs 0.690143\n",
      "epoch 0, batch1627/3125, loss0.721125 vs 0.686733\n",
      "epoch 0, batch1628/3125, loss0.721473 vs 0.689830\n",
      "epoch 0, batch1629/3125, loss0.721748 vs 0.689984\n",
      "epoch 0, batch1630/3125, loss0.722050 vs 0.689059\n",
      "epoch 0, batch1631/3125, loss0.722217 vs 0.687761\n",
      "epoch 0, batch1632/3125, loss0.722417 vs 0.687837\n",
      "epoch 0, batch1633/3125, loss0.722599 vs 0.688589\n",
      "epoch 0, batch1634/3125, loss0.723007 vs 0.686888\n",
      "epoch 0, batch1635/3125, loss0.723289 vs 0.688434\n",
      "epoch 0, batch1636/3125, loss0.723684 vs 0.688459\n",
      "epoch 0, batch1637/3125, loss0.724031 vs 0.689593\n",
      "epoch 0, batch1638/3125, loss0.724500 vs 0.689481\n",
      "epoch 0, batch1639/3125, loss0.724568 vs 0.687278\n",
      "epoch 0, batch1640/3125, loss0.724917 vs 0.687063\n",
      "epoch 0, batch1641/3125, loss0.725220 vs 0.688253\n",
      "epoch 0, batch1642/3125, loss0.725411 vs 0.684849\n",
      "epoch 0, batch1643/3125, loss0.725704 vs 0.688549\n",
      "epoch 0, batch1644/3125, loss0.726034 vs 0.685125\n",
      "epoch 0, batch1645/3125, loss0.726269 vs 0.685734\n",
      "epoch 0, batch1646/3125, loss0.726438 vs 0.687838\n",
      "epoch 0, batch1647/3125, loss0.726657 vs 0.686518\n",
      "epoch 0, batch1648/3125, loss0.726860 vs 0.687788\n",
      "epoch 0, batch1649/3125, loss0.727022 vs 0.687201\n",
      "epoch 0, batch1650/3125, loss0.727228 vs 0.685665\n",
      "epoch 0, batch1651/3125, loss0.727320 vs 0.685677\n",
      "epoch 0, batch1652/3125, loss0.727184 vs 0.690081\n",
      "epoch 0, batch1653/3125, loss0.727429 vs 0.687823\n",
      "epoch 0, batch1654/3125, loss0.727247 vs 0.686746\n",
      "epoch 0, batch1655/3125, loss0.726992 vs 0.688321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch1656/3125, loss0.726984 vs 0.685105\n",
      "epoch 0, batch1657/3125, loss0.726991 vs 0.687924\n",
      "epoch 0, batch1658/3125, loss0.726327 vs 0.687989\n",
      "epoch 0, batch1659/3125, loss0.726717 vs 0.684669\n",
      "epoch 0, batch1660/3125, loss0.726258 vs 0.686509\n",
      "epoch 0, batch1661/3125, loss0.726215 vs 0.685492\n",
      "epoch 0, batch1662/3125, loss0.726046 vs 0.686899\n",
      "epoch 0, batch1663/3125, loss0.725745 vs 0.686410\n",
      "epoch 0, batch1664/3125, loss0.725660 vs 0.686392\n",
      "epoch 0, batch1665/3125, loss0.725009 vs 0.685047\n",
      "epoch 0, batch1666/3125, loss0.724952 vs 0.688154\n",
      "epoch 0, batch1667/3125, loss0.724693 vs 0.680991\n",
      "epoch 0, batch1668/3125, loss0.724764 vs 0.684649\n",
      "epoch 0, batch1669/3125, loss0.724347 vs 0.686098\n",
      "epoch 0, batch1670/3125, loss0.723984 vs 0.689513\n",
      "epoch 0, batch1671/3125, loss0.723869 vs 0.687787\n",
      "epoch 0, batch1672/3125, loss0.723325 vs 0.687152\n",
      "epoch 0, batch1673/3125, loss0.722727 vs 0.685314\n",
      "epoch 0, batch1674/3125, loss0.722165 vs 0.684502\n",
      "epoch 0, batch1675/3125, loss0.721478 vs 0.686661\n",
      "epoch 0, batch1676/3125, loss0.721007 vs 0.685854\n",
      "epoch 0, batch1677/3125, loss0.720391 vs 0.683820\n",
      "epoch 0, batch1678/3125, loss0.720016 vs 0.687532\n",
      "epoch 0, batch1679/3125, loss0.719058 vs 0.689968\n",
      "epoch 0, batch1680/3125, loss0.718245 vs 0.688260\n",
      "epoch 0, batch1681/3125, loss0.717580 vs 0.686379\n",
      "epoch 0, batch1682/3125, loss0.716603 vs 0.685371\n",
      "epoch 0, batch1683/3125, loss0.715912 vs 0.688867\n",
      "epoch 0, batch1684/3125, loss0.714334 vs 0.683482\n",
      "epoch 0, batch1685/3125, loss0.713598 vs 0.686695\n",
      "epoch 0, batch1686/3125, loss0.712380 vs 0.691039\n",
      "epoch 0, batch1687/3125, loss0.711388 vs 0.694093\n",
      "epoch 0, batch1688/3125, loss0.709264 vs 0.688772\n",
      "epoch 0, batch1689/3125, loss0.707843 vs 0.691747\n",
      "epoch 0, batch1690/3125, loss0.706393 vs 0.695666\n",
      "epoch 0, batch1691/3125, loss0.704282 vs 0.696106\n",
      "epoch 0, batch1692/3125, loss0.701712 vs 0.696815\n",
      "epoch 0, batch1693/3125, loss0.699483 vs 0.694376\n",
      "epoch 0, batch1694/3125, loss0.697420 vs 0.689924\n",
      "epoch 0, batch1695/3125, loss0.694758 vs 0.699230\n",
      "epoch 0, batch1696/3125, loss0.692886 vs 0.697950\n",
      "epoch 0, batch1697/3125, loss0.689716 vs 0.695421\n",
      "epoch 0, batch1698/3125, loss0.687619 vs 0.697907\n",
      "epoch 0, batch1699/3125, loss0.684552 vs 0.701538\n",
      "epoch 0, batch1700/3125, loss0.682199 vs 0.705005\n",
      "epoch 0, batch1701/3125, loss0.679573 vs 0.702986\n",
      "epoch 0, batch1702/3125, loss0.676212 vs 0.705090\n",
      "epoch 0, batch1703/3125, loss0.674101 vs 0.709236\n",
      "epoch 0, batch1704/3125, loss0.671490 vs 0.702574\n",
      "epoch 0, batch1705/3125, loss0.669195 vs 0.704647\n",
      "epoch 0, batch1706/3125, loss0.665890 vs 0.704872\n",
      "epoch 0, batch1707/3125, loss0.663988 vs 0.709809\n",
      "epoch 0, batch1708/3125, loss0.662580 vs 0.713487\n",
      "epoch 0, batch1709/3125, loss0.661119 vs 0.707265\n",
      "epoch 0, batch1710/3125, loss0.659730 vs 0.704376\n",
      "epoch 0, batch1711/3125, loss0.658864 vs 0.708221\n",
      "epoch 0, batch1712/3125, loss0.657966 vs 0.707294\n",
      "epoch 0, batch1713/3125, loss0.657503 vs 0.706649\n",
      "epoch 0, batch1714/3125, loss0.657397 vs 0.708691\n",
      "epoch 0, batch1715/3125, loss0.657456 vs 0.707221\n",
      "epoch 0, batch1716/3125, loss0.657831 vs 0.706861\n",
      "epoch 0, batch1717/3125, loss0.657729 vs 0.709264\n",
      "epoch 0, batch1718/3125, loss0.657617 vs 0.707332\n",
      "epoch 0, batch1719/3125, loss0.657559 vs 0.706165\n",
      "epoch 0, batch1720/3125, loss0.657871 vs 0.705681\n",
      "epoch 0, batch1721/3125, loss0.657917 vs 0.701710\n",
      "epoch 0, batch1722/3125, loss0.658186 vs 0.701193\n",
      "epoch 0, batch1723/3125, loss0.658457 vs 0.705091\n",
      "epoch 0, batch1724/3125, loss0.658686 vs 0.702629\n",
      "epoch 0, batch1725/3125, loss0.658786 vs 0.703107\n",
      "epoch 0, batch1726/3125, loss0.659413 vs 0.700895\n",
      "epoch 0, batch1727/3125, loss0.660137 vs 0.697696\n",
      "epoch 0, batch1728/3125, loss0.660499 vs 0.699104\n",
      "epoch 0, batch1729/3125, loss0.661082 vs 0.699544\n",
      "epoch 0, batch1730/3125, loss0.661649 vs 0.701864\n",
      "epoch 0, batch1731/3125, loss0.662114 vs 0.701855\n",
      "epoch 0, batch1732/3125, loss0.662752 vs 0.697959\n",
      "epoch 0, batch1733/3125, loss0.663250 vs 0.700461\n",
      "epoch 0, batch1734/3125, loss0.663586 vs 0.698831\n",
      "epoch 0, batch1735/3125, loss0.664149 vs 0.699744\n",
      "epoch 0, batch1736/3125, loss0.664596 vs 0.699186\n",
      "epoch 0, batch1737/3125, loss0.664994 vs 0.700350\n",
      "epoch 0, batch1738/3125, loss0.665497 vs 0.697815\n",
      "epoch 0, batch1739/3125, loss0.666005 vs 0.699273\n",
      "epoch 0, batch1740/3125, loss0.666346 vs 0.697637\n",
      "epoch 0, batch1741/3125, loss0.666604 vs 0.697202\n",
      "epoch 0, batch1742/3125, loss0.667181 vs 0.696362\n",
      "epoch 0, batch1743/3125, loss0.667642 vs 0.697785\n",
      "epoch 0, batch1744/3125, loss0.667927 vs 0.697206\n",
      "epoch 0, batch1745/3125, loss0.668280 vs 0.697991\n",
      "epoch 0, batch1746/3125, loss0.668769 vs 0.696829\n",
      "epoch 0, batch1747/3125, loss0.669176 vs 0.696177\n",
      "epoch 0, batch1748/3125, loss0.669725 vs 0.695523\n",
      "epoch 0, batch1749/3125, loss0.670005 vs 0.694602\n",
      "epoch 0, batch1750/3125, loss0.670672 vs 0.695030\n",
      "epoch 0, batch1751/3125, loss0.671116 vs 0.694215\n",
      "epoch 0, batch1752/3125, loss0.671729 vs 0.695894\n",
      "epoch 0, batch1753/3125, loss0.672310 vs 0.694252\n",
      "epoch 0, batch1754/3125, loss0.672891 vs 0.694962\n",
      "epoch 0, batch1755/3125, loss0.673425 vs 0.693532\n",
      "epoch 0, batch1756/3125, loss0.673989 vs 0.692325\n",
      "epoch 0, batch1757/3125, loss0.674617 vs 0.692635\n",
      "epoch 0, batch1758/3125, loss0.675108 vs 0.693573\n",
      "epoch 0, batch1759/3125, loss0.675597 vs 0.693371\n",
      "epoch 0, batch1760/3125, loss0.676172 vs 0.693051\n",
      "epoch 0, batch1761/3125, loss0.676746 vs 0.693817\n",
      "epoch 0, batch1762/3125, loss0.677236 vs 0.691393\n",
      "epoch 0, batch1763/3125, loss0.677756 vs 0.692899\n",
      "epoch 0, batch1764/3125, loss0.678373 vs 0.692616\n",
      "epoch 0, batch1765/3125, loss0.678843 vs 0.691948\n",
      "epoch 0, batch1766/3125, loss0.679389 vs 0.691163\n",
      "epoch 0, batch1767/3125, loss0.679917 vs 0.691339\n",
      "epoch 0, batch1768/3125, loss0.680368 vs 0.690797\n",
      "epoch 0, batch1769/3125, loss0.680805 vs 0.691012\n",
      "epoch 0, batch1770/3125, loss0.681379 vs 0.691383\n",
      "epoch 0, batch1771/3125, loss0.681750 vs 0.689911\n",
      "epoch 0, batch1772/3125, loss0.682236 vs 0.690608\n",
      "epoch 0, batch1773/3125, loss0.682681 vs 0.690459\n",
      "epoch 0, batch1774/3125, loss0.683108 vs 0.690299\n",
      "epoch 0, batch1775/3125, loss0.683494 vs 0.689013\n",
      "epoch 0, batch1776/3125, loss0.683901 vs 0.689218\n",
      "epoch 0, batch1777/3125, loss0.684226 vs 0.689030\n",
      "epoch 0, batch1778/3125, loss0.684575 vs 0.689703\n",
      "epoch 0, batch1779/3125, loss0.684871 vs 0.688976\n",
      "epoch 0, batch1780/3125, loss0.685137 vs 0.688501\n",
      "epoch 0, batch1781/3125, loss0.685426 vs 0.689310\n",
      "epoch 0, batch1782/3125, loss0.685720 vs 0.687592\n",
      "epoch 0, batch1783/3125, loss0.685856 vs 0.686933\n",
      "epoch 0, batch1784/3125, loss0.686226 vs 0.689613\n",
      "epoch 0, batch1785/3125, loss0.686390 vs 0.688664\n",
      "epoch 0, batch1786/3125, loss0.686599 vs 0.689077\n",
      "epoch 0, batch1787/3125, loss0.686860 vs 0.685531\n",
      "epoch 0, batch1788/3125, loss0.687007 vs 0.686248\n",
      "epoch 0, batch1789/3125, loss0.687286 vs 0.687031\n",
      "epoch 0, batch1790/3125, loss0.687594 vs 0.686909\n",
      "epoch 0, batch1791/3125, loss0.687597 vs 0.686101\n",
      "epoch 0, batch1792/3125, loss0.687795 vs 0.684703\n",
      "epoch 0, batch1793/3125, loss0.688159 vs 0.684331\n",
      "epoch 0, batch1794/3125, loss0.688289 vs 0.686637\n",
      "epoch 0, batch1795/3125, loss0.688554 vs 0.685993\n",
      "epoch 0, batch1796/3125, loss0.688606 vs 0.684945\n",
      "epoch 0, batch1797/3125, loss0.689053 vs 0.685254\n",
      "epoch 0, batch1798/3125, loss0.689097 vs 0.684857\n",
      "epoch 0, batch1799/3125, loss0.689176 vs 0.683739\n",
      "epoch 0, batch1800/3125, loss0.689420 vs 0.685291\n",
      "epoch 0, batch1801/3125, loss0.689600 vs 0.686163\n",
      "epoch 0, batch1802/3125, loss0.689380 vs 0.681178\n",
      "epoch 0, batch1803/3125, loss0.689318 vs 0.683180\n",
      "epoch 0, batch1804/3125, loss0.689371 vs 0.683581\n",
      "epoch 0, batch1805/3125, loss0.689351 vs 0.681158\n",
      "epoch 0, batch1806/3125, loss0.689425 vs 0.681629\n",
      "epoch 0, batch1807/3125, loss0.689056 vs 0.684812\n",
      "epoch 0, batch1808/3125, loss0.688929 vs 0.680585\n",
      "epoch 0, batch1809/3125, loss0.688674 vs 0.684022\n",
      "epoch 0, batch1810/3125, loss0.687856 vs 0.681114\n",
      "epoch 0, batch1811/3125, loss0.687338 vs 0.678471\n",
      "epoch 0, batch1812/3125, loss0.686682 vs 0.686909\n",
      "epoch 0, batch1813/3125, loss0.686123 vs 0.680639\n",
      "epoch 0, batch1814/3125, loss0.685426 vs 0.682538\n",
      "epoch 0, batch1815/3125, loss0.684538 vs 0.681540\n",
      "epoch 0, batch1816/3125, loss0.683898 vs 0.681335\n",
      "epoch 0, batch1817/3125, loss0.683369 vs 0.682005\n",
      "epoch 0, batch1818/3125, loss0.682075 vs 0.680212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch1819/3125, loss0.681280 vs 0.684242\n",
      "epoch 0, batch1820/3125, loss0.680291 vs 0.683715\n",
      "epoch 0, batch1821/3125, loss0.678675 vs 0.684205\n",
      "epoch 0, batch1822/3125, loss0.678135 vs 0.685081\n",
      "epoch 0, batch1823/3125, loss0.676070 vs 0.687911\n",
      "epoch 0, batch1824/3125, loss0.675564 vs 0.687828\n",
      "epoch 0, batch1825/3125, loss0.673461 vs 0.685797\n",
      "epoch 0, batch1826/3125, loss0.671102 vs 0.691071\n",
      "epoch 0, batch1827/3125, loss0.670116 vs 0.692520\n",
      "epoch 0, batch1828/3125, loss0.668146 vs 0.694975\n",
      "epoch 0, batch1829/3125, loss0.666860 vs 0.686930\n",
      "epoch 0, batch1830/3125, loss0.666075 vs 0.690137\n",
      "epoch 0, batch1831/3125, loss0.664441 vs 0.690781\n",
      "epoch 0, batch1832/3125, loss0.664157 vs 0.689784\n",
      "epoch 0, batch1833/3125, loss0.662460 vs 0.699289\n",
      "epoch 0, batch1834/3125, loss0.661228 vs 0.693533\n",
      "epoch 0, batch1835/3125, loss0.660194 vs 0.696986\n",
      "epoch 0, batch1836/3125, loss0.660995 vs 0.695308\n",
      "epoch 0, batch1837/3125, loss0.660214 vs 0.695380\n",
      "epoch 0, batch1838/3125, loss0.660432 vs 0.696287\n",
      "epoch 0, batch1839/3125, loss0.660316 vs 0.698264\n",
      "epoch 0, batch1840/3125, loss0.660930 vs 0.702228\n",
      "epoch 0, batch1841/3125, loss0.661843 vs 0.697122\n",
      "epoch 0, batch1842/3125, loss0.662470 vs 0.698254\n",
      "epoch 0, batch1843/3125, loss0.662934 vs 0.702907\n",
      "epoch 0, batch1844/3125, loss0.664154 vs 0.696279\n",
      "epoch 0, batch1845/3125, loss0.665044 vs 0.697503\n",
      "epoch 0, batch1846/3125, loss0.665714 vs 0.700008\n",
      "epoch 0, batch1847/3125, loss0.667290 vs 0.703359\n",
      "epoch 0, batch1848/3125, loss0.668672 vs 0.697345\n",
      "epoch 0, batch1849/3125, loss0.669796 vs 0.699164\n",
      "epoch 0, batch1850/3125, loss0.671089 vs 0.695540\n",
      "epoch 0, batch1851/3125, loss0.672641 vs 0.703504\n",
      "epoch 0, batch1852/3125, loss0.674239 vs 0.701686\n",
      "epoch 0, batch1853/3125, loss0.675863 vs 0.701090\n",
      "epoch 0, batch1854/3125, loss0.677347 vs 0.698486\n",
      "epoch 0, batch1855/3125, loss0.678460 vs 0.698639\n",
      "epoch 0, batch1856/3125, loss0.679517 vs 0.703116\n",
      "epoch 0, batch1857/3125, loss0.680124 vs 0.700919\n",
      "epoch 0, batch1858/3125, loss0.680646 vs 0.703186\n",
      "epoch 0, batch1859/3125, loss0.681315 vs 0.700564\n",
      "epoch 0, batch1860/3125, loss0.682246 vs 0.702983\n",
      "epoch 0, batch1861/3125, loss0.682949 vs 0.704171\n",
      "epoch 0, batch1862/3125, loss0.684210 vs 0.699442\n",
      "epoch 0, batch1863/3125, loss0.685029 vs 0.702912\n",
      "epoch 0, batch1864/3125, loss0.686106 vs 0.709639\n",
      "epoch 0, batch1865/3125, loss0.687250 vs 0.705835\n",
      "epoch 0, batch1866/3125, loss0.688851 vs 0.706639\n",
      "epoch 0, batch1867/3125, loss0.689685 vs 0.707210\n",
      "epoch 0, batch1868/3125, loss0.691342 vs 0.703296\n",
      "epoch 0, batch1869/3125, loss0.692797 vs 0.702761\n",
      "epoch 0, batch1870/3125, loss0.693969 vs 0.703136\n",
      "epoch 0, batch1871/3125, loss0.695585 vs 0.703115\n",
      "epoch 0, batch1872/3125, loss0.697040 vs 0.705552\n",
      "epoch 0, batch1873/3125, loss0.698263 vs 0.700976\n",
      "epoch 0, batch1874/3125, loss0.699596 vs 0.701711\n",
      "epoch 0, batch1875/3125, loss0.700988 vs 0.702790\n",
      "epoch 0, batch1876/3125, loss0.702589 vs 0.702137\n",
      "epoch 0, batch1877/3125, loss0.704001 vs 0.701757\n",
      "epoch 0, batch1878/3125, loss0.705498 vs 0.699987\n",
      "epoch 0, batch1879/3125, loss0.706805 vs 0.698742\n",
      "epoch 0, batch1880/3125, loss0.707953 vs 0.698312\n",
      "epoch 0, batch1881/3125, loss0.708998 vs 0.700038\n",
      "epoch 0, batch1882/3125, loss0.709929 vs 0.697325\n",
      "epoch 0, batch1883/3125, loss0.710873 vs 0.705521\n",
      "epoch 0, batch1884/3125, loss0.711618 vs 0.695919\n",
      "epoch 0, batch1885/3125, loss0.712160 vs 0.697303\n",
      "epoch 0, batch1886/3125, loss0.713043 vs 0.696274\n",
      "epoch 0, batch1887/3125, loss0.713377 vs 0.696899\n",
      "epoch 0, batch1888/3125, loss0.713901 vs 0.694575\n",
      "epoch 0, batch1889/3125, loss0.714194 vs 0.697186\n",
      "epoch 0, batch1890/3125, loss0.714625 vs 0.695592\n",
      "epoch 0, batch1891/3125, loss0.714806 vs 0.695913\n",
      "epoch 0, batch1892/3125, loss0.714883 vs 0.696240\n",
      "epoch 0, batch1893/3125, loss0.715064 vs 0.694415\n",
      "epoch 0, batch1894/3125, loss0.715080 vs 0.694438\n",
      "epoch 0, batch1895/3125, loss0.715146 vs 0.693663\n",
      "epoch 0, batch1896/3125, loss0.715028 vs 0.696501\n",
      "epoch 0, batch1897/3125, loss0.715010 vs 0.693664\n",
      "epoch 0, batch1898/3125, loss0.714791 vs 0.693534\n",
      "epoch 0, batch1899/3125, loss0.714641 vs 0.693260\n",
      "epoch 0, batch1900/3125, loss0.714461 vs 0.693355\n",
      "epoch 0, batch1901/3125, loss0.714296 vs 0.693353\n",
      "epoch 0, batch1902/3125, loss0.713901 vs 0.693530\n",
      "epoch 0, batch1903/3125, loss0.713705 vs 0.694429\n",
      "epoch 0, batch1904/3125, loss0.713287 vs 0.694094\n",
      "epoch 0, batch1905/3125, loss0.712982 vs 0.695452\n",
      "epoch 0, batch1906/3125, loss0.712617 vs 0.694766\n",
      "epoch 0, batch1907/3125, loss0.712463 vs 0.693342\n",
      "epoch 0, batch1908/3125, loss0.711994 vs 0.692749\n",
      "epoch 0, batch1909/3125, loss0.711576 vs 0.693405\n",
      "epoch 0, batch1910/3125, loss0.711415 vs 0.692533\n",
      "epoch 0, batch1911/3125, loss0.711204 vs 0.694104\n",
      "epoch 0, batch1912/3125, loss0.710931 vs 0.692261\n",
      "epoch 0, batch1913/3125, loss0.710629 vs 0.692363\n",
      "epoch 0, batch1914/3125, loss0.710461 vs 0.693445\n",
      "epoch 0, batch1915/3125, loss0.710384 vs 0.691531\n",
      "epoch 0, batch1916/3125, loss0.709979 vs 0.692041\n",
      "epoch 0, batch1917/3125, loss0.709845 vs 0.691096\n",
      "epoch 0, batch1918/3125, loss0.709921 vs 0.691726\n",
      "epoch 0, batch1919/3125, loss0.709933 vs 0.691146\n",
      "epoch 0, batch1920/3125, loss0.710022 vs 0.691673\n",
      "epoch 0, batch1921/3125, loss0.710165 vs 0.691215\n",
      "epoch 0, batch1922/3125, loss0.710234 vs 0.690155\n",
      "epoch 0, batch1923/3125, loss0.710587 vs 0.690018\n",
      "epoch 0, batch1924/3125, loss0.711042 vs 0.689032\n",
      "epoch 0, batch1925/3125, loss0.711167 vs 0.689306\n",
      "epoch 0, batch1926/3125, loss0.711629 vs 0.688127\n",
      "epoch 0, batch1927/3125, loss0.711899 vs 0.689044\n",
      "epoch 0, batch1928/3125, loss0.712695 vs 0.687558\n",
      "epoch 0, batch1929/3125, loss0.712848 vs 0.687618\n",
      "epoch 0, batch1930/3125, loss0.713456 vs 0.688899\n",
      "epoch 0, batch1931/3125, loss0.714097 vs 0.686459\n",
      "epoch 0, batch1932/3125, loss0.714688 vs 0.685455\n",
      "epoch 0, batch1933/3125, loss0.715384 vs 0.685473\n",
      "epoch 0, batch1934/3125, loss0.716121 vs 0.685601\n",
      "epoch 0, batch1935/3125, loss0.716710 vs 0.684010\n",
      "epoch 0, batch1936/3125, loss0.717758 vs 0.683669\n",
      "epoch 0, batch1937/3125, loss0.718944 vs 0.683659\n",
      "epoch 0, batch1938/3125, loss0.721021 vs 0.681722\n",
      "epoch 0, batch1939/3125, loss0.722707 vs 0.680439\n",
      "epoch 0, batch1940/3125, loss0.725191 vs 0.681851\n",
      "epoch 0, batch1941/3125, loss0.726719 vs 0.680688\n",
      "epoch 0, batch1942/3125, loss0.728261 vs 0.679947\n",
      "epoch 0, batch1943/3125, loss0.730367 vs 0.683024\n",
      "epoch 0, batch1944/3125, loss0.730007 vs 0.681805\n",
      "epoch 0, batch1945/3125, loss0.730536 vs 0.680391\n",
      "epoch 0, batch1946/3125, loss0.729811 vs 0.680357\n",
      "epoch 0, batch1947/3125, loss0.727538 vs 0.682598\n",
      "epoch 0, batch1948/3125, loss0.723068 vs 0.685449\n",
      "epoch 0, batch1949/3125, loss0.718576 vs 0.689137\n",
      "epoch 0, batch1950/3125, loss0.714307 vs 0.696972\n",
      "epoch 0, batch1951/3125, loss0.710392 vs 0.689668\n",
      "epoch 0, batch1952/3125, loss0.707217 vs 0.689123\n",
      "epoch 0, batch1953/3125, loss0.701105 vs 0.699634\n",
      "epoch 0, batch1954/3125, loss0.695825 vs 0.701369\n",
      "epoch 0, batch1955/3125, loss0.692687 vs 0.695305\n",
      "epoch 0, batch1956/3125, loss0.691444 vs 0.693598\n",
      "epoch 0, batch1957/3125, loss0.691137 vs 0.696068\n",
      "epoch 0, batch1958/3125, loss0.690683 vs 0.692776\n",
      "epoch 0, batch1959/3125, loss0.690296 vs 0.691362\n",
      "epoch 0, batch1960/3125, loss0.689970 vs 0.691951\n",
      "epoch 0, batch1961/3125, loss0.689422 vs 0.693237\n",
      "epoch 0, batch1962/3125, loss0.689062 vs 0.691419\n",
      "epoch 0, batch1963/3125, loss0.688932 vs 0.691042\n",
      "epoch 0, batch1964/3125, loss0.688842 vs 0.690314\n",
      "epoch 0, batch1965/3125, loss0.688709 vs 0.692110\n",
      "epoch 0, batch1966/3125, loss0.688638 vs 0.690625\n",
      "epoch 0, batch1967/3125, loss0.688444 vs 0.690986\n",
      "epoch 0, batch1968/3125, loss0.688621 vs 0.690578\n",
      "epoch 0, batch1969/3125, loss0.688514 vs 0.690190\n",
      "epoch 0, batch1970/3125, loss0.688803 vs 0.690434\n",
      "epoch 0, batch1971/3125, loss0.689011 vs 0.692105\n",
      "epoch 0, batch1972/3125, loss0.689380 vs 0.689775\n",
      "epoch 0, batch1973/3125, loss0.689442 vs 0.690155\n",
      "epoch 0, batch1974/3125, loss0.689574 vs 0.690585\n",
      "epoch 0, batch1975/3125, loss0.689955 vs 0.689552\n",
      "epoch 0, batch1976/3125, loss0.690304 vs 0.689876\n",
      "epoch 0, batch1977/3125, loss0.690454 vs 0.690637\n",
      "epoch 0, batch1978/3125, loss0.690713 vs 0.691296\n",
      "epoch 0, batch1979/3125, loss0.691165 vs 0.690361\n",
      "epoch 0, batch1980/3125, loss0.691503 vs 0.690131\n",
      "epoch 0, batch1981/3125, loss0.691997 vs 0.689489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch1982/3125, loss0.692316 vs 0.690076\n",
      "epoch 0, batch1983/3125, loss0.692716 vs 0.689922\n",
      "epoch 0, batch1984/3125, loss0.693100 vs 0.689534\n",
      "epoch 0, batch1985/3125, loss0.693512 vs 0.689100\n",
      "epoch 0, batch1986/3125, loss0.693824 vs 0.689991\n",
      "epoch 0, batch1987/3125, loss0.694141 vs 0.690884\n",
      "epoch 0, batch1988/3125, loss0.694575 vs 0.687850\n",
      "epoch 0, batch1989/3125, loss0.695013 vs 0.689398\n",
      "epoch 0, batch1990/3125, loss0.695309 vs 0.688155\n",
      "epoch 0, batch1991/3125, loss0.695553 vs 0.690697\n",
      "epoch 0, batch1992/3125, loss0.695948 vs 0.689236\n",
      "epoch 0, batch1993/3125, loss0.696091 vs 0.689770\n",
      "epoch 0, batch1994/3125, loss0.696468 vs 0.688728\n",
      "epoch 0, batch1995/3125, loss0.696726 vs 0.689305\n",
      "epoch 0, batch1996/3125, loss0.696998 vs 0.689138\n",
      "epoch 0, batch1997/3125, loss0.697087 vs 0.689379\n",
      "epoch 0, batch1998/3125, loss0.697396 vs 0.690386\n",
      "epoch 0, batch1999/3125, loss0.697458 vs 0.689024\n",
      "epoch 0, batch2000/3125, loss0.697858 vs 0.689440\n",
      "epoch 0, batch2001/3125, loss0.697733 vs 0.688363\n",
      "epoch 0, batch2002/3125, loss0.698069 vs 0.691215\n",
      "epoch 0, batch2003/3125, loss0.698069 vs 0.689127\n",
      "epoch 0, batch2004/3125, loss0.698044 vs 0.689228\n",
      "epoch 0, batch2005/3125, loss0.698135 vs 0.690344\n",
      "epoch 0, batch2006/3125, loss0.698286 vs 0.690272\n",
      "epoch 0, batch2007/3125, loss0.698407 vs 0.690359\n",
      "epoch 0, batch2008/3125, loss0.698355 vs 0.688070\n",
      "epoch 0, batch2009/3125, loss0.698090 vs 0.690691\n",
      "epoch 0, batch2010/3125, loss0.697867 vs 0.690250\n",
      "epoch 0, batch2011/3125, loss0.698023 vs 0.691030\n",
      "epoch 0, batch2012/3125, loss0.697851 vs 0.691080\n",
      "epoch 0, batch2013/3125, loss0.697844 vs 0.691048\n",
      "epoch 0, batch2014/3125, loss0.697546 vs 0.689285\n",
      "epoch 0, batch2015/3125, loss0.697626 vs 0.690398\n",
      "epoch 0, batch2016/3125, loss0.697360 vs 0.690088\n",
      "epoch 0, batch2017/3125, loss0.696970 vs 0.690855\n",
      "epoch 0, batch2018/3125, loss0.696655 vs 0.691258\n",
      "epoch 0, batch2019/3125, loss0.696560 vs 0.690501\n",
      "epoch 0, batch2020/3125, loss0.696252 vs 0.695382\n",
      "epoch 0, batch2021/3125, loss0.695864 vs 0.691180\n",
      "epoch 0, batch2022/3125, loss0.695569 vs 0.691198\n",
      "epoch 0, batch2023/3125, loss0.695609 vs 0.692191\n",
      "epoch 0, batch2024/3125, loss0.694945 vs 0.690263\n",
      "epoch 0, batch2025/3125, loss0.694827 vs 0.692397\n",
      "epoch 0, batch2026/3125, loss0.694490 vs 0.691316\n",
      "epoch 0, batch2027/3125, loss0.694035 vs 0.692497\n",
      "epoch 0, batch2028/3125, loss0.694231 vs 0.691993\n",
      "epoch 0, batch2029/3125, loss0.693511 vs 0.692978\n",
      "epoch 0, batch2030/3125, loss0.693554 vs 0.692376\n",
      "epoch 0, batch2031/3125, loss0.693295 vs 0.693355\n",
      "epoch 0, batch2032/3125, loss0.692695 vs 0.693779\n",
      "epoch 0, batch2033/3125, loss0.692625 vs 0.693291\n",
      "epoch 0, batch2034/3125, loss0.692386 vs 0.692986\n",
      "epoch 0, batch2035/3125, loss0.692136 vs 0.697771\n",
      "epoch 0, batch2036/3125, loss0.691724 vs 0.693353\n",
      "epoch 0, batch2037/3125, loss0.691036 vs 0.693056\n",
      "epoch 0, batch2038/3125, loss0.690773 vs 0.693226\n",
      "epoch 0, batch2039/3125, loss0.690501 vs 0.693752\n",
      "epoch 0, batch2040/3125, loss0.690326 vs 0.693596\n",
      "epoch 0, batch2041/3125, loss0.689995 vs 0.694347\n",
      "epoch 0, batch2042/3125, loss0.689870 vs 0.694365\n",
      "epoch 0, batch2043/3125, loss0.689406 vs 0.692532\n",
      "epoch 0, batch2044/3125, loss0.689191 vs 0.693348\n",
      "epoch 0, batch2045/3125, loss0.689123 vs 0.694693\n",
      "epoch 0, batch2046/3125, loss0.688686 vs 0.694907\n",
      "epoch 0, batch2047/3125, loss0.688253 vs 0.695463\n",
      "epoch 0, batch2048/3125, loss0.687991 vs 0.695208\n",
      "epoch 0, batch2049/3125, loss0.687811 vs 0.693671\n",
      "epoch 0, batch2050/3125, loss0.687521 vs 0.693887\n",
      "epoch 0, batch2051/3125, loss0.687318 vs 0.694058\n",
      "epoch 0, batch2052/3125, loss0.687014 vs 0.700357\n",
      "epoch 0, batch2053/3125, loss0.686912 vs 0.696904\n",
      "epoch 0, batch2054/3125, loss0.687124 vs 0.693891\n",
      "epoch 0, batch2055/3125, loss0.686750 vs 0.695118\n",
      "epoch 0, batch2056/3125, loss0.686984 vs 0.694467\n",
      "epoch 0, batch2057/3125, loss0.686670 vs 0.694524\n",
      "epoch 0, batch2058/3125, loss0.686551 vs 0.695945\n",
      "epoch 0, batch2059/3125, loss0.686390 vs 0.695004\n",
      "epoch 0, batch2060/3125, loss0.686374 vs 0.695850\n",
      "epoch 0, batch2061/3125, loss0.686211 vs 0.694448\n",
      "epoch 0, batch2062/3125, loss0.686270 vs 0.694682\n",
      "epoch 0, batch2063/3125, loss0.686300 vs 0.694919\n",
      "epoch 0, batch2064/3125, loss0.686172 vs 0.695049\n",
      "epoch 0, batch2065/3125, loss0.686341 vs 0.694874\n",
      "epoch 0, batch2066/3125, loss0.686386 vs 0.694322\n",
      "epoch 0, batch2067/3125, loss0.686433 vs 0.695364\n",
      "epoch 0, batch2068/3125, loss0.686540 vs 0.694199\n",
      "epoch 0, batch2069/3125, loss0.686585 vs 0.694455\n",
      "epoch 0, batch2070/3125, loss0.686764 vs 0.693835\n",
      "epoch 0, batch2071/3125, loss0.686927 vs 0.695086\n",
      "epoch 0, batch2072/3125, loss0.686966 vs 0.693165\n",
      "epoch 0, batch2073/3125, loss0.687173 vs 0.693968\n",
      "epoch 0, batch2074/3125, loss0.687421 vs 0.695232\n",
      "epoch 0, batch2075/3125, loss0.687486 vs 0.694313\n",
      "epoch 0, batch2076/3125, loss0.687514 vs 0.695167\n",
      "epoch 0, batch2077/3125, loss0.687745 vs 0.694229\n",
      "epoch 0, batch2078/3125, loss0.687906 vs 0.693308\n",
      "epoch 0, batch2079/3125, loss0.688066 vs 0.694128\n",
      "epoch 0, batch2080/3125, loss0.688185 vs 0.693735\n",
      "epoch 0, batch2081/3125, loss0.688392 vs 0.693654\n",
      "epoch 0, batch2082/3125, loss0.688444 vs 0.693825\n",
      "epoch 0, batch2083/3125, loss0.688696 vs 0.693585\n",
      "epoch 0, batch2084/3125, loss0.688911 vs 0.704179\n",
      "epoch 0, batch2085/3125, loss0.688973 vs 0.693403\n",
      "epoch 0, batch2086/3125, loss0.689131 vs 0.696385\n",
      "epoch 0, batch2087/3125, loss0.689240 vs 0.692840\n",
      "epoch 0, batch2088/3125, loss0.689427 vs 0.693712\n",
      "epoch 0, batch2089/3125, loss0.689477 vs 0.692510\n",
      "epoch 0, batch2090/3125, loss0.689613 vs 0.693481\n",
      "epoch 0, batch2091/3125, loss0.689808 vs 0.692454\n",
      "epoch 0, batch2092/3125, loss0.689913 vs 0.691672\n",
      "epoch 0, batch2093/3125, loss0.690014 vs 0.692921\n",
      "epoch 0, batch2094/3125, loss0.690192 vs 0.693218\n",
      "epoch 0, batch2095/3125, loss0.690303 vs 0.693028\n",
      "epoch 0, batch2096/3125, loss0.690446 vs 0.692581\n",
      "epoch 0, batch2097/3125, loss0.690643 vs 0.692467\n",
      "epoch 0, batch2098/3125, loss0.690729 vs 0.692847\n",
      "epoch 0, batch2099/3125, loss0.690878 vs 0.692117\n",
      "epoch 0, batch2100/3125, loss0.691003 vs 0.692158\n",
      "epoch 0, batch2101/3125, loss0.691107 vs 0.692250\n",
      "epoch 0, batch2102/3125, loss0.691215 vs 0.692477\n",
      "epoch 0, batch2103/3125, loss0.691299 vs 0.691563\n",
      "epoch 0, batch2104/3125, loss0.691432 vs 0.691530\n",
      "epoch 0, batch2105/3125, loss0.691611 vs 0.691572\n",
      "epoch 0, batch2106/3125, loss0.691710 vs 0.692348\n",
      "epoch 0, batch2107/3125, loss0.691842 vs 0.691928\n",
      "epoch 0, batch2108/3125, loss0.691926 vs 0.692324\n",
      "epoch 0, batch2109/3125, loss0.692113 vs 0.693272\n",
      "epoch 0, batch2110/3125, loss0.692243 vs 0.691735\n",
      "epoch 0, batch2111/3125, loss0.692285 vs 0.692053\n",
      "epoch 0, batch2112/3125, loss0.692424 vs 0.692136\n",
      "epoch 0, batch2113/3125, loss0.692567 vs 0.691368\n",
      "epoch 0, batch2114/3125, loss0.692587 vs 0.692186\n",
      "epoch 0, batch2115/3125, loss0.692717 vs 0.691678\n",
      "epoch 0, batch2116/3125, loss0.692777 vs 0.691823\n",
      "epoch 0, batch2117/3125, loss0.692868 vs 0.691832\n",
      "epoch 0, batch2118/3125, loss0.692972 vs 0.691557\n",
      "epoch 0, batch2119/3125, loss0.693067 vs 0.691761\n",
      "epoch 0, batch2120/3125, loss0.693142 vs 0.691128\n",
      "epoch 0, batch2121/3125, loss0.693172 vs 0.691362\n",
      "epoch 0, batch2122/3125, loss0.693346 vs 0.690726\n",
      "epoch 0, batch2123/3125, loss0.693408 vs 0.691066\n",
      "epoch 0, batch2124/3125, loss0.693523 vs 0.691620\n",
      "epoch 0, batch2125/3125, loss0.693513 vs 0.691076\n",
      "epoch 0, batch2126/3125, loss0.693628 vs 0.691232\n",
      "epoch 0, batch2127/3125, loss0.693694 vs 0.691310\n",
      "epoch 0, batch2128/3125, loss0.693873 vs 0.690923\n",
      "epoch 0, batch2129/3125, loss0.693921 vs 0.692699\n",
      "epoch 0, batch2130/3125, loss0.693940 vs 0.691364\n",
      "epoch 0, batch2131/3125, loss0.693988 vs 0.691060\n",
      "epoch 0, batch2132/3125, loss0.694116 vs 0.690231\n",
      "epoch 0, batch2133/3125, loss0.694097 vs 0.691069\n",
      "epoch 0, batch2134/3125, loss0.694204 vs 0.691425\n",
      "epoch 0, batch2135/3125, loss0.694270 vs 0.691399\n",
      "epoch 0, batch2136/3125, loss0.694326 vs 0.690815\n",
      "epoch 0, batch2137/3125, loss0.694332 vs 0.691115\n",
      "epoch 0, batch2138/3125, loss0.694411 vs 0.690545\n",
      "epoch 0, batch2139/3125, loss0.694485 vs 0.690064\n",
      "epoch 0, batch2140/3125, loss0.694456 vs 0.690530\n",
      "epoch 0, batch2141/3125, loss0.694572 vs 0.690912\n",
      "epoch 0, batch2142/3125, loss0.694641 vs 0.691060\n",
      "epoch 0, batch2143/3125, loss0.694606 vs 0.690519\n",
      "epoch 0, batch2144/3125, loss0.694764 vs 0.692377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch2145/3125, loss0.694782 vs 0.690295\n",
      "epoch 0, batch2146/3125, loss0.694764 vs 0.690487\n",
      "epoch 0, batch2147/3125, loss0.694855 vs 0.689764\n",
      "epoch 0, batch2148/3125, loss0.694845 vs 0.690092\n",
      "epoch 0, batch2149/3125, loss0.694928 vs 0.690483\n",
      "epoch 0, batch2150/3125, loss0.694927 vs 0.690381\n",
      "epoch 0, batch2151/3125, loss0.694994 vs 0.691025\n",
      "epoch 0, batch2152/3125, loss0.695073 vs 0.690037\n",
      "epoch 0, batch2153/3125, loss0.695104 vs 0.690529\n",
      "epoch 0, batch2154/3125, loss0.695052 vs 0.690612\n",
      "epoch 0, batch2155/3125, loss0.695162 vs 0.692249\n",
      "epoch 0, batch2156/3125, loss0.695151 vs 0.690889\n",
      "epoch 0, batch2157/3125, loss0.695095 vs 0.689889\n",
      "epoch 0, batch2158/3125, loss0.695164 vs 0.690568\n",
      "epoch 0, batch2159/3125, loss0.695185 vs 0.690026\n",
      "epoch 0, batch2160/3125, loss0.695200 vs 0.690629\n",
      "epoch 0, batch2161/3125, loss0.695158 vs 0.691191\n",
      "epoch 0, batch2162/3125, loss0.695107 vs 0.689779\n",
      "epoch 0, batch2163/3125, loss0.695156 vs 0.689876\n",
      "epoch 0, batch2164/3125, loss0.695100 vs 0.690134\n",
      "epoch 0, batch2165/3125, loss0.695101 vs 0.690489\n",
      "epoch 0, batch2166/3125, loss0.695163 vs 0.690776\n",
      "epoch 0, batch2167/3125, loss0.695073 vs 0.690615\n",
      "epoch 0, batch2168/3125, loss0.695092 vs 0.690257\n",
      "epoch 0, batch2169/3125, loss0.695152 vs 0.689555\n",
      "epoch 0, batch2170/3125, loss0.695094 vs 0.690341\n",
      "epoch 0, batch2171/3125, loss0.695102 vs 0.690867\n",
      "epoch 0, batch2172/3125, loss0.695125 vs 0.690079\n",
      "epoch 0, batch2173/3125, loss0.695104 vs 0.690004\n",
      "epoch 0, batch2174/3125, loss0.695057 vs 0.690002\n",
      "epoch 0, batch2175/3125, loss0.695143 vs 0.690641\n",
      "epoch 0, batch2176/3125, loss0.695119 vs 0.689939\n",
      "epoch 0, batch2177/3125, loss0.695158 vs 0.690149\n",
      "epoch 0, batch2178/3125, loss0.695144 vs 0.692715\n",
      "epoch 0, batch2179/3125, loss0.695061 vs 0.689834\n",
      "epoch 0, batch2180/3125, loss0.695038 vs 0.690193\n",
      "epoch 0, batch2181/3125, loss0.695096 vs 0.689418\n",
      "epoch 0, batch2182/3125, loss0.695078 vs 0.688867\n",
      "epoch 0, batch2183/3125, loss0.695008 vs 0.689237\n",
      "epoch 0, batch2184/3125, loss0.694936 vs 0.690001\n",
      "epoch 0, batch2185/3125, loss0.694890 vs 0.689636\n",
      "epoch 0, batch2186/3125, loss0.694968 vs 0.690032\n",
      "epoch 0, batch2187/3125, loss0.694983 vs 0.690050\n",
      "epoch 0, batch2188/3125, loss0.694918 vs 0.689461\n",
      "epoch 0, batch2189/3125, loss0.694974 vs 0.689911\n",
      "epoch 0, batch2190/3125, loss0.694843 vs 0.689335\n",
      "epoch 0, batch2191/3125, loss0.694792 vs 0.690438\n",
      "epoch 0, batch2192/3125, loss0.694827 vs 0.690312\n",
      "epoch 0, batch2193/3125, loss0.694869 vs 0.690234\n",
      "epoch 0, batch2194/3125, loss0.694873 vs 0.688846\n",
      "epoch 0, batch2195/3125, loss0.694730 vs 0.689948\n",
      "epoch 0, batch2196/3125, loss0.694796 vs 0.688753\n",
      "epoch 0, batch2197/3125, loss0.694864 vs 0.689016\n",
      "epoch 0, batch2198/3125, loss0.694706 vs 0.690265\n",
      "epoch 0, batch2199/3125, loss0.694729 vs 0.687807\n",
      "epoch 0, batch2200/3125, loss0.694595 vs 0.690090\n",
      "epoch 0, batch2201/3125, loss0.694683 vs 0.689096\n",
      "epoch 0, batch2202/3125, loss0.694666 vs 0.689036\n",
      "epoch 0, batch2203/3125, loss0.694580 vs 0.689212\n",
      "epoch 0, batch2204/3125, loss0.694403 vs 0.689113\n",
      "epoch 0, batch2205/3125, loss0.694589 vs 0.688211\n",
      "epoch 0, batch2206/3125, loss0.694679 vs 0.689438\n",
      "epoch 0, batch2207/3125, loss0.694428 vs 0.689184\n",
      "epoch 0, batch2208/3125, loss0.694508 vs 0.688421\n",
      "epoch 0, batch2209/3125, loss0.694432 vs 0.691989\n",
      "epoch 0, batch2210/3125, loss0.694181 vs 0.689539\n",
      "epoch 0, batch2211/3125, loss0.694046 vs 0.688904\n",
      "epoch 0, batch2212/3125, loss0.694001 vs 0.688217\n",
      "epoch 0, batch2213/3125, loss0.693912 vs 0.692691\n",
      "epoch 0, batch2214/3125, loss0.693669 vs 0.689630\n",
      "epoch 0, batch2215/3125, loss0.693552 vs 0.688858\n",
      "epoch 0, batch2216/3125, loss0.693663 vs 0.689364\n",
      "epoch 0, batch2217/3125, loss0.693129 vs 0.690143\n",
      "epoch 0, batch2218/3125, loss0.692892 vs 0.690196\n",
      "epoch 0, batch2219/3125, loss0.692713 vs 0.688855\n",
      "epoch 0, batch2220/3125, loss0.692580 vs 0.689996\n",
      "epoch 0, batch2221/3125, loss0.692301 vs 0.692279\n",
      "epoch 0, batch2222/3125, loss0.692254 vs 0.688994\n",
      "epoch 0, batch2223/3125, loss0.691769 vs 0.689850\n",
      "epoch 0, batch2224/3125, loss0.691925 vs 0.689939\n",
      "epoch 0, batch2225/3125, loss0.691630 vs 0.689773\n",
      "epoch 0, batch2226/3125, loss0.691277 vs 0.688456\n",
      "epoch 0, batch2227/3125, loss0.691087 vs 0.689123\n",
      "epoch 0, batch2228/3125, loss0.690834 vs 0.689346\n",
      "epoch 0, batch2229/3125, loss0.690670 vs 0.689884\n",
      "epoch 0, batch2230/3125, loss0.690419 vs 0.689325\n",
      "epoch 0, batch2231/3125, loss0.690181 vs 0.690539\n",
      "epoch 0, batch2232/3125, loss0.689878 vs 0.687591\n",
      "epoch 0, batch2233/3125, loss0.689521 vs 0.690926\n",
      "epoch 0, batch2234/3125, loss0.689320 vs 0.688762\n",
      "epoch 0, batch2235/3125, loss0.689022 vs 0.690540\n",
      "epoch 0, batch2236/3125, loss0.688583 vs 0.689162\n",
      "epoch 0, batch2237/3125, loss0.688435 vs 0.689876\n",
      "epoch 0, batch2238/3125, loss0.688433 vs 0.689129\n",
      "epoch 0, batch2239/3125, loss0.687851 vs 0.690724\n",
      "epoch 0, batch2240/3125, loss0.687224 vs 0.691752\n",
      "epoch 0, batch2241/3125, loss0.686373 vs 0.692078\n",
      "epoch 0, batch2242/3125, loss0.685106 vs 0.692691\n",
      "epoch 0, batch2243/3125, loss0.683771 vs 0.692841\n",
      "epoch 0, batch2244/3125, loss0.683708 vs 0.697986\n",
      "epoch 0, batch2245/3125, loss0.680524 vs 0.693319\n",
      "epoch 0, batch2246/3125, loss0.679643 vs 0.697124\n",
      "epoch 0, batch2247/3125, loss0.678610 vs 0.696248\n",
      "epoch 0, batch2248/3125, loss0.676596 vs 0.699752\n",
      "epoch 0, batch2249/3125, loss0.676150 vs 0.697686\n",
      "epoch 0, batch2250/3125, loss0.676604 vs 0.696541\n",
      "epoch 0, batch2251/3125, loss0.673263 vs 0.699593\n",
      "epoch 0, batch2252/3125, loss0.673807 vs 0.697402\n",
      "epoch 0, batch2253/3125, loss0.670845 vs 0.702876\n",
      "epoch 0, batch2254/3125, loss0.669129 vs 0.702886\n",
      "epoch 0, batch2255/3125, loss0.666315 vs 0.704996\n",
      "epoch 0, batch2256/3125, loss0.664231 vs 0.704256\n",
      "epoch 0, batch2257/3125, loss0.664531 vs 0.706376\n",
      "epoch 0, batch2258/3125, loss0.662992 vs 0.707812\n",
      "epoch 0, batch2259/3125, loss0.660465 vs 0.708591\n",
      "epoch 0, batch2260/3125, loss0.663337 vs 0.706497\n",
      "epoch 0, batch2261/3125, loss0.663722 vs 0.704973\n",
      "epoch 0, batch2262/3125, loss0.666405 vs 0.705181\n",
      "epoch 0, batch2263/3125, loss0.668065 vs 0.704218\n",
      "epoch 0, batch2264/3125, loss0.670679 vs 0.704989\n",
      "epoch 0, batch2265/3125, loss0.673637 vs 0.704942\n",
      "epoch 0, batch2266/3125, loss0.675575 vs 0.703678\n",
      "epoch 0, batch2267/3125, loss0.677914 vs 0.704691\n",
      "epoch 0, batch2268/3125, loss0.680220 vs 0.701310\n",
      "epoch 0, batch2269/3125, loss0.682721 vs 0.701499\n",
      "epoch 0, batch2270/3125, loss0.684766 vs 0.700110\n",
      "epoch 0, batch2271/3125, loss0.687107 vs 0.700409\n",
      "epoch 0, batch2272/3125, loss0.688909 vs 0.699589\n",
      "epoch 0, batch2273/3125, loss0.690687 vs 0.697837\n",
      "epoch 0, batch2274/3125, loss0.692555 vs 0.698070\n",
      "epoch 0, batch2275/3125, loss0.694547 vs 0.698160\n",
      "epoch 0, batch2276/3125, loss0.695348 vs 0.696651\n",
      "epoch 0, batch2277/3125, loss0.696689 vs 0.696694\n",
      "epoch 0, batch2278/3125, loss0.698059 vs 0.695432\n",
      "epoch 0, batch2279/3125, loss0.698544 vs 0.695302\n",
      "epoch 0, batch2280/3125, loss0.700094 vs 0.695215\n",
      "epoch 0, batch2281/3125, loss0.700868 vs 0.696774\n",
      "epoch 0, batch2282/3125, loss0.701492 vs 0.694968\n",
      "epoch 0, batch2283/3125, loss0.702477 vs 0.694700\n",
      "epoch 0, batch2284/3125, loss0.702797 vs 0.694772\n",
      "epoch 0, batch2285/3125, loss0.703759 vs 0.693458\n",
      "epoch 0, batch2286/3125, loss0.703891 vs 0.693925\n",
      "epoch 0, batch2287/3125, loss0.704063 vs 0.695073\n",
      "epoch 0, batch2288/3125, loss0.704975 vs 0.693319\n",
      "epoch 0, batch2289/3125, loss0.704890 vs 0.693659\n",
      "epoch 0, batch2290/3125, loss0.705455 vs 0.693084\n",
      "epoch 0, batch2291/3125, loss0.705535 vs 0.692869\n",
      "epoch 0, batch2292/3125, loss0.705655 vs 0.692481\n",
      "epoch 0, batch2293/3125, loss0.705878 vs 0.692409\n",
      "epoch 0, batch2294/3125, loss0.705263 vs 0.692464\n",
      "epoch 0, batch2295/3125, loss0.705531 vs 0.692837\n",
      "epoch 0, batch2296/3125, loss0.705008 vs 0.692397\n",
      "epoch 0, batch2297/3125, loss0.705057 vs 0.692033\n",
      "epoch 0, batch2298/3125, loss0.704799 vs 0.691416\n",
      "epoch 0, batch2299/3125, loss0.704070 vs 0.692326\n",
      "epoch 0, batch2300/3125, loss0.703832 vs 0.692754\n",
      "epoch 0, batch2301/3125, loss0.702972 vs 0.692163\n",
      "epoch 0, batch2302/3125, loss0.702873 vs 0.692095\n",
      "epoch 0, batch2303/3125, loss0.702189 vs 0.692837\n",
      "epoch 0, batch2304/3125, loss0.702005 vs 0.692234\n",
      "epoch 0, batch2305/3125, loss0.700932 vs 0.692933\n",
      "epoch 0, batch2306/3125, loss0.700439 vs 0.692723\n",
      "epoch 0, batch2307/3125, loss0.699633 vs 0.692460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch2308/3125, loss0.699244 vs 0.692692\n",
      "epoch 0, batch2309/3125, loss0.697712 vs 0.693018\n",
      "epoch 0, batch2310/3125, loss0.697847 vs 0.693460\n",
      "epoch 0, batch2311/3125, loss0.697145 vs 0.693123\n",
      "epoch 0, batch2312/3125, loss0.696171 vs 0.695256\n",
      "epoch 0, batch2313/3125, loss0.695800 vs 0.693953\n",
      "epoch 0, batch2314/3125, loss0.694835 vs 0.693844\n",
      "epoch 0, batch2315/3125, loss0.694694 vs 0.694981\n",
      "epoch 0, batch2316/3125, loss0.693762 vs 0.695994\n",
      "epoch 0, batch2317/3125, loss0.693130 vs 0.695127\n",
      "epoch 0, batch2318/3125, loss0.692437 vs 0.699352\n",
      "epoch 0, batch2319/3125, loss0.691196 vs 0.695850\n",
      "epoch 0, batch2320/3125, loss0.690461 vs 0.695140\n",
      "epoch 0, batch2321/3125, loss0.689701 vs 0.702652\n",
      "epoch 0, batch2322/3125, loss0.689247 vs 0.699389\n",
      "epoch 0, batch2323/3125, loss0.689064 vs 0.697508\n",
      "epoch 0, batch2324/3125, loss0.688790 vs 0.696901\n",
      "epoch 0, batch2325/3125, loss0.688876 vs 0.696262\n",
      "epoch 0, batch2326/3125, loss0.688877 vs 0.696733\n",
      "epoch 0, batch2327/3125, loss0.688981 vs 0.695946\n",
      "epoch 0, batch2328/3125, loss0.688920 vs 0.695412\n",
      "epoch 0, batch2329/3125, loss0.689007 vs 0.695512\n",
      "epoch 0, batch2330/3125, loss0.689196 vs 0.695407\n",
      "epoch 0, batch2331/3125, loss0.689488 vs 0.696009\n",
      "epoch 0, batch2332/3125, loss0.689538 vs 0.696051\n",
      "epoch 0, batch2333/3125, loss0.689748 vs 0.696408\n",
      "epoch 0, batch2334/3125, loss0.689892 vs 0.695885\n",
      "epoch 0, batch2335/3125, loss0.689976 vs 0.697821\n",
      "epoch 0, batch2336/3125, loss0.690217 vs 0.696181\n",
      "epoch 0, batch2337/3125, loss0.690316 vs 0.696233\n",
      "epoch 0, batch2338/3125, loss0.690565 vs 0.695504\n",
      "epoch 0, batch2339/3125, loss0.690680 vs 0.695672\n",
      "epoch 0, batch2340/3125, loss0.690845 vs 0.695426\n",
      "epoch 0, batch2341/3125, loss0.691090 vs 0.695828\n",
      "epoch 0, batch2342/3125, loss0.691170 vs 0.695164\n",
      "epoch 0, batch2343/3125, loss0.691499 vs 0.696123\n",
      "epoch 0, batch2344/3125, loss0.691625 vs 0.696408\n",
      "epoch 0, batch2345/3125, loss0.691840 vs 0.695529\n",
      "epoch 0, batch2346/3125, loss0.691871 vs 0.696440\n",
      "epoch 0, batch2347/3125, loss0.692044 vs 0.695966\n",
      "epoch 0, batch2348/3125, loss0.692086 vs 0.696189\n",
      "epoch 0, batch2349/3125, loss0.692125 vs 0.695863\n",
      "epoch 0, batch2350/3125, loss0.692248 vs 0.695674\n",
      "epoch 0, batch2351/3125, loss0.692392 vs 0.696185\n",
      "epoch 0, batch2352/3125, loss0.692375 vs 0.696065\n",
      "epoch 0, batch2353/3125, loss0.692392 vs 0.695981\n",
      "epoch 0, batch2354/3125, loss0.692650 vs 0.695872\n",
      "epoch 0, batch2355/3125, loss0.692545 vs 0.696149\n",
      "epoch 0, batch2356/3125, loss0.692513 vs 0.695489\n",
      "epoch 0, batch2357/3125, loss0.692415 vs 0.695066\n",
      "epoch 0, batch2358/3125, loss0.692532 vs 0.695728\n",
      "epoch 0, batch2359/3125, loss0.692478 vs 0.695569\n",
      "epoch 0, batch2360/3125, loss0.692427 vs 0.695596\n",
      "epoch 0, batch2361/3125, loss0.692290 vs 0.696725\n",
      "epoch 0, batch2362/3125, loss0.692343 vs 0.695528\n",
      "epoch 0, batch2363/3125, loss0.692539 vs 0.696188\n",
      "epoch 0, batch2364/3125, loss0.692385 vs 0.695784\n",
      "epoch 0, batch2365/3125, loss0.692359 vs 0.695697\n",
      "epoch 0, batch2366/3125, loss0.692442 vs 0.697246\n",
      "epoch 0, batch2367/3125, loss0.692438 vs 0.696570\n",
      "epoch 0, batch2368/3125, loss0.692395 vs 0.696026\n",
      "epoch 0, batch2369/3125, loss0.692461 vs 0.695764\n",
      "epoch 0, batch2370/3125, loss0.692370 vs 0.696337\n",
      "epoch 0, batch2371/3125, loss0.692456 vs 0.696455\n",
      "epoch 0, batch2372/3125, loss0.692257 vs 0.695953\n",
      "epoch 0, batch2373/3125, loss0.692356 vs 0.696267\n",
      "epoch 0, batch2374/3125, loss0.692204 vs 0.696916\n",
      "epoch 0, batch2375/3125, loss0.692347 vs 0.697190\n",
      "epoch 0, batch2376/3125, loss0.692316 vs 0.695834\n",
      "epoch 0, batch2377/3125, loss0.692209 vs 0.696928\n",
      "epoch 0, batch2378/3125, loss0.692240 vs 0.695244\n",
      "epoch 0, batch2379/3125, loss0.692342 vs 0.695978\n",
      "epoch 0, batch2380/3125, loss0.692288 vs 0.695645\n",
      "epoch 0, batch2381/3125, loss0.692464 vs 0.696816\n",
      "epoch 0, batch2382/3125, loss0.692193 vs 0.695885\n",
      "epoch 0, batch2383/3125, loss0.692263 vs 0.696975\n",
      "epoch 0, batch2384/3125, loss0.692326 vs 0.696642\n",
      "epoch 0, batch2385/3125, loss0.692239 vs 0.698717\n",
      "epoch 0, batch2386/3125, loss0.692296 vs 0.696285\n",
      "epoch 0, batch2387/3125, loss0.692180 vs 0.696683\n",
      "epoch 0, batch2388/3125, loss0.692176 vs 0.696133\n",
      "epoch 0, batch2389/3125, loss0.692128 vs 0.696115\n",
      "epoch 0, batch2390/3125, loss0.692065 vs 0.696677\n",
      "epoch 0, batch2391/3125, loss0.692003 vs 0.696284\n",
      "epoch 0, batch2392/3125, loss0.691860 vs 0.696800\n",
      "epoch 0, batch2393/3125, loss0.691974 vs 0.696482\n",
      "epoch 0, batch2394/3125, loss0.691849 vs 0.697038\n",
      "epoch 0, batch2395/3125, loss0.691734 vs 0.696522\n",
      "epoch 0, batch2396/3125, loss0.691589 vs 0.697352\n",
      "epoch 0, batch2397/3125, loss0.691524 vs 0.697071\n",
      "epoch 0, batch2398/3125, loss0.691422 vs 0.697070\n",
      "epoch 0, batch2399/3125, loss0.691225 vs 0.696406\n",
      "epoch 0, batch2400/3125, loss0.691189 vs 0.696861\n",
      "epoch 0, batch2401/3125, loss0.690977 vs 0.696829\n",
      "epoch 0, batch2402/3125, loss0.690919 vs 0.697241\n",
      "epoch 0, batch2403/3125, loss0.690760 vs 0.697357\n",
      "epoch 0, batch2404/3125, loss0.690661 vs 0.695694\n",
      "epoch 0, batch2405/3125, loss0.690541 vs 0.696551\n",
      "epoch 0, batch2406/3125, loss0.690371 vs 0.696844\n",
      "epoch 0, batch2407/3125, loss0.690235 vs 0.696904\n",
      "epoch 0, batch2408/3125, loss0.690318 vs 0.696461\n",
      "epoch 0, batch2409/3125, loss0.690140 vs 0.695865\n",
      "epoch 0, batch2410/3125, loss0.690042 vs 0.696450\n",
      "epoch 0, batch2411/3125, loss0.689936 vs 0.696585\n",
      "epoch 0, batch2412/3125, loss0.689906 vs 0.696398\n",
      "epoch 0, batch2413/3125, loss0.689957 vs 0.700904\n",
      "epoch 0, batch2414/3125, loss0.689816 vs 0.695961\n",
      "epoch 0, batch2415/3125, loss0.689715 vs 0.696221\n",
      "epoch 0, batch2416/3125, loss0.689708 vs 0.695322\n",
      "epoch 0, batch2417/3125, loss0.689525 vs 0.697012\n",
      "epoch 0, batch2418/3125, loss0.689343 vs 0.696522\n",
      "epoch 0, batch2419/3125, loss0.689230 vs 0.696909\n",
      "epoch 0, batch2420/3125, loss0.689245 vs 0.696545\n",
      "epoch 0, batch2421/3125, loss0.689191 vs 0.696041\n",
      "epoch 0, batch2422/3125, loss0.689013 vs 0.696683\n",
      "epoch 0, batch2423/3125, loss0.688928 vs 0.696739\n",
      "epoch 0, batch2424/3125, loss0.688871 vs 0.696544\n",
      "epoch 0, batch2425/3125, loss0.688896 vs 0.696393\n",
      "epoch 0, batch2426/3125, loss0.688743 vs 0.695354\n",
      "epoch 0, batch2427/3125, loss0.688648 vs 0.696338\n",
      "epoch 0, batch2428/3125, loss0.688662 vs 0.696455\n",
      "epoch 0, batch2429/3125, loss0.688571 vs 0.695432\n",
      "epoch 0, batch2430/3125, loss0.688471 vs 0.695757\n",
      "epoch 0, batch2431/3125, loss0.688446 vs 0.696505\n",
      "epoch 0, batch2432/3125, loss0.688356 vs 0.696046\n",
      "epoch 0, batch2433/3125, loss0.688343 vs 0.697006\n",
      "epoch 0, batch2434/3125, loss0.688313 vs 0.696356\n",
      "epoch 0, batch2435/3125, loss0.688272 vs 0.696736\n",
      "epoch 0, batch2436/3125, loss0.688346 vs 0.696434\n",
      "epoch 0, batch2437/3125, loss0.688445 vs 0.696016\n",
      "epoch 0, batch2438/3125, loss0.688420 vs 0.695758\n",
      "epoch 0, batch2439/3125, loss0.688544 vs 0.695894\n",
      "epoch 0, batch2440/3125, loss0.688598 vs 0.696230\n",
      "epoch 0, batch2441/3125, loss0.688779 vs 0.695519\n",
      "epoch 0, batch2442/3125, loss0.688958 vs 0.694928\n",
      "epoch 0, batch2443/3125, loss0.689036 vs 0.695429\n",
      "epoch 0, batch2444/3125, loss0.689194 vs 0.695771\n",
      "epoch 0, batch2445/3125, loss0.689301 vs 0.695284\n",
      "epoch 0, batch2446/3125, loss0.689606 vs 0.695161\n",
      "epoch 0, batch2447/3125, loss0.689740 vs 0.694778\n",
      "epoch 0, batch2448/3125, loss0.689915 vs 0.694714\n",
      "epoch 0, batch2449/3125, loss0.690182 vs 0.694969\n",
      "epoch 0, batch2450/3125, loss0.690249 vs 0.694598\n",
      "epoch 0, batch2451/3125, loss0.690419 vs 0.694826\n",
      "epoch 0, batch2452/3125, loss0.690594 vs 0.694870\n",
      "epoch 0, batch2453/3125, loss0.690715 vs 0.694252\n",
      "epoch 0, batch2454/3125, loss0.690771 vs 0.694676\n",
      "epoch 0, batch2455/3125, loss0.690864 vs 0.694073\n",
      "epoch 0, batch2456/3125, loss0.691047 vs 0.696001\n",
      "epoch 0, batch2457/3125, loss0.691147 vs 0.694137\n",
      "epoch 0, batch2458/3125, loss0.691146 vs 0.693778\n",
      "epoch 0, batch2459/3125, loss0.691305 vs 0.694629\n",
      "epoch 0, batch2460/3125, loss0.691451 vs 0.694249\n",
      "epoch 0, batch2461/3125, loss0.691592 vs 0.693781\n",
      "epoch 0, batch2462/3125, loss0.691676 vs 0.694131\n",
      "epoch 0, batch2463/3125, loss0.691795 vs 0.693564\n",
      "epoch 0, batch2464/3125, loss0.691876 vs 0.694121\n",
      "epoch 0, batch2465/3125, loss0.692110 vs 0.693362\n",
      "epoch 0, batch2466/3125, loss0.692096 vs 0.694023\n",
      "epoch 0, batch2467/3125, loss0.692127 vs 0.694262\n",
      "epoch 0, batch2468/3125, loss0.692196 vs 0.693591\n",
      "epoch 0, batch2469/3125, loss0.692248 vs 0.693766\n",
      "epoch 0, batch2470/3125, loss0.692254 vs 0.693524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch2471/3125, loss0.692366 vs 0.693482\n",
      "epoch 0, batch2472/3125, loss0.692415 vs 0.693741\n",
      "epoch 0, batch2473/3125, loss0.692418 vs 0.694042\n",
      "epoch 0, batch2474/3125, loss0.692504 vs 0.698219\n",
      "epoch 0, batch2475/3125, loss0.692473 vs 0.693047\n",
      "epoch 0, batch2476/3125, loss0.692502 vs 0.693077\n",
      "epoch 0, batch2477/3125, loss0.692531 vs 0.693145\n",
      "epoch 0, batch2478/3125, loss0.692612 vs 0.694692\n",
      "epoch 0, batch2479/3125, loss0.692606 vs 0.693577\n",
      "epoch 0, batch2480/3125, loss0.692663 vs 0.693421\n",
      "epoch 0, batch2481/3125, loss0.692644 vs 0.693208\n",
      "epoch 0, batch2482/3125, loss0.692623 vs 0.692878\n",
      "epoch 0, batch2483/3125, loss0.692577 vs 0.694200\n",
      "epoch 0, batch2484/3125, loss0.692503 vs 0.694179\n",
      "epoch 0, batch2485/3125, loss0.692550 vs 0.693489\n",
      "epoch 0, batch2486/3125, loss0.692499 vs 0.693329\n",
      "epoch 0, batch2487/3125, loss0.692370 vs 0.692639\n",
      "epoch 0, batch2488/3125, loss0.692405 vs 0.693587\n",
      "epoch 0, batch2489/3125, loss0.692424 vs 0.693756\n",
      "epoch 0, batch2490/3125, loss0.692491 vs 0.692485\n",
      "epoch 0, batch2491/3125, loss0.692398 vs 0.693123\n",
      "epoch 0, batch2492/3125, loss0.692371 vs 0.693551\n",
      "epoch 0, batch2493/3125, loss0.692322 vs 0.692984\n",
      "epoch 0, batch2494/3125, loss0.692291 vs 0.693065\n",
      "epoch 0, batch2495/3125, loss0.692203 vs 0.693107\n",
      "epoch 0, batch2496/3125, loss0.692160 vs 0.693544\n",
      "epoch 0, batch2497/3125, loss0.692064 vs 0.693482\n",
      "epoch 0, batch2498/3125, loss0.692017 vs 0.692869\n",
      "epoch 0, batch2499/3125, loss0.691933 vs 0.692674\n",
      "epoch 0, batch2500/3125, loss0.691888 vs 0.693462\n",
      "epoch 0, batch2501/3125, loss0.691737 vs 0.692263\n",
      "epoch 0, batch2502/3125, loss0.691696 vs 0.692173\n",
      "epoch 0, batch2503/3125, loss0.691527 vs 0.692735\n",
      "epoch 0, batch2504/3125, loss0.691445 vs 0.694602\n",
      "epoch 0, batch2505/3125, loss0.691400 vs 0.692850\n",
      "epoch 0, batch2506/3125, loss0.691351 vs 0.693125\n",
      "epoch 0, batch2507/3125, loss0.691271 vs 0.692875\n",
      "epoch 0, batch2508/3125, loss0.691117 vs 0.692875\n",
      "epoch 0, batch2509/3125, loss0.691038 vs 0.693764\n",
      "epoch 0, batch2510/3125, loss0.690944 vs 0.692205\n",
      "epoch 0, batch2511/3125, loss0.690824 vs 0.692910\n",
      "epoch 0, batch2512/3125, loss0.690791 vs 0.693228\n",
      "epoch 0, batch2513/3125, loss0.690639 vs 0.693041\n",
      "epoch 0, batch2514/3125, loss0.690474 vs 0.694172\n",
      "epoch 0, batch2515/3125, loss0.690484 vs 0.693306\n",
      "epoch 0, batch2516/3125, loss0.690411 vs 0.693431\n",
      "epoch 0, batch2517/3125, loss0.690299 vs 0.693598\n",
      "epoch 0, batch2518/3125, loss0.690135 vs 0.692755\n",
      "epoch 0, batch2519/3125, loss0.690166 vs 0.693239\n",
      "epoch 0, batch2520/3125, loss0.690073 vs 0.692734\n",
      "epoch 0, batch2521/3125, loss0.690077 vs 0.693593\n",
      "epoch 0, batch2522/3125, loss0.690005 vs 0.693192\n",
      "epoch 0, batch2523/3125, loss0.689892 vs 0.693032\n",
      "epoch 0, batch2524/3125, loss0.689833 vs 0.694828\n",
      "epoch 0, batch2525/3125, loss0.689857 vs 0.693202\n",
      "epoch 0, batch2526/3125, loss0.689643 vs 0.691990\n",
      "epoch 0, batch2527/3125, loss0.689648 vs 0.693997\n",
      "epoch 0, batch2528/3125, loss0.689590 vs 0.693515\n",
      "epoch 0, batch2529/3125, loss0.689438 vs 0.694699\n",
      "epoch 0, batch2530/3125, loss0.689288 vs 0.694432\n",
      "epoch 0, batch2531/3125, loss0.689312 vs 0.693425\n",
      "epoch 0, batch2532/3125, loss0.689378 vs 0.693545\n",
      "epoch 0, batch2533/3125, loss0.689205 vs 0.695023\n",
      "epoch 0, batch2534/3125, loss0.689157 vs 0.694358\n",
      "epoch 0, batch2535/3125, loss0.689040 vs 0.696266\n",
      "epoch 0, batch2536/3125, loss0.688905 vs 0.693938\n",
      "epoch 0, batch2537/3125, loss0.688880 vs 0.695345\n",
      "epoch 0, batch2538/3125, loss0.688859 vs 0.695522\n",
      "epoch 0, batch2539/3125, loss0.688845 vs 0.695431\n",
      "epoch 0, batch2540/3125, loss0.688850 vs 0.694684\n",
      "epoch 0, batch2541/3125, loss0.688824 vs 0.694292\n",
      "epoch 0, batch2542/3125, loss0.688924 vs 0.694663\n",
      "epoch 0, batch2543/3125, loss0.688929 vs 0.696740\n",
      "epoch 0, batch2544/3125, loss0.689059 vs 0.695011\n",
      "epoch 0, batch2545/3125, loss0.689259 vs 0.695574\n",
      "epoch 0, batch2546/3125, loss0.689428 vs 0.695632\n",
      "epoch 0, batch2547/3125, loss0.689532 vs 0.695279\n",
      "epoch 0, batch2548/3125, loss0.689665 vs 0.695843\n",
      "epoch 0, batch2549/3125, loss0.689855 vs 0.694535\n",
      "epoch 0, batch2550/3125, loss0.690070 vs 0.695885\n",
      "epoch 0, batch2551/3125, loss0.690300 vs 0.695333\n",
      "epoch 0, batch2552/3125, loss0.690470 vs 0.695308\n",
      "epoch 0, batch2553/3125, loss0.690765 vs 0.697393\n",
      "epoch 0, batch2554/3125, loss0.691020 vs 0.696251\n",
      "epoch 0, batch2555/3125, loss0.691422 vs 0.695984\n",
      "epoch 0, batch2556/3125, loss0.691678 vs 0.695623\n",
      "epoch 0, batch2557/3125, loss0.692002 vs 0.695146\n",
      "epoch 0, batch2558/3125, loss0.692255 vs 0.695844\n",
      "epoch 0, batch2559/3125, loss0.692685 vs 0.695098\n",
      "epoch 0, batch2560/3125, loss0.693152 vs 0.694916\n",
      "epoch 0, batch2561/3125, loss0.693537 vs 0.694135\n",
      "epoch 0, batch2562/3125, loss0.693976 vs 0.694619\n",
      "epoch 0, batch2563/3125, loss0.694273 vs 0.695471\n",
      "epoch 0, batch2564/3125, loss0.694737 vs 0.694949\n",
      "epoch 0, batch2565/3125, loss0.695092 vs 0.695421\n",
      "epoch 0, batch2566/3125, loss0.695537 vs 0.695362\n",
      "epoch 0, batch2567/3125, loss0.695932 vs 0.694455\n",
      "epoch 0, batch2568/3125, loss0.696325 vs 0.694592\n",
      "epoch 0, batch2569/3125, loss0.696662 vs 0.694953\n",
      "epoch 0, batch2570/3125, loss0.696986 vs 0.694529\n",
      "epoch 0, batch2571/3125, loss0.697261 vs 0.694486\n",
      "epoch 0, batch2572/3125, loss0.697600 vs 0.694404\n",
      "epoch 0, batch2573/3125, loss0.697939 vs 0.694046\n",
      "epoch 0, batch2574/3125, loss0.698150 vs 0.694208\n",
      "epoch 0, batch2575/3125, loss0.698485 vs 0.694281\n",
      "epoch 0, batch2576/3125, loss0.698660 vs 0.694205\n",
      "epoch 0, batch2577/3125, loss0.698876 vs 0.694110\n",
      "epoch 0, batch2578/3125, loss0.699046 vs 0.693988\n",
      "epoch 0, batch2579/3125, loss0.699343 vs 0.693080\n",
      "epoch 0, batch2580/3125, loss0.699470 vs 0.694116\n",
      "epoch 0, batch2581/3125, loss0.699668 vs 0.692571\n",
      "epoch 0, batch2582/3125, loss0.699788 vs 0.693635\n",
      "epoch 0, batch2583/3125, loss0.699773 vs 0.693885\n",
      "epoch 0, batch2584/3125, loss0.699965 vs 0.693283\n",
      "epoch 0, batch2585/3125, loss0.699937 vs 0.692897\n",
      "epoch 0, batch2586/3125, loss0.699892 vs 0.693476\n",
      "epoch 0, batch2587/3125, loss0.699825 vs 0.693265\n",
      "epoch 0, batch2588/3125, loss0.699721 vs 0.693766\n",
      "epoch 0, batch2589/3125, loss0.699644 vs 0.693113\n",
      "epoch 0, batch2590/3125, loss0.699225 vs 0.693794\n",
      "epoch 0, batch2591/3125, loss0.698809 vs 0.693696\n",
      "epoch 0, batch2592/3125, loss0.697504 vs 0.694138\n",
      "epoch 0, batch2593/3125, loss0.696422 vs 0.694547\n",
      "epoch 0, batch2594/3125, loss0.695501 vs 0.694007\n",
      "epoch 0, batch2595/3125, loss0.695197 vs 0.694312\n",
      "epoch 0, batch2596/3125, loss0.695603 vs 0.693940\n",
      "epoch 0, batch2597/3125, loss0.697198 vs 0.693153\n",
      "epoch 0, batch2598/3125, loss0.699514 vs 0.691958\n",
      "epoch 0, batch2599/3125, loss0.702996 vs 0.689519\n",
      "epoch 0, batch2600/3125, loss0.707517 vs 0.687503\n",
      "epoch 0, batch2601/3125, loss0.713099 vs 0.686037\n",
      "epoch 0, batch2602/3125, loss0.719458 vs 0.681209\n",
      "epoch 0, batch2603/3125, loss0.725808 vs 0.678032\n",
      "epoch 0, batch2604/3125, loss0.732043 vs 0.674743\n",
      "epoch 0, batch2605/3125, loss0.737266 vs 0.672482\n",
      "epoch 0, batch2606/3125, loss0.739024 vs 0.670862\n",
      "epoch 0, batch2607/3125, loss0.736575 vs 0.672366\n",
      "epoch 0, batch2608/3125, loss0.725656 vs 0.676650\n",
      "epoch 0, batch2609/3125, loss0.707136 vs 0.686102\n",
      "epoch 0, batch2610/3125, loss0.691544 vs 0.691522\n",
      "epoch 0, batch2611/3125, loss0.691779 vs 0.692183\n",
      "epoch 0, batch2612/3125, loss0.692127 vs 0.691443\n",
      "epoch 0, batch2613/3125, loss0.691254 vs 0.691306\n",
      "epoch 0, batch2614/3125, loss0.689480 vs 0.701146\n",
      "epoch 0, batch2615/3125, loss0.688237 vs 0.692607\n",
      "epoch 0, batch2616/3125, loss0.685612 vs 0.699990\n",
      "epoch 0, batch2617/3125, loss0.684304 vs 0.697836\n",
      "epoch 0, batch2618/3125, loss0.683515 vs 0.722348\n",
      "epoch 0, batch2619/3125, loss0.682496 vs 0.698418\n",
      "epoch 0, batch2620/3125, loss0.681974 vs 0.696988\n",
      "epoch 0, batch2621/3125, loss0.681421 vs 0.694723\n",
      "epoch 0, batch2622/3125, loss0.681291 vs 0.705662\n",
      "epoch 0, batch2623/3125, loss0.680897 vs 0.693408\n",
      "epoch 0, batch2624/3125, loss0.680623 vs 0.694162\n",
      "epoch 0, batch2625/3125, loss0.680709 vs 0.697439\n",
      "epoch 0, batch2626/3125, loss0.680335 vs 0.695251\n",
      "epoch 0, batch2627/3125, loss0.680655 vs 0.694424\n",
      "epoch 0, batch2628/3125, loss0.680901 vs 0.694945\n",
      "epoch 0, batch2629/3125, loss0.680824 vs 0.695396\n",
      "epoch 0, batch2630/3125, loss0.680837 vs 0.694199\n",
      "epoch 0, batch2631/3125, loss0.680972 vs 0.695234\n",
      "epoch 0, batch2632/3125, loss0.681283 vs 0.693874\n",
      "epoch 0, batch2633/3125, loss0.681537 vs 0.697248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch2634/3125, loss0.681924 vs 0.701023\n",
      "epoch 0, batch2635/3125, loss0.682045 vs 0.693483\n",
      "epoch 0, batch2636/3125, loss0.682597 vs 0.695980\n",
      "epoch 0, batch2637/3125, loss0.682841 vs 0.701755\n",
      "epoch 0, batch2638/3125, loss0.683189 vs 0.693839\n",
      "epoch 0, batch2639/3125, loss0.683674 vs 0.693567\n",
      "epoch 0, batch2640/3125, loss0.684183 vs 0.693681\n",
      "epoch 0, batch2641/3125, loss0.684617 vs 0.693272\n",
      "epoch 0, batch2642/3125, loss0.685013 vs 0.693158\n",
      "epoch 0, batch2643/3125, loss0.685535 vs 0.693328\n",
      "epoch 0, batch2644/3125, loss0.685745 vs 0.694790\n",
      "epoch 0, batch2645/3125, loss0.686336 vs 0.693247\n",
      "epoch 0, batch2646/3125, loss0.686804 vs 0.692768\n",
      "epoch 0, batch2647/3125, loss0.687088 vs 0.692083\n",
      "epoch 0, batch2648/3125, loss0.687405 vs 0.692578\n",
      "epoch 0, batch2649/3125, loss0.687646 vs 0.691466\n",
      "epoch 0, batch2650/3125, loss0.688160 vs 0.693683\n",
      "epoch 0, batch2651/3125, loss0.688370 vs 0.693688\n",
      "epoch 0, batch2652/3125, loss0.688694 vs 0.693160\n",
      "epoch 0, batch2653/3125, loss0.688803 vs 0.692319\n",
      "epoch 0, batch2654/3125, loss0.689214 vs 0.691589\n",
      "epoch 0, batch2655/3125, loss0.689411 vs 0.691324\n",
      "epoch 0, batch2656/3125, loss0.689430 vs 0.695016\n",
      "epoch 0, batch2657/3125, loss0.689781 vs 0.692706\n",
      "epoch 0, batch2658/3125, loss0.690036 vs 0.693565\n",
      "epoch 0, batch2659/3125, loss0.690203 vs 0.693208\n",
      "epoch 0, batch2660/3125, loss0.690114 vs 0.693611\n",
      "epoch 0, batch2661/3125, loss0.690389 vs 0.694950\n",
      "epoch 0, batch2662/3125, loss0.690364 vs 0.692121\n",
      "epoch 0, batch2663/3125, loss0.690542 vs 0.693225\n",
      "epoch 0, batch2664/3125, loss0.690356 vs 0.693714\n",
      "epoch 0, batch2665/3125, loss0.690609 vs 0.690691\n",
      "epoch 0, batch2666/3125, loss0.690642 vs 0.692098\n",
      "epoch 0, batch2667/3125, loss0.690599 vs 0.690284\n",
      "epoch 0, batch2668/3125, loss0.690675 vs 0.693821\n",
      "epoch 0, batch2669/3125, loss0.690453 vs 0.690150\n",
      "epoch 0, batch2670/3125, loss0.690449 vs 0.690095\n",
      "epoch 0, batch2671/3125, loss0.690459 vs 0.691850\n",
      "epoch 0, batch2672/3125, loss0.690507 vs 0.689546\n",
      "epoch 0, batch2673/3125, loss0.690727 vs 0.689039\n",
      "epoch 0, batch2674/3125, loss0.690570 vs 0.692156\n",
      "epoch 0, batch2675/3125, loss0.690460 vs 0.692267\n",
      "epoch 0, batch2676/3125, loss0.689999 vs 0.691698\n",
      "epoch 0, batch2677/3125, loss0.689999 vs 0.691001\n",
      "epoch 0, batch2678/3125, loss0.690013 vs 0.690385\n",
      "epoch 0, batch2679/3125, loss0.689758 vs 0.692571\n",
      "epoch 0, batch2680/3125, loss0.689577 vs 0.690526\n",
      "epoch 0, batch2681/3125, loss0.689530 vs 0.689522\n",
      "epoch 0, batch2682/3125, loss0.688710 vs 0.690141\n",
      "epoch 0, batch2683/3125, loss0.689035 vs 0.691242\n",
      "epoch 0, batch2684/3125, loss0.688732 vs 0.690976\n",
      "epoch 0, batch2685/3125, loss0.688386 vs 0.691887\n",
      "epoch 0, batch2686/3125, loss0.688314 vs 0.692482\n",
      "epoch 0, batch2687/3125, loss0.687673 vs 0.691920\n",
      "epoch 0, batch2688/3125, loss0.688051 vs 0.693941\n",
      "epoch 0, batch2689/3125, loss0.687051 vs 0.692218\n",
      "epoch 0, batch2690/3125, loss0.687050 vs 0.692905\n",
      "epoch 0, batch2691/3125, loss0.687035 vs 0.694339\n",
      "epoch 0, batch2692/3125, loss0.686480 vs 0.692973\n",
      "epoch 0, batch2693/3125, loss0.686054 vs 0.692486\n",
      "epoch 0, batch2694/3125, loss0.685976 vs 0.690331\n",
      "epoch 0, batch2695/3125, loss0.685512 vs 0.692704\n",
      "epoch 0, batch2696/3125, loss0.684461 vs 0.693447\n",
      "epoch 0, batch2697/3125, loss0.684606 vs 0.695033\n",
      "epoch 0, batch2698/3125, loss0.684232 vs 0.695674\n",
      "epoch 0, batch2699/3125, loss0.684345 vs 0.698193\n",
      "epoch 0, batch2700/3125, loss0.683700 vs 0.694062\n",
      "epoch 0, batch2701/3125, loss0.683946 vs 0.696038\n",
      "epoch 0, batch2702/3125, loss0.683623 vs 0.691666\n",
      "epoch 0, batch2703/3125, loss0.682882 vs 0.695742\n",
      "epoch 0, batch2704/3125, loss0.682654 vs 0.695786\n",
      "epoch 0, batch2705/3125, loss0.682119 vs 0.694966\n",
      "epoch 0, batch2706/3125, loss0.682969 vs 0.695524\n",
      "epoch 0, batch2707/3125, loss0.682330 vs 0.698259\n",
      "epoch 0, batch2708/3125, loss0.682794 vs 0.697669\n",
      "epoch 0, batch2709/3125, loss0.682899 vs 0.698785\n",
      "epoch 0, batch2710/3125, loss0.683298 vs 0.699699\n",
      "epoch 0, batch2711/3125, loss0.683197 vs 0.699580\n",
      "epoch 0, batch2712/3125, loss0.683703 vs 0.696974\n",
      "epoch 0, batch2713/3125, loss0.684611 vs 0.698500\n",
      "epoch 0, batch2714/3125, loss0.685208 vs 0.697330\n",
      "epoch 0, batch2715/3125, loss0.685718 vs 0.697342\n",
      "epoch 0, batch2716/3125, loss0.686278 vs 0.696272\n",
      "epoch 0, batch2717/3125, loss0.687369 vs 0.700628\n",
      "epoch 0, batch2718/3125, loss0.688252 vs 0.699018\n",
      "epoch 0, batch2719/3125, loss0.688978 vs 0.698115\n",
      "epoch 0, batch2720/3125, loss0.689949 vs 0.699869\n",
      "epoch 0, batch2721/3125, loss0.690907 vs 0.699168\n",
      "epoch 0, batch2722/3125, loss0.691844 vs 0.697287\n",
      "epoch 0, batch2723/3125, loss0.693015 vs 0.696099\n",
      "epoch 0, batch2724/3125, loss0.693564 vs 0.697456\n",
      "epoch 0, batch2725/3125, loss0.694640 vs 0.698330\n",
      "epoch 0, batch2726/3125, loss0.695412 vs 0.695868\n",
      "epoch 0, batch2727/3125, loss0.696333 vs 0.697493\n",
      "epoch 0, batch2728/3125, loss0.697001 vs 0.699536\n",
      "epoch 0, batch2729/3125, loss0.697877 vs 0.696487\n",
      "epoch 0, batch2730/3125, loss0.698786 vs 0.697511\n",
      "epoch 0, batch2731/3125, loss0.699526 vs 0.697108\n",
      "epoch 0, batch2732/3125, loss0.700168 vs 0.695530\n",
      "epoch 0, batch2733/3125, loss0.700844 vs 0.696378\n",
      "epoch 0, batch2734/3125, loss0.701374 vs 0.695631\n",
      "epoch 0, batch2735/3125, loss0.701962 vs 0.695169\n",
      "epoch 0, batch2736/3125, loss0.702395 vs 0.696363\n",
      "epoch 0, batch2737/3125, loss0.703038 vs 0.695821\n",
      "epoch 0, batch2738/3125, loss0.703445 vs 0.694755\n",
      "epoch 0, batch2739/3125, loss0.703818 vs 0.695138\n",
      "epoch 0, batch2740/3125, loss0.704213 vs 0.696218\n",
      "epoch 0, batch2741/3125, loss0.704548 vs 0.695495\n",
      "epoch 0, batch2742/3125, loss0.704889 vs 0.694787\n",
      "epoch 0, batch2743/3125, loss0.705176 vs 0.695130\n",
      "epoch 0, batch2744/3125, loss0.705509 vs 0.693592\n",
      "epoch 0, batch2745/3125, loss0.705691 vs 0.694988\n",
      "epoch 0, batch2746/3125, loss0.705881 vs 0.693571\n",
      "epoch 0, batch2747/3125, loss0.706067 vs 0.693545\n",
      "epoch 0, batch2748/3125, loss0.706204 vs 0.693157\n",
      "epoch 0, batch2749/3125, loss0.706395 vs 0.693295\n",
      "epoch 0, batch2750/3125, loss0.706509 vs 0.693745\n",
      "epoch 0, batch2751/3125, loss0.706670 vs 0.692998\n",
      "epoch 0, batch2752/3125, loss0.706718 vs 0.692820\n",
      "epoch 0, batch2753/3125, loss0.706764 vs 0.694037\n",
      "epoch 0, batch2754/3125, loss0.706848 vs 0.692395\n",
      "epoch 0, batch2755/3125, loss0.706887 vs 0.692141\n",
      "epoch 0, batch2756/3125, loss0.706946 vs 0.692329\n",
      "epoch 0, batch2757/3125, loss0.707010 vs 0.691977\n",
      "epoch 0, batch2758/3125, loss0.707026 vs 0.691920\n",
      "epoch 0, batch2759/3125, loss0.707029 vs 0.691612\n",
      "epoch 0, batch2760/3125, loss0.707012 vs 0.691714\n",
      "epoch 0, batch2761/3125, loss0.707078 vs 0.691436\n",
      "epoch 0, batch2762/3125, loss0.707034 vs 0.691309\n",
      "epoch 0, batch2763/3125, loss0.707017 vs 0.691109\n",
      "epoch 0, batch2764/3125, loss0.707006 vs 0.690988\n",
      "epoch 0, batch2765/3125, loss0.706989 vs 0.690709\n",
      "epoch 0, batch2766/3125, loss0.706923 vs 0.691344\n",
      "epoch 0, batch2767/3125, loss0.706950 vs 0.690543\n",
      "epoch 0, batch2768/3125, loss0.706913 vs 0.690179\n",
      "epoch 0, batch2769/3125, loss0.706966 vs 0.690231\n",
      "epoch 0, batch2770/3125, loss0.706923 vs 0.689417\n",
      "epoch 0, batch2771/3125, loss0.706910 vs 0.689283\n",
      "epoch 0, batch2772/3125, loss0.707039 vs 0.688943\n",
      "epoch 0, batch2773/3125, loss0.707148 vs 0.688857\n",
      "epoch 0, batch2774/3125, loss0.707242 vs 0.689365\n",
      "epoch 0, batch2775/3125, loss0.707317 vs 0.688546\n",
      "epoch 0, batch2776/3125, loss0.707480 vs 0.689481\n",
      "epoch 0, batch2777/3125, loss0.707677 vs 0.688540\n",
      "epoch 0, batch2778/3125, loss0.707771 vs 0.688543\n",
      "epoch 0, batch2779/3125, loss0.707929 vs 0.688299\n",
      "epoch 0, batch2780/3125, loss0.707992 vs 0.688282\n",
      "epoch 0, batch2781/3125, loss0.708228 vs 0.687459\n",
      "epoch 0, batch2782/3125, loss0.708366 vs 0.687468\n",
      "epoch 0, batch2783/3125, loss0.708506 vs 0.688715\n",
      "epoch 0, batch2784/3125, loss0.708759 vs 0.687377\n",
      "epoch 0, batch2785/3125, loss0.708953 vs 0.686881\n",
      "epoch 0, batch2786/3125, loss0.709169 vs 0.687563\n",
      "epoch 0, batch2787/3125, loss0.709379 vs 0.686799\n",
      "epoch 0, batch2788/3125, loss0.709625 vs 0.687785\n",
      "epoch 0, batch2789/3125, loss0.709832 vs 0.686715\n",
      "epoch 0, batch2790/3125, loss0.710055 vs 0.686992\n",
      "epoch 0, batch2791/3125, loss0.710309 vs 0.685684\n",
      "epoch 0, batch2792/3125, loss0.710498 vs 0.685622\n",
      "epoch 0, batch2793/3125, loss0.710799 vs 0.685613\n",
      "epoch 0, batch2794/3125, loss0.711044 vs 0.685787\n",
      "epoch 0, batch2795/3125, loss0.711246 vs 0.686792\n",
      "epoch 0, batch2796/3125, loss0.711275 vs 0.685406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch2797/3125, loss0.711499 vs 0.687614\n",
      "epoch 0, batch2798/3125, loss0.711470 vs 0.686093\n",
      "epoch 0, batch2799/3125, loss0.711464 vs 0.686079\n",
      "epoch 0, batch2800/3125, loss0.711442 vs 0.686377\n",
      "epoch 0, batch2801/3125, loss0.711499 vs 0.684799\n",
      "epoch 0, batch2802/3125, loss0.711375 vs 0.688957\n",
      "epoch 0, batch2803/3125, loss0.710979 vs 0.686188\n",
      "epoch 0, batch2804/3125, loss0.710614 vs 0.687604\n",
      "epoch 0, batch2805/3125, loss0.710079 vs 0.686400\n",
      "epoch 0, batch2806/3125, loss0.709813 vs 0.686340\n",
      "epoch 0, batch2807/3125, loss0.709364 vs 0.689810\n",
      "epoch 0, batch2808/3125, loss0.708752 vs 0.683850\n",
      "epoch 0, batch2809/3125, loss0.708340 vs 0.688935\n",
      "epoch 0, batch2810/3125, loss0.707619 vs 0.688053\n",
      "epoch 0, batch2811/3125, loss0.706883 vs 0.689843\n",
      "epoch 0, batch2812/3125, loss0.706676 vs 0.687411\n",
      "epoch 0, batch2813/3125, loss0.706017 vs 0.689182\n",
      "epoch 0, batch2814/3125, loss0.706436 vs 0.687455\n",
      "epoch 0, batch2815/3125, loss0.705642 vs 0.688503\n",
      "epoch 0, batch2816/3125, loss0.704919 vs 0.690468\n",
      "epoch 0, batch2817/3125, loss0.703498 vs 0.692089\n",
      "epoch 0, batch2818/3125, loss0.703315 vs 0.689838\n",
      "epoch 0, batch2819/3125, loss0.702972 vs 0.690518\n",
      "epoch 0, batch2820/3125, loss0.701742 vs 0.689887\n",
      "epoch 0, batch2821/3125, loss0.701094 vs 0.688883\n",
      "epoch 0, batch2822/3125, loss0.700493 vs 0.686912\n",
      "epoch 0, batch2823/3125, loss0.699648 vs 0.690046\n",
      "epoch 0, batch2824/3125, loss0.698997 vs 0.690279\n",
      "epoch 0, batch2825/3125, loss0.697965 vs 0.693204\n",
      "epoch 0, batch2826/3125, loss0.697260 vs 0.690336\n",
      "epoch 0, batch2827/3125, loss0.696668 vs 0.687741\n",
      "epoch 0, batch2828/3125, loss0.695853 vs 0.691827\n",
      "epoch 0, batch2829/3125, loss0.694994 vs 0.691945\n",
      "epoch 0, batch2830/3125, loss0.693819 vs 0.693573\n",
      "epoch 0, batch2831/3125, loss0.693968 vs 0.691772\n",
      "epoch 0, batch2832/3125, loss0.692832 vs 0.691600\n",
      "epoch 0, batch2833/3125, loss0.691748 vs 0.691832\n",
      "epoch 0, batch2834/3125, loss0.691237 vs 0.690875\n",
      "epoch 0, batch2835/3125, loss0.690992 vs 0.694128\n",
      "epoch 0, batch2836/3125, loss0.690492 vs 0.687616\n",
      "epoch 0, batch2837/3125, loss0.689657 vs 0.691419\n",
      "epoch 0, batch2838/3125, loss0.688747 vs 0.689975\n",
      "epoch 0, batch2839/3125, loss0.688199 vs 0.688432\n",
      "epoch 0, batch2840/3125, loss0.688246 vs 0.692510\n",
      "epoch 0, batch2841/3125, loss0.687659 vs 0.692234\n",
      "epoch 0, batch2842/3125, loss0.687423 vs 0.695159\n",
      "epoch 0, batch2843/3125, loss0.686901 vs 0.693015\n",
      "epoch 0, batch2844/3125, loss0.687626 vs 0.693291\n",
      "epoch 0, batch2845/3125, loss0.686841 vs 0.693037\n",
      "epoch 0, batch2846/3125, loss0.686202 vs 0.694365\n",
      "epoch 0, batch2847/3125, loss0.686561 vs 0.693851\n",
      "epoch 0, batch2848/3125, loss0.686485 vs 0.695420\n",
      "epoch 0, batch2849/3125, loss0.686472 vs 0.690951\n",
      "epoch 0, batch2850/3125, loss0.685831 vs 0.691470\n",
      "epoch 0, batch2851/3125, loss0.685529 vs 0.693012\n",
      "epoch 0, batch2852/3125, loss0.685639 vs 0.694711\n",
      "epoch 0, batch2853/3125, loss0.685588 vs 0.694251\n",
      "epoch 0, batch2854/3125, loss0.684891 vs 0.693805\n",
      "epoch 0, batch2855/3125, loss0.684356 vs 0.692639\n",
      "epoch 0, batch2856/3125, loss0.684761 vs 0.693615\n",
      "epoch 0, batch2857/3125, loss0.683955 vs 0.691604\n",
      "epoch 0, batch2858/3125, loss0.683660 vs 0.695912\n",
      "epoch 0, batch2859/3125, loss0.683612 vs 0.695074\n",
      "epoch 0, batch2860/3125, loss0.682823 vs 0.693767\n",
      "epoch 0, batch2861/3125, loss0.682156 vs 0.696080\n",
      "epoch 0, batch2862/3125, loss0.681243 vs 0.696221\n",
      "epoch 0, batch2863/3125, loss0.680211 vs 0.695999\n",
      "epoch 0, batch2864/3125, loss0.679749 vs 0.698712\n",
      "epoch 0, batch2865/3125, loss0.679156 vs 0.696526\n",
      "epoch 0, batch2866/3125, loss0.678757 vs 0.698111\n",
      "epoch 0, batch2867/3125, loss0.678135 vs 0.697497\n",
      "epoch 0, batch2868/3125, loss0.678054 vs 0.696754\n",
      "epoch 0, batch2869/3125, loss0.678033 vs 0.697330\n",
      "epoch 0, batch2870/3125, loss0.677810 vs 0.696278\n",
      "epoch 0, batch2871/3125, loss0.677653 vs 0.696941\n",
      "epoch 0, batch2872/3125, loss0.677680 vs 0.696479\n",
      "epoch 0, batch2873/3125, loss0.678076 vs 0.696767\n",
      "epoch 0, batch2874/3125, loss0.678201 vs 0.698744\n",
      "epoch 0, batch2875/3125, loss0.678739 vs 0.695621\n",
      "epoch 0, batch2876/3125, loss0.678705 vs 0.695635\n",
      "epoch 0, batch2877/3125, loss0.679360 vs 0.696675\n",
      "epoch 0, batch2878/3125, loss0.679827 vs 0.695799\n",
      "epoch 0, batch2879/3125, loss0.680339 vs 0.694725\n",
      "epoch 0, batch2880/3125, loss0.680840 vs 0.699065\n",
      "epoch 0, batch2881/3125, loss0.681571 vs 0.695231\n",
      "epoch 0, batch2882/3125, loss0.681880 vs 0.695101\n",
      "epoch 0, batch2883/3125, loss0.682412 vs 0.695295\n",
      "epoch 0, batch2884/3125, loss0.683135 vs 0.695556\n",
      "epoch 0, batch2885/3125, loss0.683946 vs 0.695025\n",
      "epoch 0, batch2886/3125, loss0.684470 vs 0.694997\n",
      "epoch 0, batch2887/3125, loss0.685189 vs 0.694548\n",
      "epoch 0, batch2888/3125, loss0.685913 vs 0.693918\n",
      "epoch 0, batch2889/3125, loss0.686746 vs 0.692855\n",
      "epoch 0, batch2890/3125, loss0.687412 vs 0.694203\n",
      "epoch 0, batch2891/3125, loss0.687937 vs 0.692069\n",
      "epoch 0, batch2892/3125, loss0.688591 vs 0.692988\n",
      "epoch 0, batch2893/3125, loss0.689419 vs 0.695403\n",
      "epoch 0, batch2894/3125, loss0.689883 vs 0.690207\n",
      "epoch 0, batch2895/3125, loss0.690614 vs 0.690065\n",
      "epoch 0, batch2896/3125, loss0.691040 vs 0.691325\n",
      "epoch 0, batch2897/3125, loss0.691546 vs 0.694597\n",
      "epoch 0, batch2898/3125, loss0.692310 vs 0.689509\n",
      "epoch 0, batch2899/3125, loss0.692758 vs 0.690269\n",
      "epoch 0, batch2900/3125, loss0.693449 vs 0.690531\n",
      "epoch 0, batch2901/3125, loss0.693518 vs 0.690093\n",
      "epoch 0, batch2902/3125, loss0.694077 vs 0.688799\n",
      "epoch 0, batch2903/3125, loss0.694444 vs 0.688412\n",
      "epoch 0, batch2904/3125, loss0.694903 vs 0.691960\n",
      "epoch 0, batch2905/3125, loss0.695013 vs 0.689852\n",
      "epoch 0, batch2906/3125, loss0.695220 vs 0.689541\n",
      "epoch 0, batch2907/3125, loss0.695416 vs 0.689042\n",
      "epoch 0, batch2908/3125, loss0.695441 vs 0.688742\n",
      "epoch 0, batch2909/3125, loss0.695364 vs 0.686155\n",
      "epoch 0, batch2910/3125, loss0.695475 vs 0.686373\n",
      "epoch 0, batch2911/3125, loss0.695245 vs 0.686494\n",
      "epoch 0, batch2912/3125, loss0.695151 vs 0.686517\n",
      "epoch 0, batch2913/3125, loss0.694716 vs 0.688781\n",
      "epoch 0, batch2914/3125, loss0.694593 vs 0.687112\n",
      "epoch 0, batch2915/3125, loss0.694189 vs 0.686889\n",
      "epoch 0, batch2916/3125, loss0.693617 vs 0.688517\n",
      "epoch 0, batch2917/3125, loss0.693367 vs 0.688852\n",
      "epoch 0, batch2918/3125, loss0.693078 vs 0.686748\n",
      "epoch 0, batch2919/3125, loss0.692804 vs 0.686778\n",
      "epoch 0, batch2920/3125, loss0.692770 vs 0.689573\n",
      "epoch 0, batch2921/3125, loss0.692138 vs 0.686533\n",
      "epoch 0, batch2922/3125, loss0.692106 vs 0.687519\n",
      "epoch 0, batch2923/3125, loss0.691771 vs 0.687319\n",
      "epoch 0, batch2924/3125, loss0.691761 vs 0.688758\n",
      "epoch 0, batch2925/3125, loss0.691509 vs 0.685224\n",
      "epoch 0, batch2926/3125, loss0.691229 vs 0.686426\n",
      "epoch 0, batch2927/3125, loss0.691284 vs 0.685235\n",
      "epoch 0, batch2928/3125, loss0.690802 vs 0.690552\n",
      "epoch 0, batch2929/3125, loss0.690788 vs 0.688782\n",
      "epoch 0, batch2930/3125, loss0.690497 vs 0.686177\n",
      "epoch 0, batch2931/3125, loss0.690144 vs 0.687013\n",
      "epoch 0, batch2932/3125, loss0.690005 vs 0.684591\n",
      "epoch 0, batch2933/3125, loss0.689580 vs 0.690041\n",
      "epoch 0, batch2934/3125, loss0.689110 vs 0.691194\n",
      "epoch 0, batch2935/3125, loss0.688779 vs 0.684853\n",
      "epoch 0, batch2936/3125, loss0.688342 vs 0.689161\n",
      "epoch 0, batch2937/3125, loss0.688059 vs 0.686644\n",
      "epoch 0, batch2938/3125, loss0.687132 vs 0.684856\n",
      "epoch 0, batch2939/3125, loss0.686380 vs 0.687840\n",
      "epoch 0, batch2940/3125, loss0.685653 vs 0.689211\n",
      "epoch 0, batch2941/3125, loss0.684915 vs 0.691524\n",
      "epoch 0, batch2942/3125, loss0.683949 vs 0.691490\n",
      "epoch 0, batch2943/3125, loss0.683289 vs 0.687934\n",
      "epoch 0, batch2944/3125, loss0.681779 vs 0.686736\n",
      "epoch 0, batch2945/3125, loss0.680401 vs 0.689469\n",
      "epoch 0, batch2946/3125, loss0.679035 vs 0.691452\n",
      "epoch 0, batch2947/3125, loss0.677431 vs 0.691323\n",
      "epoch 0, batch2948/3125, loss0.676276 vs 0.691783\n",
      "epoch 0, batch2949/3125, loss0.674427 vs 0.696765\n",
      "epoch 0, batch2950/3125, loss0.673210 vs 0.702464\n",
      "epoch 0, batch2951/3125, loss0.672512 vs 0.697955\n",
      "epoch 0, batch2952/3125, loss0.671856 vs 0.697427\n",
      "epoch 0, batch2953/3125, loss0.672534 vs 0.695868\n",
      "epoch 0, batch2954/3125, loss0.673817 vs 0.698499\n",
      "epoch 0, batch2955/3125, loss0.674434 vs 0.697663\n",
      "epoch 0, batch2956/3125, loss0.676630 vs 0.702935\n",
      "epoch 0, batch2957/3125, loss0.678364 vs 0.695418\n",
      "epoch 0, batch2958/3125, loss0.680205 vs 0.700131\n",
      "epoch 0, batch2959/3125, loss0.682906 vs 0.701112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch2960/3125, loss0.688240 vs 0.697645\n",
      "epoch 0, batch2961/3125, loss0.695657 vs 0.700063\n",
      "epoch 0, batch2962/3125, loss0.702104 vs 0.691762\n",
      "epoch 0, batch2963/3125, loss0.706872 vs 0.698296\n",
      "epoch 0, batch2964/3125, loss0.715451 vs 0.694947\n",
      "epoch 0, batch2965/3125, loss0.721937 vs 0.689383\n",
      "epoch 0, batch2966/3125, loss0.724801 vs 0.695927\n",
      "epoch 0, batch2967/3125, loss0.724783 vs 0.706397\n",
      "epoch 0, batch2968/3125, loss0.724559 vs 0.695505\n",
      "epoch 0, batch2969/3125, loss0.722271 vs 0.701122\n",
      "epoch 0, batch2970/3125, loss0.720421 vs 0.708512\n",
      "epoch 0, batch2971/3125, loss0.718703 vs 0.701395\n",
      "epoch 0, batch2972/3125, loss0.717641 vs 0.702318\n",
      "epoch 0, batch2973/3125, loss0.715747 vs 0.721539\n",
      "epoch 0, batch2974/3125, loss0.715152 vs 0.715133\n",
      "epoch 0, batch2975/3125, loss0.716236 vs 0.713421\n",
      "epoch 0, batch2976/3125, loss0.718520 vs 0.702444\n",
      "epoch 0, batch2977/3125, loss0.722896 vs 0.711898\n",
      "epoch 0, batch2978/3125, loss0.723846 vs 0.708758\n",
      "epoch 0, batch2979/3125, loss0.725074 vs 0.705554\n",
      "epoch 0, batch2980/3125, loss0.724627 vs 0.695080\n",
      "epoch 0, batch2981/3125, loss0.720925 vs 0.693066\n",
      "epoch 0, batch2982/3125, loss0.715805 vs 0.704977\n",
      "epoch 0, batch2983/3125, loss0.705657 vs 0.704173\n",
      "epoch 0, batch2984/3125, loss0.699416 vs 0.713151\n",
      "epoch 0, batch2985/3125, loss0.693605 vs 0.706983\n",
      "epoch 0, batch2986/3125, loss0.690782 vs 0.709542\n",
      "epoch 0, batch2987/3125, loss0.686778 vs 0.707253\n",
      "epoch 0, batch2988/3125, loss0.684757 vs 0.700629\n",
      "epoch 0, batch2989/3125, loss0.681698 vs 0.703132\n",
      "epoch 0, batch2990/3125, loss0.680066 vs 0.700545\n",
      "epoch 0, batch2991/3125, loss0.678074 vs 0.704876\n",
      "epoch 0, batch2992/3125, loss0.676395 vs 0.704640\n",
      "epoch 0, batch2993/3125, loss0.675299 vs 0.701788\n",
      "epoch 0, batch2994/3125, loss0.675251 vs 0.697499\n",
      "epoch 0, batch2995/3125, loss0.674843 vs 0.699905\n",
      "epoch 0, batch2996/3125, loss0.674989 vs 0.704887\n",
      "epoch 0, batch2997/3125, loss0.674621 vs 0.697494\n",
      "epoch 0, batch2998/3125, loss0.674402 vs 0.693871\n",
      "epoch 0, batch2999/3125, loss0.674151 vs 0.695372\n",
      "epoch 0, batch3000/3125, loss0.674013 vs 0.696793\n",
      "epoch 0, batch3001/3125, loss0.674613 vs 0.697683\n",
      "epoch 0, batch3002/3125, loss0.675101 vs 0.700434\n",
      "epoch 0, batch3003/3125, loss0.675734 vs 0.694418\n",
      "epoch 0, batch3004/3125, loss0.676594 vs 0.699865\n",
      "epoch 0, batch3005/3125, loss0.678289 vs 0.694927\n",
      "epoch 0, batch3006/3125, loss0.679529 vs 0.695234\n",
      "epoch 0, batch3007/3125, loss0.681023 vs 0.696259\n",
      "epoch 0, batch3008/3125, loss0.681843 vs 0.695672\n",
      "epoch 0, batch3009/3125, loss0.683003 vs 0.692996\n",
      "epoch 0, batch3010/3125, loss0.684308 vs 0.694843\n",
      "epoch 0, batch3011/3125, loss0.685383 vs 0.693425\n",
      "epoch 0, batch3012/3125, loss0.686432 vs 0.693381\n",
      "epoch 0, batch3013/3125, loss0.687667 vs 0.692967\n",
      "epoch 0, batch3014/3125, loss0.688681 vs 0.694964\n",
      "epoch 0, batch3015/3125, loss0.689879 vs 0.691946\n",
      "epoch 0, batch3016/3125, loss0.690624 vs 0.693439\n",
      "epoch 0, batch3017/3125, loss0.691570 vs 0.694492\n",
      "epoch 0, batch3018/3125, loss0.692507 vs 0.692011\n",
      "epoch 0, batch3019/3125, loss0.693414 vs 0.693122\n",
      "epoch 0, batch3020/3125, loss0.694055 vs 0.691323\n",
      "epoch 0, batch3021/3125, loss0.694717 vs 0.692101\n",
      "epoch 0, batch3022/3125, loss0.695450 vs 0.692166\n",
      "epoch 0, batch3023/3125, loss0.695962 vs 0.692169\n",
      "epoch 0, batch3024/3125, loss0.696561 vs 0.693618\n",
      "epoch 0, batch3025/3125, loss0.697194 vs 0.691299\n",
      "epoch 0, batch3026/3125, loss0.697662 vs 0.691776\n",
      "epoch 0, batch3027/3125, loss0.698159 vs 0.691812\n",
      "epoch 0, batch3028/3125, loss0.698539 vs 0.692005\n",
      "epoch 0, batch3029/3125, loss0.698902 vs 0.694535\n",
      "epoch 0, batch3030/3125, loss0.699124 vs 0.691251\n",
      "epoch 0, batch3031/3125, loss0.699429 vs 0.693176\n",
      "epoch 0, batch3032/3125, loss0.699712 vs 0.692525\n",
      "epoch 0, batch3033/3125, loss0.699976 vs 0.692699\n",
      "epoch 0, batch3034/3125, loss0.700085 vs 0.691881\n",
      "epoch 0, batch3035/3125, loss0.700169 vs 0.693735\n",
      "epoch 0, batch3036/3125, loss0.700329 vs 0.692604\n",
      "epoch 0, batch3037/3125, loss0.700227 vs 0.692284\n",
      "epoch 0, batch3038/3125, loss0.700254 vs 0.692518\n",
      "epoch 0, batch3039/3125, loss0.700327 vs 0.691980\n",
      "epoch 0, batch3040/3125, loss0.700241 vs 0.691759\n",
      "epoch 0, batch3041/3125, loss0.700388 vs 0.692242\n",
      "epoch 0, batch3042/3125, loss0.699901 vs 0.691657\n",
      "epoch 0, batch3043/3125, loss0.699979 vs 0.691451\n",
      "epoch 0, batch3044/3125, loss0.699874 vs 0.692041\n",
      "epoch 0, batch3045/3125, loss0.699831 vs 0.692466\n",
      "epoch 0, batch3046/3125, loss0.699564 vs 0.691752\n",
      "epoch 0, batch3047/3125, loss0.699692 vs 0.692015\n",
      "epoch 0, batch3048/3125, loss0.699510 vs 0.691758\n",
      "epoch 0, batch3049/3125, loss0.699331 vs 0.691604\n",
      "epoch 0, batch3050/3125, loss0.699224 vs 0.691974\n",
      "epoch 0, batch3051/3125, loss0.698927 vs 0.691776\n",
      "epoch 0, batch3052/3125, loss0.698853 vs 0.692206\n",
      "epoch 0, batch3053/3125, loss0.698739 vs 0.693899\n",
      "epoch 0, batch3054/3125, loss0.698625 vs 0.691379\n",
      "epoch 0, batch3055/3125, loss0.698457 vs 0.691868\n",
      "epoch 0, batch3056/3125, loss0.698361 vs 0.692977\n",
      "epoch 0, batch3057/3125, loss0.698375 vs 0.692623\n",
      "epoch 0, batch3058/3125, loss0.698066 vs 0.692197\n",
      "epoch 0, batch3059/3125, loss0.697976 vs 0.692148\n",
      "epoch 0, batch3060/3125, loss0.697789 vs 0.692428\n",
      "epoch 0, batch3061/3125, loss0.697806 vs 0.692402\n",
      "epoch 0, batch3062/3125, loss0.697560 vs 0.691582\n",
      "epoch 0, batch3063/3125, loss0.697413 vs 0.691554\n",
      "epoch 0, batch3064/3125, loss0.697383 vs 0.693491\n",
      "epoch 0, batch3065/3125, loss0.697240 vs 0.693747\n",
      "epoch 0, batch3066/3125, loss0.697154 vs 0.692554\n",
      "epoch 0, batch3067/3125, loss0.696977 vs 0.692500\n",
      "epoch 0, batch3068/3125, loss0.696792 vs 0.691584\n",
      "epoch 0, batch3069/3125, loss0.696733 vs 0.693380\n",
      "epoch 0, batch3070/3125, loss0.696608 vs 0.693853\n",
      "epoch 0, batch3071/3125, loss0.696485 vs 0.692581\n",
      "epoch 0, batch3072/3125, loss0.696166 vs 0.691727\n",
      "epoch 0, batch3073/3125, loss0.696232 vs 0.693191\n",
      "epoch 0, batch3074/3125, loss0.696136 vs 0.693741\n",
      "epoch 0, batch3075/3125, loss0.696197 vs 0.691984\n",
      "epoch 0, batch3076/3125, loss0.695844 vs 0.692552\n",
      "epoch 0, batch3077/3125, loss0.695804 vs 0.692741\n",
      "epoch 0, batch3078/3125, loss0.695665 vs 0.693379\n",
      "epoch 0, batch3079/3125, loss0.695542 vs 0.692254\n",
      "epoch 0, batch3080/3125, loss0.695317 vs 0.693411\n",
      "epoch 0, batch3081/3125, loss0.695282 vs 0.692077\n",
      "epoch 0, batch3082/3125, loss0.695199 vs 0.693429\n",
      "epoch 0, batch3083/3125, loss0.695213 vs 0.693606\n",
      "epoch 0, batch3084/3125, loss0.695072 vs 0.693902\n",
      "epoch 0, batch3085/3125, loss0.694903 vs 0.692311\n",
      "epoch 0, batch3086/3125, loss0.694876 vs 0.692364\n",
      "epoch 0, batch3087/3125, loss0.694749 vs 0.693995\n",
      "epoch 0, batch3088/3125, loss0.694568 vs 0.693092\n",
      "epoch 0, batch3089/3125, loss0.694395 vs 0.693290\n",
      "epoch 0, batch3090/3125, loss0.694179 vs 0.693244\n",
      "epoch 0, batch3091/3125, loss0.693987 vs 0.693259\n",
      "epoch 0, batch3092/3125, loss0.693903 vs 0.695829\n",
      "epoch 0, batch3093/3125, loss0.693685 vs 0.692839\n",
      "epoch 0, batch3094/3125, loss0.693330 vs 0.694893\n",
      "epoch 0, batch3095/3125, loss0.693092 vs 0.692913\n",
      "epoch 0, batch3096/3125, loss0.692771 vs 0.694951\n",
      "epoch 0, batch3097/3125, loss0.692026 vs 0.695653\n",
      "epoch 0, batch3098/3125, loss0.691819 vs 0.694308\n",
      "epoch 0, batch3099/3125, loss0.691007 vs 0.695394\n",
      "epoch 0, batch3100/3125, loss0.690744 vs 0.695653\n",
      "epoch 0, batch3101/3125, loss0.690377 vs 0.694303\n",
      "epoch 0, batch3102/3125, loss0.689916 vs 0.694857\n",
      "epoch 0, batch3103/3125, loss0.689485 vs 0.695248\n",
      "epoch 0, batch3104/3125, loss0.688922 vs 0.696528\n",
      "epoch 0, batch3105/3125, loss0.688576 vs 0.695778\n",
      "epoch 0, batch3106/3125, loss0.688428 vs 0.695780\n",
      "epoch 0, batch3107/3125, loss0.688276 vs 0.694541\n",
      "epoch 0, batch3108/3125, loss0.688329 vs 0.695522\n",
      "epoch 0, batch3109/3125, loss0.688222 vs 0.695010\n",
      "epoch 0, batch3110/3125, loss0.688344 vs 0.696103\n",
      "epoch 0, batch3111/3125, loss0.688507 vs 0.695329\n",
      "epoch 0, batch3112/3125, loss0.688717 vs 0.694921\n",
      "epoch 0, batch3113/3125, loss0.689016 vs 0.694796\n",
      "epoch 0, batch3114/3125, loss0.689389 vs 0.695121\n",
      "epoch 0, batch3115/3125, loss0.689746 vs 0.695785\n",
      "epoch 0, batch3116/3125, loss0.690110 vs 0.695037\n",
      "epoch 0, batch3117/3125, loss0.690555 vs 0.693933\n",
      "epoch 0, batch3118/3125, loss0.690900 vs 0.693912\n",
      "epoch 0, batch3119/3125, loss0.691434 vs 0.692926\n",
      "epoch 0, batch3120/3125, loss0.691757 vs 0.693084\n",
      "epoch 0, batch3121/3125, loss0.692213 vs 0.692591\n",
      "epoch 0, batch3122/3125, loss0.692290 vs 0.692800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch3123/3125, loss0.692660 vs 0.694355\n",
      "epoch 0, batch3124/3125, loss0.692380 vs 0.692529\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "#トレーニング\n",
    "#エポック数の指定\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    #データ全てのトータルロス\n",
    "    running_loss = 0.0 \n",
    "\n",
    "\n",
    "    for i, (inputs, _) in enumerate(trainloader):\n",
    "\n",
    "        # Variableに変形\n",
    "        # wrap them in Variable\n",
    "        inputs = Variable(inputs, requires_grad=True)\n",
    "        \n",
    "        valid = Variable(torch.ones(16), requires_grad=False)\n",
    "        fake = Variable(torch.zeros(16), requires_grad=False)\n",
    "        \n",
    "        #generator\n",
    "        \n",
    "        # optimizerの初期化\n",
    "        # zero the parameter gradients\n",
    "        optimizer_g.zero_grad()\n",
    "\n",
    "        z = Variable(torch.randn(16, 100), requires_grad=True)\n",
    "        \n",
    "        #一連の流れ\n",
    "        # forward + backward + optimize\n",
    "        img = generator(z)\n",
    "\n",
    "        #ここでラベルデータに対するCross-Entropyがとられる\n",
    "        loss_g = criterion(discriminator(img), valid)\n",
    "        loss_g.backward(retain_graph=True)\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        #discriminator\n",
    "        optimizer_d.zero_grad()\n",
    "        \n",
    "        # optimizerの初期化\n",
    "        # zero the parameter gradients\n",
    "\n",
    "        #ここでラベルデータに対するCross-Entropyがとられる\n",
    "        valid_loss = criterion(discriminator(inputs), valid)\n",
    "        fake_loss = criterion(discriminator(img.clone()), fake)\n",
    "        loss_d = (valid_loss + fake_loss) / 2\n",
    "        loss_d.backward(retain_graph=True)\n",
    "        optimizer_d.step()\n",
    "        \n",
    "        # ロスの表示\n",
    "        # print statistics\n",
    "        print(\"epoch %d, batch%d/%d, loss%f vs %f\" \n",
    "              % (epoch, i, len(trainloader), loss_g.item(), loss_d.item()))\n",
    "        if i % 100 == 0:\n",
    "            save_image(img.data[:9], \"images/%d.png\" % (i + epoch * len(trainloader)),\n",
    "                       nrow=3, normalize=True)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 100)\n",
      "\n",
      "↓ Linear(in_features=100, out_features=4096, bias=True)\n",
      "↓ weight: (4096, 100)\n",
      "↓ bias: (4096,)\n",
      "\n",
      "(16, 4096)\n",
      "(16, 256, 4, 4)\n",
      "\n",
      "↓ ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "↓ weight: (256, 128, 5, 5)\n",
      "↓ bias: (128,)\n",
      "\n",
      "(16, 128, 8, 8)\n",
      "\n",
      "↓ BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "↓ weight: (128,)\n",
      "↓ bias: (128,)\n",
      "\n",
      "(16, 128, 8, 8)\n",
      "\n",
      "↓ LeakyReLU(negative_slope=0.01)\n",
      "\n",
      "(16, 128, 8, 8)\n",
      "\n",
      "↓ ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "↓ weight: (128, 64, 5, 5)\n",
      "↓ bias: (64,)\n",
      "\n",
      "(16, 64, 16, 16)\n",
      "\n",
      "↓ BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "↓ weight: (64,)\n",
      "↓ bias: (64,)\n",
      "\n",
      "(16, 64, 16, 16)\n",
      "\n",
      "↓ LeakyReLU(negative_slope=0.01)\n",
      "\n",
      "(16, 64, 16, 16)\n",
      "\n",
      "↓ ConvTranspose2d(64, 3, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "↓ weight: (64, 3, 5, 5)\n",
      "↓ bias: (3,)\n",
      "\n",
      "(16, 3, 32, 32)\n",
      "\n",
      "↓ BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "↓ weight: (3,)\n",
      "↓ bias: (3,)\n",
      "\n",
      "(16, 3, 32, 32)\n",
      "\n",
      "↓ Tanh()\n",
      "\n",
      "(16, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input = torch.rand(16, 100)\n",
    "# input = torch.rand(16, 256, 4, 4)\n",
    "# pc1 = nn.Linear(100, 256)\n",
    "# pc2 = nn.Linear(256, 256 * 4)\n",
    "# pc3 = nn.Linear(256 * 4, 256 * 4 * 4)\n",
    "pc1 = nn.Linear(100, 256 * 4 * 4)\n",
    "\n",
    "conv1 = nn.ConvTranspose2d(256, 128, 5, stride=2, padding=2, output_padding=1)\n",
    "conv2 = nn.ConvTranspose2d(128, 64, 5, stride=2, padding=2, output_padding=1)\n",
    "# conv2 = nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1)\n",
    "conv3 = nn.ConvTranspose2d(64, 3, 5, stride=2, padding=2, output_padding=1)\n",
    "relu = nn.LeakyReLU()\n",
    "batch1 =  nn.BatchNorm2d(128)\n",
    "batch2 =  nn.BatchNorm2d(64)\n",
    "batch3 =  nn.BatchNorm2d(3)\n",
    "out = nn.Tanh()\n",
    "\n",
    "pre_pipe = [pc1]\n",
    "pipe = [conv1, batch1, relu, conv2, batch2, relu, conv3, batch3, out]\n",
    "\n",
    "def print_pipe(pipe, input):\n",
    "    print(tuple(input.shape))\n",
    "    for layer in pipe:\n",
    "        print(\"\")\n",
    "        print(\"↓\", layer)\n",
    "        input = layer(input)\n",
    "        try:\n",
    "            print(\"↓ weight:\", tuple(layer.weight.shape))\n",
    "            print(\"↓ bias:\", tuple(layer.bias.shape))\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        print(\"\")\n",
    "        print(tuple(input.shape))\n",
    "    return input\n",
    "\n",
    "input = print_pipe(pre_pipe, input)\n",
    "input = input.view(-1, 256, 4, 4)\n",
    "output = print_pipe(pipe, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 3, 32, 32)\n",
      "\n",
      "↓ Conv2d(3, 3, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "↓ weight: (3, 3, 5, 5)\n",
      "↓ bias: (3,)\n",
      "\n",
      "(16, 3, 16, 16)\n",
      "\n",
      "↓ BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "↓ weight: (3,)\n",
      "↓ bias: (3,)\n",
      "\n",
      "(16, 3, 16, 16)\n",
      "\n",
      "↓ LeakyReLU(negative_slope=0.01)\n",
      "\n",
      "(16, 3, 16, 16)\n",
      "\n",
      "↓ Conv2d(3, 3, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "↓ weight: (3, 3, 5, 5)\n",
      "↓ bias: (3,)\n",
      "\n",
      "(16, 3, 8, 8)\n",
      "\n",
      "↓ BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "↓ weight: (3,)\n",
      "↓ bias: (3,)\n",
      "\n",
      "(16, 3, 8, 8)\n",
      "\n",
      "↓ LeakyReLU(negative_slope=0.01)\n",
      "\n",
      "(16, 3, 8, 8)\n",
      "\n",
      "↓ Conv2d(3, 3, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "↓ weight: (3, 3, 5, 5)\n",
      "↓ bias: (3,)\n",
      "\n",
      "(16, 3, 4, 4)\n",
      "\n",
      "↓ Conv2d(3, 1, kernel_size=(4, 4), stride=(1, 1))\n",
      "↓ weight: (1, 3, 4, 4)\n",
      "↓ bias: (1,)\n",
      "\n",
      "(16, 1, 1, 1)\n",
      "\n",
      "↓ Sigmoid()\n",
      "\n",
      "(16, 1, 1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5577, 0.5150, 0.5563, 0.5154, 0.5007, 0.5275, 0.5034, 0.5058, 0.5836,\n",
       "        0.5439, 0.5099, 0.5122, 0.5819, 0.4811, 0.5670, 0.5467],\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ! no linear\n",
    "\n",
    "# Discriminator\n",
    "# input: [3, 32, 32]\n",
    "# layer [] conv & leakyReLU\n",
    "# output [1]\n",
    "\n",
    "input = torch.rand(16, 3, 32, 32)\n",
    "\n",
    "conv1 = nn.Conv2d(3, 3, 5, stride=2, padding=2)\n",
    "conv2 = nn.Conv2d(3, 3, 5, stride=2, padding=2)\n",
    "conv3 = nn.Conv2d(3, 3, 5, stride=2, padding=2)\n",
    "pc = nn.Conv2d(3, 1, 4)\n",
    "relu = nn.LeakyReLU()\n",
    "batch1 =  nn.BatchNorm2d(3)\n",
    "batch2 =  nn.BatchNorm2d(3)\n",
    "out = nn.Sigmoid()\n",
    "\n",
    "pipe = [conv1, batch1, relu, conv2, batch2, relu, conv3, pc, out]\n",
    "\n",
    "def print_pipe(pipe, input):\n",
    "    print(tuple(input.shape))\n",
    "    for layer in pipe:\n",
    "        print(\"\")\n",
    "        print(\"↓\", layer)\n",
    "        input = layer(input)\n",
    "        try:\n",
    "            print(\"↓ weight:\", tuple(layer.weight.shape))\n",
    "            print(\"↓ bias:\", tuple(layer.bias.shape))\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        print(\"\")\n",
    "        print(tuple(input.shape))\n",
    "    return input\n",
    "\n",
    "output = print_pipe(pipe, input)\n",
    "output.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1631e+00,  4.8946e-01, -9.0157e-01, -1.9192e-01,  4.3275e-01,\n",
       "        -5.4418e-01, -1.6182e-01, -3.3701e-01,  7.7639e-01,  2.3676e-02,\n",
       "        -1.3282e+00, -7.3611e-01, -5.1528e-02, -2.7328e-04,  1.4827e+00,\n",
       "        -1.4353e+00,  1.2153e+00,  7.6078e-01,  1.7725e+00, -2.3429e-01,\n",
       "        -6.8469e-01,  5.2390e-01, -1.0138e+00,  1.5600e+00, -1.4037e-01,\n",
       "         1.7212e-01, -8.9528e-01, -8.2807e-02, -3.8931e-01, -2.7677e-01,\n",
       "        -3.6832e-01, -1.9761e+00, -3.2232e-01,  5.7051e-01,  4.3654e-02,\n",
       "         6.4523e-02, -1.0529e+00,  4.3544e-01, -4.4667e-01,  7.6477e-01,\n",
       "        -2.9100e-01, -8.9874e-02,  3.8855e-01,  3.6674e-02,  1.3649e+00,\n",
       "         1.8008e+00,  1.3792e+00,  4.2143e-02,  1.5043e-01,  9.8680e-01,\n",
       "        -3.8085e-01,  3.0854e-01,  6.8735e-01, -4.8478e-01, -7.4162e-01,\n",
       "         1.0992e-01, -2.4957e-01, -2.4525e-01,  1.5533e+00,  4.8228e-01,\n",
       "         1.8783e+00, -2.9088e-01, -1.8227e+00, -4.0738e-01, -7.7897e-01,\n",
       "         4.2631e-01, -1.8404e+00,  2.5480e-01, -2.1308e+00, -1.6130e+00,\n",
       "        -1.1573e+00, -8.4564e-01, -1.0961e-02,  1.1824e+00,  2.4335e+00,\n",
       "         9.5850e-01,  7.0862e-01,  2.9254e+00, -3.8041e-02, -1.0261e+00,\n",
       "         1.0317e-01, -1.2940e+00, -2.0792e+00, -4.3188e-01, -7.1197e-02,\n",
       "        -8.3795e-01,  2.8539e-01,  1.4667e+00, -2.5170e-01, -6.9033e-01,\n",
       "        -1.5886e-02,  3.0017e-01, -7.9351e-01, -6.0687e-01,  1.4772e-01,\n",
       "         4.7175e-01,  9.7525e-01,  4.3948e-01,  1.0517e-01,  2.4136e+00])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[ 0.5608,  0.6000,  0.7020,  ...,  0.5608,  0.6471,  0.6863],\n",
       "           [ 0.4980,  0.6627,  0.6941,  ...,  0.6078,  0.6471,  0.5765],\n",
       "           [ 0.4902,  0.6627,  0.6549,  ...,  0.6549,  0.6235,  0.5529],\n",
       "           ...,\n",
       "           [ 0.1608, -0.0824, -0.0902,  ...,  0.2627,  0.7804,  0.6392],\n",
       "           [ 0.3490, -0.1137, -0.4196,  ..., -0.3333,  0.3020,  0.5686],\n",
       "           [ 0.2471, -0.2627, -0.4510,  ..., -0.4431, -0.2314,  0.3020]],\n",
       " \n",
       "          [[ 0.5059,  0.5373,  0.6314,  ...,  0.4824,  0.5843,  0.6314],\n",
       "           [ 0.4431,  0.6000,  0.6235,  ...,  0.5294,  0.5765,  0.5137],\n",
       "           [ 0.4353,  0.6000,  0.5843,  ...,  0.5843,  0.5529,  0.4980],\n",
       "           ...,\n",
       "           [ 0.1059, -0.1294, -0.1451,  ...,  0.2314,  0.7490,  0.5843],\n",
       "           [ 0.2863, -0.1608, -0.4667,  ..., -0.3647,  0.2627,  0.5137],\n",
       "           [ 0.2157, -0.2941, -0.4824,  ..., -0.4824, -0.2784,  0.2549]],\n",
       " \n",
       "          [[ 0.4275,  0.5216,  0.6471,  ...,  0.5294,  0.5922,  0.5765],\n",
       "           [ 0.3725,  0.5843,  0.6392,  ...,  0.5765,  0.5843,  0.4588],\n",
       "           [ 0.3569,  0.5843,  0.6000,  ...,  0.6235,  0.5686,  0.4431],\n",
       "           ...,\n",
       "           [-0.0745, -0.3490, -0.3647,  ..., -0.0118,  0.6314,  0.5608],\n",
       "           [ 0.2314, -0.2706, -0.6000,  ..., -0.5686,  0.1451,  0.4588],\n",
       "           [ 0.1608, -0.3412, -0.5451,  ..., -0.6000, -0.3804,  0.1529]]],\n",
       " \n",
       " \n",
       "         [[[-0.1529, -0.1843,  0.0196,  ..., -0.1216, -0.2627, -0.3961],\n",
       "           [-0.1765, -0.1137,  0.1137,  ..., -0.5451, -0.5843, -0.6235],\n",
       "           [-0.0745,  0.0275,  0.0902,  ..., -0.5843, -0.5529, -0.5529],\n",
       "           ...,\n",
       "           [-0.3569, -0.3020, -0.2941,  ...,  0.0196,  0.1059,  0.0667],\n",
       "           [-0.4039, -0.3333, -0.2784,  ...,  0.0118,  0.0275, -0.0118],\n",
       "           [-0.3804, -0.3412, -0.2549,  ...,  0.0039,  0.1137, -0.0431]],\n",
       " \n",
       "          [[ 0.0510,  0.0196,  0.2235,  ...,  0.0745, -0.0980, -0.2235],\n",
       "           [ 0.0667,  0.1294,  0.3647,  ..., -0.4275, -0.4824, -0.5137],\n",
       "           [ 0.1843,  0.2784,  0.3490,  ..., -0.5294, -0.5137, -0.4980],\n",
       "           ...,\n",
       "           [-0.4824, -0.4196, -0.4118,  ..., -0.1843, -0.0980, -0.1373],\n",
       "           [-0.5216, -0.4510, -0.3961,  ..., -0.1922, -0.1765, -0.2157],\n",
       "           [-0.4745, -0.4588, -0.3725,  ..., -0.1843, -0.0745, -0.2235]],\n",
       " \n",
       "          [[ 0.2706,  0.2157,  0.3882,  ...,  0.2078,  0.0431, -0.0745],\n",
       "           [ 0.2000,  0.2392,  0.4510,  ..., -0.2941, -0.3490, -0.3882],\n",
       "           [ 0.3255,  0.4196,  0.4745,  ..., -0.4196, -0.3961, -0.4039],\n",
       "           ...,\n",
       "           [-0.7804, -0.7333, -0.7490,  ..., -0.4510, -0.3569, -0.3804],\n",
       "           [-0.8353, -0.7725, -0.7333,  ..., -0.4588, -0.4275, -0.4667],\n",
       "           [-0.7804, -0.7961, -0.7098,  ..., -0.4824, -0.3569, -0.4980]]],\n",
       " \n",
       " \n",
       "         [[[-0.3804, -0.1529, -0.1451,  ..., -0.4039, -0.3804, -0.4431],\n",
       "           [-0.6000, -0.4431, -0.5529,  ..., -0.6157, -0.6784, -0.7412],\n",
       "           [-0.6863, -0.6392, -0.7333,  ..., -0.6941, -0.7490, -0.7804],\n",
       "           ...,\n",
       "           [-0.0118, -0.0118,  0.0431,  ..., -0.7176, -0.7098, -0.7647],\n",
       "           [-0.0039, -0.0039,  0.0745,  ..., -0.7412, -0.7490, -0.7569],\n",
       "           [-0.0118, -0.0353, -0.0275,  ..., -0.6627, -0.6941, -0.6784]],\n",
       " \n",
       "          [[-0.3098, -0.0667, -0.0431,  ..., -0.3098, -0.2863, -0.3804],\n",
       "           [-0.5294, -0.3569, -0.4431,  ..., -0.5216, -0.5765, -0.6627],\n",
       "           [-0.6078, -0.5451, -0.6235,  ..., -0.5843, -0.6314, -0.6941],\n",
       "           ...,\n",
       "           [ 0.0275,  0.0353,  0.0902,  ..., -0.7098, -0.7020, -0.7647],\n",
       "           [ 0.0275,  0.0431,  0.1216,  ..., -0.7333, -0.7490, -0.7569],\n",
       "           [ 0.0118, -0.0039, -0.0039,  ..., -0.6549, -0.6941, -0.6784]],\n",
       " \n",
       "          [[-0.3490, -0.0745, -0.0510,  ..., -0.3647, -0.3176, -0.4039],\n",
       "           [-0.5765, -0.3961, -0.4667,  ..., -0.5608, -0.6157, -0.6941],\n",
       "           [-0.6706, -0.6314, -0.6706,  ..., -0.6235, -0.6706, -0.7255],\n",
       "           ...,\n",
       "           [-0.2000, -0.2627, -0.2000,  ..., -0.7255, -0.7020, -0.7647],\n",
       "           [-0.1843, -0.2392, -0.1608,  ..., -0.7569, -0.7490, -0.7569],\n",
       "           [-0.1137, -0.1922, -0.1843,  ..., -0.6706, -0.6941, -0.6784]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-0.4039, -0.3882, -0.3569,  ...,  0.1843,  0.3176, -0.3490],\n",
       "           [-0.4196, -0.3882, -0.3490,  ..., -0.1608, -0.3490, -0.4902],\n",
       "           [-0.4039, -0.3804, -0.3412,  ..., -0.7804, -0.7647, -0.7333],\n",
       "           ...,\n",
       "           [-0.7020, -0.4353, -0.5137,  ..., -0.8353, -0.7412, -0.1373],\n",
       "           [-0.5843, -0.3176, -0.4824,  ..., -0.8196, -0.8196, -0.4275],\n",
       "           [-0.6863, -0.4275, -0.4824,  ..., -0.8431, -0.8588, -0.7804]],\n",
       " \n",
       "          [[-0.3490, -0.3176, -0.2941,  ...,  0.2000,  0.3176, -0.3725],\n",
       "           [-0.3490, -0.3176, -0.2863,  ..., -0.1451, -0.2863, -0.4431],\n",
       "           [-0.3490, -0.3176, -0.2863,  ..., -0.6863, -0.6706, -0.6627],\n",
       "           ...,\n",
       "           [-0.7255, -0.4824, -0.5686,  ..., -0.8431, -0.7647, -0.2314],\n",
       "           [-0.6235, -0.4039, -0.5529,  ..., -0.8196, -0.8118, -0.4824],\n",
       "           [-0.7176, -0.5059, -0.5373,  ..., -0.8431, -0.8510, -0.8039]],\n",
       " \n",
       "          [[-0.3490, -0.3176, -0.2784,  ...,  0.1059,  0.2549, -0.3569],\n",
       "           [-0.3490, -0.3255, -0.2863,  ..., -0.2000, -0.2549, -0.3333],\n",
       "           [-0.3490, -0.3255, -0.2863,  ..., -0.7255, -0.6706, -0.6314],\n",
       "           ...,\n",
       "           [-0.8118, -0.6549, -0.7020,  ..., -0.8588, -0.8118, -0.4667],\n",
       "           [-0.7098, -0.6078, -0.6863,  ..., -0.8510, -0.8588, -0.6314],\n",
       "           [-0.7961, -0.6549, -0.6549,  ..., -0.8745, -0.8588, -0.8431]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0980,  0.1137,  0.1137,  ...,  0.2157,  0.2078,  0.2078],\n",
       "           [ 0.1373,  0.1451,  0.1686,  ...,  0.2863,  0.2078,  0.1922],\n",
       "           [ 0.1451,  0.1451,  0.1765,  ...,  0.3647,  0.2392,  0.1843],\n",
       "           ...,\n",
       "           [ 0.3333,  0.1843,  0.0980,  ...,  0.5608,  0.4039,  0.3569],\n",
       "           [ 0.2314,  0.2549,  0.3020,  ...,  0.5373,  0.3412,  0.2863],\n",
       "           [ 0.1451,  0.3725,  0.4902,  ...,  0.4196,  0.2627,  0.2235]],\n",
       " \n",
       "          [[ 0.2863,  0.3020,  0.3098,  ...,  0.4588,  0.4431,  0.4196],\n",
       "           [ 0.2784,  0.2863,  0.2941,  ...,  0.4431,  0.4275,  0.4353],\n",
       "           [ 0.2314,  0.2392,  0.2471,  ...,  0.3882,  0.4039,  0.4275],\n",
       "           ...,\n",
       "           [ 0.4588,  0.0980, -0.3882,  ...,  0.3255,  0.3490,  0.4902],\n",
       "           [ 0.3961,  0.2078, -0.1608,  ...,  0.3569,  0.3725,  0.4667],\n",
       "           [ 0.3490,  0.3725,  0.0667,  ...,  0.3020,  0.3725,  0.4196]],\n",
       " \n",
       "          [[-0.0824, -0.0902, -0.1059,  ...,  0.0745,  0.0588,  0.0588],\n",
       "           [-0.0588, -0.0588, -0.0431,  ..., -0.0196, -0.0588, -0.0510],\n",
       "           [-0.0510, -0.0431, -0.0196,  ..., -0.0353, -0.0745, -0.0824],\n",
       "           ...,\n",
       "           [ 0.6941,  0.2157, -0.6000,  ...,  0.1765, -0.0431, -0.0118],\n",
       "           [ 0.4980,  0.3098, -0.2941,  ...,  0.1216, -0.1137, -0.0824],\n",
       "           [ 0.2000,  0.3176, -0.1137,  ..., -0.0118, -0.1843, -0.1922]]],\n",
       " \n",
       " \n",
       "         [[[-0.4275, -0.4275, -0.4196,  ..., -0.5843, -0.5843, -0.6000],\n",
       "           [-0.3804, -0.3882, -0.3882,  ..., -0.5529, -0.5529, -0.5686],\n",
       "           [-0.3725, -0.3961, -0.4431,  ..., -0.5529, -0.5529, -0.5373],\n",
       "           ...,\n",
       "           [-0.6235, -0.6157, -0.6078,  ..., -0.6000, -0.5765, -0.5608],\n",
       "           [-0.6549, -0.6627, -0.6627,  ..., -0.5451, -0.5373, -0.5137],\n",
       "           [-0.6392, -0.6549, -0.6471,  ..., -0.5608, -0.6000, -0.6157]],\n",
       " \n",
       "          [[-0.4431, -0.4510, -0.4431,  ..., -0.6627, -0.6627, -0.6784],\n",
       "           [-0.4118, -0.4275, -0.4275,  ..., -0.6314, -0.6314, -0.6471],\n",
       "           [-0.4275, -0.4510, -0.4902,  ..., -0.6314, -0.6314, -0.6157],\n",
       "           ...,\n",
       "           [-0.7098, -0.7020, -0.6941,  ..., -0.6863, -0.6471, -0.6157],\n",
       "           [-0.7412, -0.7490, -0.7490,  ..., -0.6078, -0.5843, -0.5529],\n",
       "           [-0.7255, -0.7412, -0.7333,  ..., -0.6078, -0.6314, -0.6314]],\n",
       " \n",
       "          [[-0.5059, -0.5059, -0.4980,  ..., -0.5765, -0.5765, -0.5922],\n",
       "           [-0.4667, -0.4745, -0.4745,  ..., -0.5451, -0.5451, -0.5608],\n",
       "           [-0.4745, -0.4980, -0.5373,  ..., -0.5451, -0.5451, -0.5294],\n",
       "           ...,\n",
       "           [-0.6784, -0.6706, -0.6627,  ..., -0.6784, -0.6314, -0.5843],\n",
       "           [-0.7098, -0.7176, -0.7176,  ..., -0.6000, -0.5608, -0.5216],\n",
       "           [-0.6941, -0.7098, -0.7020,  ..., -0.5922, -0.6078, -0.6000]]]]),\n",
       " tensor([2, 5, 7, 3, 0, 8, 3, 2, 6, 7, 7, 3, 2, 1, 7, 6])]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainloader.__iter__().next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Variable(torch.ones(16), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
