{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #基本モジュール\n",
    "from torch.autograd import Variable #自動微分用\n",
    "import torch.nn as nn #ネットワーク構築用\n",
    "import torch.optim as optim #最適化関数\n",
    "import torch.nn.functional as F #ネットワーク用の様々な関数\n",
    "import torch.utils.data #データセット読み込み関連\n",
    "import torchvision #画像関連\n",
    "from torchvision import datasets, models, transforms #画像用データセット諸々\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "from logging.config import dictConfig\n",
    "from logging import getLogger\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) # [3, 32, 32] => [6, 28, 28]\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # [6, 28, 28] => [16, 24, 24]\n",
    "        self.pool = nn.MaxPool2d(2, 2) # [N,C,H,W] => [N, C, H/2, W/2]\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120) # \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion, logger):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            logger.debug(\"[train] batch : %s/%s (%.0f%%),\\tloss : %.6f\",\n",
    "                batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item())\n",
    "    return (None, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion, logger):\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    correct = 0\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss.append(criterion(output, target).item())  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            result += torch.eq(torch.max(output, 1).indices, target)\n",
    "            \n",
    "    logger.debug(\"[dump] %s\", test_loss)\n",
    "    test_loss = np.mean(np.array(test_loss))\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    logger.debug(\"[test] ave loss : %.4f,\\taccu : %d/%d(%.0f%%)\",\n",
    "        test_loss, correct, len(test_loader.dataset), accuracy)\n",
    "    \n",
    "    return (torch.tensor(result).numpy(), (test_loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[system] start\n",
      "[argparse] batch_size=16\n",
      "[argparse] epochs=1\n",
      "[argparse] save_model=False\n",
      "[device] cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[system] epoch 0\n",
      "[system] train_a -> test_b\n",
      "[system] train_b -> test_a\n",
      "[system] finish\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    with open('logging.json') as f:\n",
    "        dictConfig(json.load(f))\n",
    "    logger = getLogger('env')\n",
    "    logger.debug(\"<\" * 40)\n",
    "    logger.info(\"[system] start\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    def fetch_args(args=[]):\n",
    "        parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "        parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                            help='input batch size for training (default: 64)')\n",
    "        parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
    "                            help='number of epochs to train (default: 14)')\n",
    "        parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                            help='For Saving the current Model')\n",
    "        return parser.parse_args(args=args)\n",
    "  \n",
    "    args = fetch_args([\"--epochs\", \"1\", \"--batch-size\", \"16\"])\n",
    "    for arg in vars(args):\n",
    "        logger.info(\"[argparse] %s=%s\", arg, vars(args)[arg])\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    logger.info(\"[device] %s\", device)\n",
    "    \n",
    "    #画像の変形処理\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    #CIFAR-10のtrain, testsetのロード\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    \n",
    "    model_X = CNN().to(device)\n",
    "    model_Y = CNN().to(device)\n",
    "    optimizer_X = optim.SGD(model_X.parameters(), lr=0.001, momentum=0.9)\n",
    "    optimizer_Y = optim.SGD(model_Y.parameters(), lr=0.001, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # init\n",
    "    N = int(len(trainset) / 2)\n",
    "    indices = np.arange(2 * N)\n",
    "    A, B = indices[:N], indices[N:]\n",
    "    \n",
    "    log = {\"X_train\":[], \"X_test\":[], \"Y_train\":[], \"Y_test\":[], \"X_acc\":[], \"Y_acc\":[]}\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        logger.debug(\"-\" * 20)\n",
    "        logger.info(\"[system] epoch %d\", epoch)\n",
    "        \n",
    "        # Split\n",
    "        A_loader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.Subset(trainset, A), batch_size=args.batch_size)\n",
    "        B_loader = torch.utils.data.DataLoader(\n",
    "            torch.utils.data.Subset(trainset, B), batch_size=args.batch_size)\n",
    "        \n",
    "        # test B\n",
    "        logger.info(\"[system] train_a -> test_b\")\n",
    "        (_, loss_XA) = train(model_X, device, A_loader, optimizer_X, criterion, logger)\n",
    "        (testB, (loss_XB, acc_X)) = test(model_X, device, B_loader, criterion, logger)\n",
    "        \n",
    "        # test A\n",
    "        logger.info(\"[system] train_b -> test_a\")\n",
    "        (_, loss_YB) = train(model_Y, device, B_loader, optimizer_Y, criterion, logger)\n",
    "        (testA, (loss_YA, acc_Y)) = test(model_Y, device, A_loader, criterion, logger)\n",
    "        \n",
    "        # Swap\n",
    "        A_o, A_x = A[testA], A[np.logical_not(testA)]\n",
    "        B_o, B_x = B[testB], B[np.logical_not(testB)]\n",
    "        s = min(len(A_x), len(B_o))\n",
    "        A = np.concatenate([A_o, B_o[:s], A_x[s:]])\n",
    "        B = np.concatenate([B_x, A_x[:s], B_o[s:]])\n",
    "        \n",
    "        # log\n",
    "        logger.debug(\"[dump] test_b : %s...\", testB[:10])\n",
    "        logger.debug(\"[dump] test_a : %s...\", testA[:10])\n",
    "        logger.debug(\"[dump] swap : B -> A : %s...\", B_o[:s][:10])\n",
    "        logger.debug(\"[dump] swap : A -> B : %s...\", A_x[:s][:10])\n",
    "        logger.debug(\"[dump] X loss : %s / %s,\\tacc : %s\", loss_XA, loss_XB, acc_X)\n",
    "        logger.debug(\"[dump] Y loss : %s / %s,\\tacc : %s\", loss_YB, loss_YA, acc_Y)\n",
    "        log[\"X_train\"].append(loss_XA)\n",
    "        log[\"X_test\"].append(loss_XB)\n",
    "        log[\"Y_train\"].append(loss_YB)\n",
    "        log[\"Y_test\"].append(loss_YA)\n",
    "        log[\"X_acc\"].append(acc_X)\n",
    "        log[\"Y_acc\"].append(acc_Y)\n",
    "    \n",
    "    logger.debug(\"%s statistics %s\", \"-\" * 10, \"-\" * 10)\n",
    "    logger.debug(\"[stat] X train loss : %s\", log[\"X_train\"])\n",
    "    logger.debug(\"[stat] X test loss : %s\", log[\"X_test\"])\n",
    "    logger.debug(\"[stat] Y train loss : %s\", log[\"Y_train\"])\n",
    "    logger.debug(\"[stat] Y test loss : %s\", log[\"Y_test\"])\n",
    "    logger.debug(\"[stat] X accuracy : %s\", log[\"X_acc\"])\n",
    "    logger.debug(\"[stat] Y accuracy : %s\", log[\"Y_acc\"])\n",
    "    logger.debug(\"[stat] elapsed time : %s[s]\", time.time() - start_time)\n",
    "    \n",
    "    if args.save_model:\n",
    "        logger.info(\"[system] saving...\")\n",
    "        torch.save(model.state_dict(), \"swap.pt\")\n",
    "    \n",
    "    logger.info(\"[system] finish\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#画像の変形処理\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#CIFAR-10のtrain, testsetのロード\n",
    "#変形はtransformを適用\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "#DataLoaderの適用->これによりバッチの割り当て・シャッフルをまとめて行うことができる\n",
    "#batch_sizeでバッチサイズを指定\n",
    "#num_workersでいくつのコアでデータをロードするか指定(デフォルトはメインのみ)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=16,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=16,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = [torch.tensor(False), torch.tensor(True)]\n",
    "torch.tensor(r).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "log = {\"A_acc\":[\"a\", \"bb\"], \"B_acc\":[1, 2]}\n",
    "\n",
    "with open(\"stock.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f, lineterminator=\"\\n\")\n",
    "    writer.writerows(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "from logging import getLogger, StreamHandler, DEBUG\n",
    "logger = getLogger(__name__)\n",
    "handler = StreamHandler()\n",
    "handler.setLevel(DEBUG)\n",
    "logger.setLevel(DEBUG)\n",
    "logger.addHandler(handler)\n",
    "logger.propagate = False\n",
    "\n",
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "messagea\n",
      "messagea\n",
      "messagea\n",
      "messagea\n",
      "messagea\n",
      "messagea\n",
      "messagea\n",
      "messagea\n",
      "messagea\n",
      "messagea\n",
      "mess\n",
      "mess\n",
      "mess\n",
      "mess\n",
      "mess\n",
      "mess\n",
      "mess\n",
      "mess\n",
      "mess\n",
      "mess\n"
     ]
    }
   ],
   "source": [
    "# from logging import getLogger\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "from logging import StreamHandler, basicConfig, DEBUG, getLogger, Formatter\n",
    "\n",
    "def setup_logger(log_filename):\n",
    "    format_str = '%(asctime)s@%(name)s %(levelname)s # %(message)s'\n",
    "    basicConfig(filename=log_filename, level=DEBUG, format=format_str)\n",
    "    stream_handler = StreamHandler()\n",
    "    stream_handler.setFormatter(Formatter(format_str))\n",
    "    getLogger().addHandler(stream_handler)\n",
    "    \n",
    "\n",
    "logger.info(\"messagea\")\n",
    "\n",
    "logger.info(\"mess\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_logger(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aa\n",
      "aa\n",
      "aa\n",
      "aa\n",
      "aa\n",
      "aa\n",
      "aa\n",
      "aa\n",
      "aa\n",
      "aa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Logger __main__ (DEBUG)>\n"
     ]
    }
   ],
   "source": [
    "from logging import StreamHandler, basicConfig, INFO, getLogger, Formatter, shutdown\n",
    "\n",
    "def setup_logger(log_filename):\n",
    "    format_str = '%(asctime)s@%(name)s %(levelname)s # %(message)s'\n",
    "    basicConfig(filename=log_filename, level=INFO, format=format_str)\n",
    "    stream_handler = StreamHandler()\n",
    "    stream_handler.setFormatter(Formatter(format_str))\n",
    "    getLogger().addHandler(stream_handler)\n",
    "\n",
    "def kill_logger():\n",
    "    # for l in getLogger(__name__):\n",
    "    # logger = loggers.get(l)\n",
    "    for h in logger.handlers:\n",
    "        logger.removeHandler(h)\n",
    "    # getLogger().removeHandler()\n",
    "    shutdown()\n",
    "    return\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "setup_logger(\"log.txt\")\n",
    "\n",
    "logger.info(\"aa\")\n",
    "print(logger)\n",
    "# kill_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-29 19:33:53,564: RootLogger: error\n",
      "2020-04-29 19:33:53,566: myapp: error\n",
      "2020-04-29 19:33:53,568: env: error\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from logging.config import dictConfig\n",
    "\n",
    "with open('logging.json') as f:\n",
    "    dictConfig(json.load(f))\n",
    "\n",
    "\n",
    "# 動作確認\n",
    "from logging import getLogger\n",
    "\n",
    "# RootLogger: error のみが標準エラーに\n",
    "root_logger = getLogger()\n",
    "root_logger.debug('RootLogger: debug')\n",
    "root_logger.error('RootLogger: error')\n",
    "\n",
    "# myapp: debug は指定したファイルに\n",
    "# myapp: error は指定したファイルと標準エラーに\n",
    "myapp_logger = getLogger('env')\n",
    "myapp_logger.debug('myapp: debug')\n",
    "myapp_logger.error('myapp: error')\n",
    "myapp_logger = getLogger('env.model')\n",
    "myapp_logger.error('env: error')\n",
    "\n",
    "# これはすべて無視される\n",
    "hoge_logger = getLogger('hoge.fuga.piyo')\n",
    "hoge_logger.debug('hoge: debug')\n",
    "hoge_logger.error('hoge: error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo BAR\n",
      "{'foo': 'BAR'}\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--foo')\n",
    "args = parser.parse_args(['--foo', 'BAR'])\n",
    "for a in vars(args):\n",
    "    print(a, vars(args)[a])\n",
    "print(vars(args))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
